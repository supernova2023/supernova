repo,description
ydataai/ydata-profiling,"Bug Report: SystemError: unknown opcode: https://github.com/ydataai/ydata-profiling/issues/1295
Description: ### Current Behaviour

I am running in Jupyter Lab and just upgraded to Python 3.10.6.  My  initial dataset is in a Pandas dataframe that is less than 8000 rows and less than 50 columns.  When I used this dataframe, I received the error `SystemError: unknown opcode`.  It also blows through my computer memory and bogs down my browser.  (I have nothing else running.)

I then tried it with a subset of the data.  Here is my code:

```
import numpy as np
import pandas as pd
from ydata_profiling import ProfileReport

df = pd.read_excel('../data/mendeley.xlsx')
test_df = df.head(10)
test_profile = ProfileReport(df)
test_profile
```

(Note that I also get the following error if I use `.to_widgets()`.)

These 10 rows also crash.  I receive the following error:

```
XXX lineno: 77306, opcode: 36
XXX lineno: 77306, opcode: 36
XXX lineno: 77306, opcode: 36
XXX lineno: 77306, opcode: 36
XXX lineno: 77306, opcode: 36
XXX lineno: 77306, opcode: 36
XXX lineno: 77306, opcode: 36
XXX lineno: 77306, opcode: 36
XXX lineno: 77306, opcode: 36
XXX lineno: 77306, opcode: 36
XXX lineno: 77306, opcode: 36
XXX lineno: 77306, opcode: 36
XXX lineno: 77306, opcode: 36
XXX lineno: 77306, opcode: 36
XXX lineno: 77306, opcode: 36
XXX lineno: 77306, opcode: 36
XXX lineno: 77306, opcode: 36
---------------------------------------------------------------------------
SystemError                               Traceback (most recent call last)
File ~/notebook_env/lib/python3.10/site-packages/IPython/core/formatters.py:342, in BaseFormatter.__call__(self, obj)
    340     method = get_real_method(obj, self.print_method)
    341     if method is not None:
--> 342         return method()
    343     return None
    344 else:

File ~/notebook_env/lib/python3.10/site-packages/typeguard/__init__.py:1033, in typechecked.<locals>.wrapper(*args, **kwargs)
   1031 memo = _CallMemo(python_func, _localns, args=args, kwargs=kwargs)
   1032 check_argument_types(memo)
-> 1033 retval = func(*args, **kwargs)
   1034 try:
   1035     check_return_type(retval, memo)

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/profile_report.py:511, in ProfileReport._repr_html_(self)
    509 def _repr_html_(self) -> None:
    510     """"""The ipython notebook widgets user interface gets called by the jupyter notebook.""""""
--> 511     self.to_notebook_iframe()

File ~/notebook_env/lib/python3.10/site-packages/typeguard/__init__.py:1033, in typechecked.<locals>.wrapper(*args, **kwargs)
   1031 memo = _CallMemo(python_func, _localns, args=args, kwargs=kwargs)
   1032 check_argument_types(memo)
-> 1033 retval = func(*args, **kwargs)
   1034 try:
   1035     check_return_type(retval, memo)

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/profile_report.py:491, in ProfileReport.to_notebook_iframe(self)
    489 with warnings.catch_warnings():
    490     warnings.simplefilter(""ignore"")
--> 491     display(get_notebook_iframe(self.config, self))

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/report/presentation/flavours/widget/notebook.py:75, in get_notebook_iframe(config, profile)
     73     output = get_notebook_iframe_src(config, profile)
     74 elif attribute == IframeAttribute.srcdoc:
---> 75     output = get_notebook_iframe_srcdoc(config, profile)
     76 else:
     77     raise ValueError(
     78         f'Iframe Attribute can be ""src"" or ""srcdoc"" (current: {attribute}).'
     79     )

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/report/presentation/flavours/widget/notebook.py:29, in get_notebook_iframe_srcdoc(config, profile)
     27 width = config.notebook.iframe.width
     28 height = config.notebook.iframe.height
---> 29 src = html.escape(profile.to_html())
     31 iframe = f'<iframe width=""{width}"" height=""{height}"" srcdoc=""{src}"" frameborder=""0"" allowfullscreen></iframe>'
     33 return HTML(iframe)

File ~/notebook_env/lib/python3.10/site-packages/typeguard/__init__.py:1033, in typechecked.<locals>.wrapper(*args, **kwargs)
   1031 memo = _CallMemo(python_func, _localns, args=args, kwargs=kwargs)
   1032 check_argument_types(memo)
-> 1033 retval = func(*args, **kwargs)
   1034 try:
   1035     check_return_type(retval, memo)

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/profile_report.py:461, in ProfileReport.to_html(self)
    453 def to_html(self) -> str:
    454     """"""Generate and return complete template as lengthy string
    455         for using with frameworks.
    456 
   (...)
    459 
    460     """"""
--> 461     return self.html

File ~/notebook_env/lib/python3.10/site-packages/typeguard/__init__.py:1033, in typechecked.<locals>.wrapper(*args, **kwargs)
   1031 memo = _CallMemo(python_func, _localns, args=args, kwargs=kwargs)
   1032 check_argument_types(memo)
-> 1033 retval = func(*args, **kwargs)
   1034 try:
   1035     check_return_type(retval, memo)

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/profile_report.py:272, in ProfileReport.html(self)
    269 @property
    270 def html(self) -> str:
    271     if self._html is None:
--> 272         self._html = self._render_html()
    273     return self._html

File ~/notebook_env/lib/python3.10/site-packages/typeguard/__init__.py:1033, in typechecked.<locals>.wrapper(*args, **kwargs)
   1031 memo = _CallMemo(python_func, _localns, args=args, kwargs=kwargs)
   1032 check_argument_types(memo)
-> 1033 retval = func(*args, **kwargs)
   1034 try:
   1035     check_return_type(retval, memo)

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/profile_report.py:380, in ProfileReport._render_html(self)
    377 def _render_html(self) -> str:
    378     from ydata_profiling.report.presentation.flavours import HTMLReport
--> 380     report = self.report
    382     with tqdm(
    383         total=1, desc=""Render HTML"", disable=not self.config.progress_bar
    384     ) as pbar:
    385         html = HTMLReport(copy.deepcopy(report)).render(
    386             nav=self.config.html.navbar_show,
    387             offline=self.config.html.use_local_assets,
   (...)
    395             version=self.description_set[""package""][""ydata_profiling_version""],
    396         )

File ~/notebook_env/lib/python3.10/site-packages/typeguard/__init__.py:1033, in typechecked.<locals>.wrapper(*args, **kwargs)
   1031 memo = _CallMemo(python_func, _localns, args=args, kwargs=kwargs)
   1032 check_argument_types(memo)
-> 1033 retval = func(*args, **kwargs)
   1034 try:
   1035     check_return_type(retval, memo)

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/profile_report.py:266, in ProfileReport.report(self)
    263 @property
    264 def report(self) -> Root:
    265     if self._report is None:
--> 266         self._report = get_report_structure(self.config, self.description_set)
    267     return self._report

File ~/notebook_env/lib/python3.10/site-packages/typeguard/__init__.py:1033, in typechecked.<locals>.wrapper(*args, **kwargs)
   1031 memo = _CallMemo(python_func, _localns, args=args, kwargs=kwargs)
   1032 check_argument_types(memo)
-> 1033 retval = func(*args, **kwargs)
   1034 try:
   1035     check_return_type(retval, memo)

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/profile_report.py:248, in ProfileReport.description_set(self)
    245 @property
    246 def description_set(self) -> Dict[str, Any]:
    247     if self._description_set is None:
--> 248         self._description_set = describe_df(
    249             self.config,
    250             self.df,
    251             self.summarizer,
    252             self.typeset,
    253             self._sample,
    254         )
    255     return self._description_set

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/model/describe.py:71, in describe(config, df, summarizer, typeset, sample)
     69 # Variable-specific
     70 pbar.total += len(df.columns)
---> 71 series_description = get_series_descriptions(
     72     config, df, summarizer, typeset, pbar
     73 )
     75 pbar.set_postfix_str(""Get variable types"")
     76 pbar.total += 1

File ~/notebook_env/lib/python3.10/site-packages/multimethod/__init__.py:315, in multimethod.__call__(self, *args, **kwargs)
    313 func = self[tuple(func(arg) for func, arg in zip(self.type_checkers, args))]
    314 try:
--> 315     return func(*args, **kwargs)
    316 except TypeError as ex:
    317     raise DispatchError(f""Function {func.__code__}"") from ex

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/model/pandas/summary_pandas.py:99, in pandas_get_series_descriptions(config, df, summarizer, typeset, pbar)
     96 else:
     97     # TODO: use `Pool` for Linux-based systems
     98     with multiprocessing.pool.ThreadPool(pool_size) as executor:
---> 99         for i, (column, description) in enumerate(
    100             executor.imap_unordered(multiprocess_1d, args)
    101         ):
    102             pbar.set_postfix_str(f""Describe variable:{column}"")
    103             series_description[column] = description

File /usr/lib/python3.10/multiprocessing/pool.py:873, in IMapIterator.next(self, timeout)
    871 if success:
    872     return value
--> 873 raise value

File /usr/lib/python3.10/multiprocessing/pool.py:125, in worker(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)
    123 job, i, func, args, kwds = task
    124 try:
--> 125     result = (True, func(*args, **kwds))
    126 except Exception as e:
    127     if wrap_exception and func is not _helper_reraises_exception:

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/model/pandas/summary_pandas.py:79, in pandas_get_series_descriptions.<locals>.multiprocess_1d(args)
     69 """"""Wrapper to process series in parallel.
     70 
     71 Args:
   (...)
     76     A tuple with column and the series description.
     77 """"""
     78 column, series = args
---> 79 return column, describe_1d(config, series, summarizer, typeset)

File ~/notebook_env/lib/python3.10/site-packages/multimethod/__init__.py:315, in multimethod.__call__(self, *args, **kwargs)
    313 func = self[tuple(func(arg) for func, arg in zip(self.type_checkers, args))]
    314 try:
--> 315     return func(*args, **kwargs)
    316 except TypeError as ex:
    317     raise DispatchError(f""Function {func.__code__}"") from ex

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/model/pandas/summary_pandas.py:57, in pandas_describe_1d(config, series, summarizer, typeset)
     54     vtype = typeset.detect_type(series)
     56 typeset.type_schema[series.name] = vtype
---> 57 return summarizer.summarize(config, series, dtype=vtype)

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/model/summarizer.py:39, in BaseSummarizer.summarize(self, config, series, dtype)
     31 def summarize(
     32     self, config: Settings, series: pd.Series, dtype: Type[VisionsBaseType]
     33 ) -> dict:
     34     """"""
     35 
     36     Returns:
     37         object:
     38     """"""
---> 39     _, _, summary = self.handle(str(dtype), config, series, {""type"": str(dtype)})
     40     return summary

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/model/handler.py:62, in Handler.handle(self, dtype, *args, **kwargs)
     60 funcs = self.mapping.get(dtype, [])
     61 op = compose(funcs)
---> 62 return op(*args)

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/model/handler.py:21, in compose.<locals>.func.<locals>.func2(*x)
     19     return f(*x)
     20 else:
---> 21     return f(*res)

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/model/handler.py:21, in compose.<locals>.func.<locals>.func2(*x)
     19     return f(*x)
     20 else:
---> 21     return f(*res)

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/model/handler.py:21, in compose.<locals>.func.<locals>.func2(*x)
     19     return f(*x)
     20 else:
---> 21     return f(*res)

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/model/handler.py:17, in compose.<locals>.func.<locals>.func2(*x)
     16 def func2(*x) -> Any:
---> 17     res = g(*x)
     18     if type(res) == bool:
     19         return f(*x)

File ~/notebook_env/lib/python3.10/site-packages/multimethod/__init__.py:315, in multimethod.__call__(self, *args, **kwargs)
    313 func = self[tuple(func(arg) for func, arg in zip(self.type_checkers, args))]
    314 try:
--> 315     return func(*args, **kwargs)
    316 except TypeError as ex:
    317     raise DispatchError(f""Function {func.__code__}"") from ex

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/model/summary_algorithms.py:65, in series_hashable.<locals>.inner(config, series, summary)
     63 if not summary[""hashable""]:
     64     return config, series, summary
---> 65 return fn(config, series, summary)

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/model/summary_algorithms.py:82, in series_handle_nulls.<locals>.inner(config, series, summary)
     79 if series.hasnans:
     80     series = series.dropna()
---> 82 return fn(config, series, summary)

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/model/pandas/describe_categorical_pandas.py:256, in pandas_describe_categorical_1d(config, series, summary)
    245     summary.update(
    246         histogram_compute(
    247             config,
   (...)
    252         )
    253     )
    255 if config.vars.cat.characters:
--> 256     summary.update(unicode_summary_vc(value_counts))
    258 if config.vars.cat.words:
    259     summary.update(word_summary_vc(value_counts, config.vars.cat.stop_words))

File ~/notebook_env/lib/python3.10/site-packages/ydata_profiling/model/pandas/describe_categorical_pandas.py:58, in unicode_summary_vc(vc)
     56 def unicode_summary_vc(vc: pd.Series) -> dict:
     57     try:
---> 58         from tangled_up_in_unicode import (  # type: ignore
     59             block,
     60             block_abbr,
     61             category,
     62             category_long,
     63             script,
     64         )
     65     except ImportError:
     66         from unicodedata import category as _category  # pylint: disable=import-error

File ~/notebook_env/lib/python3.10/site-packages/tangled_up_in_unicode/__init__.py:1
----> 1 from tangled_up_in_unicode.tangled_up_in_unicode_14_0_0 import (
      2     name,
      3     decimal,
      4     digit,
      5     numeric,
      6     combining,
      7     mirrored,
      8     decomposition,
      9     category,
     10     bidirectional,
     11     east_asian_width,
     12     script,
     13     block,
     14     age,
     15     unidata_version,
     16     combining_long,
     17     category_long,
     18     bidirectional_long,
     19     east_asian_width_long,
     20     script_abbr,
     21     block_abbr,
     22     age_long,
     23     prop_list,
     24     titlecase,
     25     lowercase,
     26     uppercase,
     27 )
     29 __version__ = ""0.2.0""
     31 __all__ = [
     32     ""name"",
     33     ""decimal"",
   (...)
     57     ""__version__"",
     58 ]

File ~/notebook_env/lib/python3.10/site-packages/tangled_up_in_unicode/tangled_up_in_unicode_14_0_0.py:21
     19 from tangled_up_in_unicode.u14_0_0_data.derived_age_to_age_start import derived_age_to_age_start
     20 from tangled_up_in_unicode.u14_0_0_data.derived_age_to_age_end import derived_age_to_age_end
---> 21 from tangled_up_in_unicode.u14_0_0_data.unicode_data_to_name_start import unicode_data_to_name_start
     22 from tangled_up_in_unicode.u14_0_0_data.unicode_data_to_category_start import unicode_data_to_category_start
     23 from tangled_up_in_unicode.u14_0_0_data.unicode_data_to_category_end import unicode_data_to_category_end

File ~/notebook_env/lib/python3.10/site-packages/tangled_up_in_unicode/u14_0_0_data/unicode_data_to_name_start.py:77306
      1 unicode_data_to_name_start = {
      2     32: ""SPACE"",
      3     33: ""EXCLAMATION MARK"",
      4     34: ""QUOTATION MARK"",
      5     35: ""NUMBER SIGN"",
      6     36: ""DOLLAR SIGN"",
      7     37: ""PERCENT SIGN"",
      8     38: ""AMPERSAND"",
      9     39: ""APOSTROPHE"",
     10     40: ""LEFT PARENTHESIS"",
     11     41: ""RIGHT PARENTHESIS"",
     12     42: ""ASTERISK"",
     13     43: ""PLUS SIGN"",
.
.
.
143861 }

SystemError: unknown opcode
```

Please advise.  Thank you!

### Expected Behaviour

When I ran with Python 3.9.7 I received a normal profile in Jupyter Lab with no memory issues or errors.

### Data Description

My dataset is publicly available from [https://data.mendeley.com/datasets/6w4tzrs3yw](https://data.mendeley.com/datasets/6w4tzrs3yw).

### Code that reproduces the bug

```Python
import numpy as np
import pandas as pd
from ydata_profiling import ProfileReport

df = pd.read_excel('../data/mendeley.xlsx')
test_df = df.head(10)
test_profile = ProfileReport(df)
test_profile
```
```


### pandas-profiling version

v3.6.6

### Dependencies

```Text
(notebook_env) pop-osnotebook_env$ pip3 list
Package                  Version
------------------------ -----------
aiofiles                 22.1.0
aiosqlite                0.18.0
altair                   4.2.2
anyio                    3.6.2
argon2-cffi              21.3.0
argon2-cffi-bindings     21.2.0
arrow                    1.2.3
asttokens                2.2.1
attrs                    22.2.0
Babel                    2.12.1
backcall                 0.2.0
beautifulsoup4           4.12.0
bleach                   6.0.0
blinker                  1.5
blis                     0.7.9
cachetools               5.3.0
catalogue                2.0.8
certifi                  2022.12.7
cffi                     1.15.1
charset-normalizer       3.1.0
click                    8.1.3
cloudpickle              2.2.1
comm                     0.1.3
confection               0.0.4
contourpy                1.0.7
cycler                   0.11.0
cymem                    2.0.7
dask                     2023.3.2
debugpy                  1.6.6
decorator                5.1.1
defusedxml               0.7.1
entrypoints              0.4
et-xmlfile               1.1.0
executing                1.2.0
fastjsonschema           2.16.3
fonttools                4.39.2
fqdn                     1.5.1
fsspec                   2023.3.0
gitdb                    4.0.10
GitPython                3.1.31
greenlet                 2.0.2
htmlmin                  0.1.12
idna                     3.4
ImageHash                4.3.1
importlib-metadata       6.1.0
interchange              2021.0.4
ipykernel                6.22.0
ipython                  8.11.0
ipython-genutils         0.2.0
ipywidgets               8.0.5
isoduration              20.11.0
jedi                     0.18.2
Jinja2                   3.1.2
joblib                   1.2.0
json5                    0.9.11
jsonpointer              2.3
jsonschema               4.17.3
jupyter_client           8.1.0
jupyter_core             5.3.0
jupyter-events           0.6.3
jupyter_server           2.5.0
jupyter_server_fileid    0.8.0
jupyter_server_terminals 0.4.4
jupyter_server_ydoc      0.8.0
jupyter-ydoc             0.2.3
jupyterlab               3.6.2
jupyterlab-pygments      0.2.2
jupyterlab_server        2.21.0
jupyterlab-widgets       3.0.6
kiwisolver               1.4.4
langcodes                3.3.0
locket                   1.0.0
markdown-it-py           2.2.0
MarkupSafe               2.1.2
matplotlib               3.6.3
matplotlib-inline        0.1.6
mdurl                    0.1.2
mistune                  2.0.5
monotonic                1.6
multimethod              1.9.1
murmurhash               1.0.9
nbclassic                0.5.3
nbclient                 0.7.2
nbconvert                7.2.10
nbformat                 5.8.0
neo4j                    5.6.0
nest-asyncio             1.5.6
networkx                 3.0
notebook                 6.5.3
notebook_shim            0.2.2
numpy                    1.23.5
openpyxl                 3.1.2
packaging                23.0
pandas                   1.5.3
pandas-profiling         3.6.6
pandocfilters            1.5.0
pansi                    2020.7.3
parso                    0.8.3
partd                    1.3.0
pathy                    0.10.1
patsy                    0.5.3
pexpect                  4.8.0
phik                     0.12.3
pickleshare              0.7.5
Pillow                   9.4.0
pip                      22.0.2
platformdirs             3.2.0
preshed                  3.0.8
prometheus-client        0.16.0
prompt-toolkit           3.0.38
protobuf                 3.20.3
psutil                   5.9.4
ptyprocess               0.7.0
pure-eval                0.2.2
py2neo                   2021.2.3
pyarrow                  11.0.0
pycparser                2.21
pydantic                 1.10.7
pydeck                   0.8.0
Pygments                 2.14.0
Pympler                  1.0.1
pyparsing                3.0.9
pyrsistent               0.19.3
python-dateutil          2.8.2
python-json-logger       2.0.7
pytz                     2023.2
pytz-deprecation-shim    0.1.0.post0
PyWavelets               1.4.1
PyYAML                   6.0
pyzmq                    25.0.2
regex                    2023.3.23
requests                 2.28.2
rfc3339-validator        0.1.4
rfc3986-validator        0.1.1
rich                     13.3.2
scikit-learn             1.2.2
scipy                    1.9.3
seaborn                  0.12.2
semver                   2.13.0
Send2Trash               1.8.0
setuptools               59.6.0
six                      1.16.0
smart-open               6.3.0
smmap                    5.0.0
sniffio                  1.3.0
soupsieve                2.4
spacy                    3.5.1
spacy-legacy             3.0.12
spacy-loggers            1.0.4
SQLAlchemy               2.0.7
srsly                    2.4.6
stack-data               0.6.2
statsmodels              0.13.5
streamlit                1.20.0
tangled-up-in-unicode    0.2.0
terminado                0.17.1
thinc                    8.1.9
threadpoolctl            3.1.0
tinycss2                 1.2.1
toml                     0.10.2
tomli                    2.0.1
toolz                    0.12.0
tornado                  6.2
tqdm                     4.64.1
traitlets                5.9.0
typeguard                2.13.3
typer                    0.7.0
typing_extensions        4.5.0
tzdata                   2023.2
tzlocal                  4.3
uri-template             1.2.0
urllib3                  1.26.15
validators               0.20.0
visions                  0.7.5
wasabi                   1.1.1
watchdog                 3.0.0
wcwidth                  0.2.6
webcolors                1.12
webencodings             0.5.1
websocket-client         1.5.1
wheel                    0.40.0
widgetsnbextension       4.0.6
y-py                     0.5.9
ydata-profiling          4.1.1
ypy-websocket            0.8.2
zipp                     3.15.0
```
```


### OS

Pop_OS! 22.04

### Checklist

- [X] There is not yet another bug report for this issue in the [issue tracker](https://github.com/ydataai/pandas-profiling/issues)
- [X] The problem is reproducible from this bug report. [This guide](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) can help to craft a minimal bug report.
- [X] The issue has not been resolved by the entries listed under [Common Issues](https://pandas-profiling.ydata.ai/docs/master/pages/support_contrib/common_issues.html).
"
sdpython/jyquickhelper,"Make it work with Jupyter Lab: https://github.com/sdpython/jyquickhelper/issues/11
Description: If you want to have your work expanded to Jupyter Lab, maybe you can use this:

https://github.com/gbrault/jpjwidgets

and/or use this:

https://github.com/gbrault/ipybus

"
sdpython/jyquickhelper,"Wrong name in the doc for adding notebook menu: https://github.com/sdpython/jyquickhelper/issues/10
Description: Hi, 
The following command from the doc: 
```
from jyquickhelper import add_menu_notebook
add_menu_notebook()
```

will raise an error, since the function is add_notebook_menu
;) 

"
sdpython/jyquickhelper,"create an extension for jupyter lab (javascript does not run on jupyter lab): https://github.com/sdpython/jyquickhelper/issues/8
Description: None

"
sdpython/jyquickhelper,"add notebook on vis.js: https://github.com/sdpython/jyquickhelper/issues/7
Description: None

"
sdpython/jyquickhelper,"add a function to show diagram in notebooks based on viz.js: https://github.com/sdpython/jyquickhelper/issues/5
Description: None

"
sdpython/jyquickhelper,"add a notebook on vizjs: https://github.com/sdpython/jyquickhelper/issues/4
Description: None

"
sdpython/jyquickhelper,"add notebook on mermaid: https://github.com/sdpython/jyquickhelper/issues/3
Description: None
"
finos/perspective,"New `playwright` test suite: https://github.com/finos/perspective/pull/2192
Description: Forked from #2170 

* Ports _all_ tests and removes `jest` entirely.
* Archives snapshot files.
* Splits ""standard"" tests to individual runtimes.
* Disables jupyterlab (accidently everywhere) - but this should only run during the `test_js_and_python` phase.
* Force reload browser between tests (otherwise test order impacts the results, as the configs are only partial settings).
* Fixes broken tests, `ts-ignore` annotations, test bugs & general cleanup.

There are still test failures.  Some are `style` attributes with `px` position fields, some are mis-placed text-overflow ellipses, some I have not debugged.  I'm not sure how to deal with the ellipsis - perhaps disable them globally via CSS rule in the test? - but the `style` attribtues can be cleaned with regex as we do for other fields.  Otherwise, this seems stable now.

I suspect a lot of weight can be cut from the `build.yml` with this change, I'd be interested in this cleanup landing as well.
"
microsoft/responsible-ai-toolbox,"Tree Map not working in error analysis: https://github.com/microsoft/responsible-ai-toolbox/issues/2028
Description: **Describe the bug**
I couldnt get the tree map in Error analysis.

**To Reproduce**
I followed the steps in the [notebook](https://github.com/microsoft/responsible-ai-toolbox/blob/main/notebooks/individual-dashboards/erroranalysis-dashboard/erroranalysis-interpretability-dashboard-census.ipynb)

**Screenshots**
<img width=""1237"" alt=""image"" src=""https://user-images.githubusercontent.com/48387818/230582199-3cc1f43e-4be6-40d3-bce8-e758b6d2e7fc.png"">

**Desktop (please complete the following information):**

- OS: Ubuntu 18.04
- Browser chrome
- Python version: 3.8
- raiwidgets and responsibleai package versions [e.g. 0.19.0]

To get the package versions please run in your command line:

```
Name: raiwidgets
Version: 0.26.0
Name: responsibleai
Version: 0.26.0
```
Please help if i am missing anything


"
microsoft/responsible-ai-toolbox,"release raiwidgets and responsibleai 0.26.0: https://github.com/microsoft/responsible-ai-toolbox/pull/2022
Description: ## Description

## v0.26.0

- educational materials
  - Update notebooks for categorical features to pass through FeatureMetadata by @tongyu-microsoft in https://github.com/microsoft/responsible-ai-toolbox/pull/2011
- new features
  - ## Responsible AI Dashboard
    - Adding Object Detection Fridge Data by @natalie-isak in https://github.com/microsoft/responsible-ai-toolbox/pull/1998
  - ## RAIInsights
    - Support forecasting in RAIInsights by @romanlutz in https://github.com/microsoft/responsible-ai-toolbox/pull/1948
  - ## Model Overview
    - [Model Overview] Metrics Template support with OD-specific optional args by @Advitya17 in https://github.com/microsoft/responsible-ai-toolbox/pull/2002
    - Object Detection Model Overview Flask template by @Advitya17 in https://github.com/microsoft/responsible-ai-toolbox/pull/2004
    - Model Overview: Object Detection Widgets for Aggregate Methods, Class selection, & IoU Threshold by @Advitya17 in https://github.com/microsoft/responsible-ai-toolbox/pull/1997
- bug fixes and tests
  - ## Responsible AI Dashboard
    - Forecasting: UI code adjustments to renamed datetime_features and time_series_id_features by @romanlutz in https://github.com/microsoft/responsible-ai-toolbox/pull/1993
    - [Refactor]1.Add DatasetCohort 2. Move FeatureFlights to core by @RubyZ10 in https://github.com/microsoft/responsible-ai-toolbox/pull/2003
    - [Refactor]Add columnRanges and modelType to ModelAssessmentContext by @RubyZ10 in https://github.com/microsoft/responsible-ai-toolbox/pull/2006
  - ## RAIInsights
    - Add validations for model predictions by @gaugup in https://github.com/microsoft/responsible-ai-toolbox/pull/2008
    - Add validations if target column is included in `FeatureMetadata` by @gaugup in https://github.com/microsoft/responsible-ai-toolbox/pull/2013
    - try pickling model instead of failing based on state functions and change to UserConfigValidationException by @imatiach-msft in https://github.com/microsoft/responsible-ai-toolbox/pull/2014
    - update dependencies for rai-core-flask, raiwidgets and responsibleai packages by @imatiach-msft in https://github.com/microsoft/responsible-ai-toolbox/pull/2016
    - Fix deprecated DataFrame indexer type in Error Analysis package by @JarvisG495 in https://github.com/microsoft/responsible-ai-toolbox/pull/2019
  - ## Error Analysis
    - fix bug in tree view where slider params are not used by @imatiach-msft in https://github.com/microsoft/responsible-ai-toolbox/pull/1990
    - fix error analysis showing float values with many zeros for some classification task nodes by @imatiach-msft in https://github.com/microsoft/responsible-ai-toolbox/pull/1991
    - release erroranalysis v0.4.2 by @imatiach-msft in https://github.com/microsoft/responsible-ai-toolbox/pull/2000
    - update raiwidgets and responsibleai to erroranalysis 0.4.2 by @imatiach-msft in https://github.com/microsoft/responsible-ai-toolbox/pull/2001

## Checklist

<!--- Make sure to satisfy all the criteria listed below. -->

- [ ] I have added screenshots above for all UI changes.
- [ ] I have added e2e tests for all UI changes.
- [ ] Documentation was updated if it was needed.


"
microsoft/responsible-ai-toolbox,"Update notebooks for categorical features to pass through FeatureMetadata: https://github.com/microsoft/responsible-ai-toolbox/pull/2011
Description: Previously we have a PR where we deprecated the existing categorical_features arg with a note to switch to feature_metadata.categorical_features: https://github.com/microsoft/responsible-ai-toolbox/pull/1934

This PR updates the sample notebooks for customers to pass categorical_features via feature_metadata.categorical_features

## Checklist

- [ ] I have added screenshots above for all UI changes.
- [ ] I have added e2e tests for all UI changes.
- [ ] Documentation was updated if it was needed.

"
GeoscienceAustralia/dea-notebooks,"Need to mention adding new notebooks to README files in PR checklist: https://github.com/GeoscienceAustralia/dea-notebooks/issues/1040
Description: Adding new notebooks to the README files in each folder is required to make them appear in the DEA Docs User Guide. We should add this as a new step in the PR checklist.

"
GeoscienceAustralia/dea-notebooks,"Update functions to use `odc-geo` and add additional content to tests: https://github.com/GeoscienceAustralia/dea-notebooks/pull/1039
Description: ### Proposed changes


### Closes issues (optional)
- Closes Issue #1038 

### Checklist (replace `[ ]` with `[x]` to check off)
- [ ] Notebook created using the [DEA-notebooks template](https://github.com/GeoscienceAustralia/dea-notebooks/tree/develop)
- [ ] Remove any unused Python packages from `Load packages`
- [ ] Remove any unused/empty code cells
- [ ] Remove any guidance cells (e.g. `General advice`)
- [ ] Ensure that all code cells follow the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) for code. The `jupyterlab_code_formatter` tool can be used to format code cells to a consistent style: select each code cell, then click `Edit` and then one of the `Apply X Formatter` options (`YAPF` or `Black` are recommended).
- [ ] Include relevant tags in the final notebook cell (refer to the [DEA Tags Index](https://docs.dea.ga.gov.au/genindex.html), and re-use tags if possible)
- [ ] Clear all outputs, run notebook from start to finish, and save the notebook in the state where all cells have been sequentially evaluated
- [ ] Test notebook on both the `NCI` and `DEA Sandbox` (flag if not working as part of PR and ask for help to solve if needed)
- [ ] If applicable, update the `Notebook currently compatible with the NCI|DEA Sandbox environment only` line below the notebook title to reflect the environments the notebook is compatible with




"
GeoscienceAustralia/dea-notebooks,"Update DEA Tools functions to be compatible with `odc.geo`: https://github.com/GeoscienceAustralia/dea-notebooks/issues/1038
Description: [The `odc.geo` package](https://github.com/opendatacube/odc-geo) has been written to eventually replace how datacube deals with spatial information (i.e. reprojecting/warping/geotransforms etc).

Currently any spatially-aware code in DEA Notebooks accesses a datacube ""Geobox"" like this:
```
ds.geobox
```

To make the eventual transition to `odc.geo` smoother, we should update our DEA Tools functions to also support accessing spatial info via the `odc.geo` `.odc` accessor:
```
ds.odc.geobox
```

This would largely affect functions in the `dea_tools.spatial` module.

"
GeoscienceAustralia/dea-notebooks,"Replace conference paper with Wetlands paper: https://github.com/GeoscienceAustralia/dea-notebooks/pull/1034
Description: updated citation list with paper

### Proposed changes
Include a brief description of the changes being proposed, and why they are necessary.

### Closes issues (optional)
- Closes Issue #000
- Closes Issue #000

### Checklist (replace `[ ]` with `[x]` to check off)
- [ ] Notebook created using the [DEA-notebooks template](https://github.com/GeoscienceAustralia/dea-notebooks/tree/develop)
- [ ] Remove any unused Python packages from `Load packages`
- [ ] Remove any unused/empty code cells
- [ ] Remove any guidance cells (e.g. `General advice`)
- [ ] Ensure that all code cells follow the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) for code. The `jupyterlab_code_formatter` tool can be used to format code cells to a consistent style: select each code cell, then click `Edit` and then one of the `Apply X Formatter` options (`YAPF` or `Black` are recommended).
- [ ] Include relevant tags in the final notebook cell (refer to the [DEA Tags Index](https://docs.dea.ga.gov.au/genindex.html), and re-use tags if possible)
- [ ] Clear all outputs, run notebook from start to finish, and save the notebook in the state where all cells have been sequentially evaluated
- [ ] Test notebook on both the `NCI` and `DEA Sandbox` (flag if not working as part of PR and ask for help to solve if needed)
- [ ] If applicable, update the `Notebook currently compatible with the NCI|DEA Sandbox environment only` line below the notebook title to reflect the environments the notebook is compatible with




"
GeoscienceAustralia/dea-notebooks,"Merge to stable to add coastal transect notebook to Sandbox: https://github.com/GeoscienceAustralia/dea-notebooks/pull/1033
Description: None

"
GeoscienceAustralia/dea-notebooks,"Extract coastline app from product notebook and move to `Interactive_apps`: https://github.com/GeoscienceAustralia/dea-notebooks/pull/1032
Description: ### Proposed changes
The coastline transect extraction tool currently lives at the bottom of the `DEA_products/DEA_Coastlines.ipynb` notebook.

Now that we have a new `Interactive_apps` folder, this PR extracts the tool and moves it there instead as a new `Interactive_apps/Coastal_transects.ipynb` notebook!

![image](https://user-images.githubusercontent.com/17680388/231336462-a4f37637-4f3a-4e02-915c-f66615f56dcb.png)


### Checklist (replace `[ ]` with `[x]` to check off)
- [x] Notebook created using the [DEA-notebooks template](https://github.com/GeoscienceAustralia/dea-notebooks/tree/develop)
- [x] Remove any unused Python packages from `Load packages`
- [x] Remove any unused/empty code cells
- [x] Remove any guidance cells (e.g. `General advice`)
- [x] Ensure that all code cells follow the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) for code. The `jupyterlab_code_formatter` tool can be used to format code cells to a consistent style: select each code cell, then click `Edit` and then one of the `Apply X Formatter` options (`YAPF` or `Black` are recommended).
- [x] Include relevant tags in the final notebook cell (refer to the [DEA Tags Index](https://docs.dea.ga.gov.au/genindex.html), and re-use tags if possible)
- [x] Clear all outputs, run notebook from start to finish, and save the notebook in the state where all cells have been sequentially evaluated
- [x] Test notebook on both the `NCI` and `DEA Sandbox` (flag if not working as part of PR and ask for help to solve if needed)
- [x] If applicable, update the `Notebook currently compatible with the NCI|DEA Sandbox environment only` line below the notebook title to reflect the environments the notebook is compatible with




"
GeoscienceAustralia/dea-notebooks,"Merge develop to stable: https://github.com/GeoscienceAustralia/dea-notebooks/pull/1031
Description: ### Proposed changes
Brings directory restructure to Sandbox and User Guide

### Closes issues (optional)
- Closes Issue #000
- Closes Issue #000

### Checklist (replace `[ ]` with `[x]` to check off)
- [ ] Notebook created using the [DEA-notebooks template](https://github.com/GeoscienceAustralia/dea-notebooks/tree/develop)
- [ ] Remove any unused Python packages from `Load packages`
- [ ] Remove any unused/empty code cells
- [ ] Remove any guidance cells (e.g. `General advice`)
- [ ] Ensure that all code cells follow the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) for code. The `jupyterlab_code_formatter` tool can be used to format code cells to a consistent style: select each code cell, then click `Edit` and then one of the `Apply X Formatter` options (`YAPF` or `Black` are recommended).
- [ ] Include relevant tags in the final notebook cell (refer to the [DEA Tags Index](https://docs.dea.ga.gov.au/genindex.html), and re-use tags if possible)
- [ ] Clear all outputs, run notebook from start to finish, and save the notebook in the state where all cells have been sequentially evaluated
- [ ] Test notebook on both the `NCI` and `DEA Sandbox` (flag if not working as part of PR and ask for help to solve if needed)
- [ ] If applicable, update the `Notebook currently compatible with the NCI|DEA Sandbox environment only` line below the notebook title to reflect the environments the notebook is compatible with




"
GeoscienceAustralia/dea-notebooks,"Fix geocoding func to still work if geocoding service is down: https://github.com/GeoscienceAustralia/dea-notebooks/pull/1030
Description: ### Proposed changes
A small fix while we're waiting to merge `develop` into `stable` for the directory re-structure: I noticed that the interactive app notebooks are currently failing because the geocoding service is down. This PR allows the function to work even when this happens, by returning coordinates instead of a geocoded place name. 


"
GeoscienceAustralia/dea-notebooks,"Update USAGE.rst: https://github.com/GeoscienceAustralia/dea-notebooks/pull/1029
Description: I've added a recently accepted manuscript where I used DEA to calulcate Geomedians and show landscape changes. And the relevant years where we've used DEA sandbox in our remote sensing topics at Flidners university. Additionally added a conference presentation from AEO22 from Earth Observation Australia 2022 in Brisbane,

### Proposed changes
Include a brief description of the changes being proposed, and why they are necessary.

### Closes issues (optional)
- Closes Issue #000
- Closes Issue #000

### Checklist (replace `[ ]` with `[x]` to check off)
- [ ] Notebook created using the [DEA-notebooks template](https://github.com/GeoscienceAustralia/dea-notebooks/tree/develop)
- [ ] Remove any unused Python packages from `Load packages`
- [ ] Remove any unused/empty code cells
- [ ] Remove any guidance cells (e.g. `General advice`)
- [ ] Ensure that all code cells follow the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) for code. The `jupyterlab_code_formatter` tool can be used to format code cells to a consistent style: select each code cell, then click `Edit` and then one of the `Apply X Formatter` options (`YAPF` or `Black` are recommended).
- [ ] Include relevant tags in the final notebook cell (refer to the [DEA Tags Index](https://docs.dea.ga.gov.au/genindex.html), and re-use tags if possible)
- [ ] Clear all outputs, run notebook from start to finish, and save the notebook in the state where all cells have been sequentially evaluated
- [ ] Test notebook on both the `NCI` and `DEA Sandbox` (flag if not working as part of PR and ask for help to solve if needed)
- [ ] If applicable, update the `Notebook currently compatible with the NCI|DEA Sandbox environment only` line below the notebook title to reflect the environments the notebook is compatible with




"
GeoscienceAustralia/dea-notebooks,"Fix `pixel_tides` issues when `ds` has no time dimension: https://github.com/GeoscienceAustralia/dea-notebooks/pull/1028
Description: ### Proposed changes
This PR updates the `pixel_tides` function to better handle situations where an input array has no time dimension. 

The function failed in this scenario:

- You pass in a dataset `ds` with no time dimension, and a set of custom times via the `times` param
- The function models tides into a pandas dataframe, then tries to reshape and transpose the data back into 3D xarray format by matching it to the original dimensions from `ds`
- Because the modelled tides dataframe has multiple timesteps but the original `ds` has no time dim, the [transpose steps fails](https://github.com/GeoscienceAustralia/dea-notebooks/compare/pixel_tides_update?expand=1#diff-9a8d3841a66e6cc352119d7194f661f0ac4be96b059c0ea527640d30d65e55d9L605) when trying to reshape x/y/time data into x/y dimensions

![image](https://user-images.githubusercontent.com/17680388/230244082-a611ce7b-bc99-4609-a3b9-0b4f90126ff7.png)


Two fixes to address this:

- Better error catching to make the function fail completely if a dataset with no time dimension is passed and no custom times are provided
- Hard code the list of dimensions instead of getting them from the input `ds`. This ensures they are in the order expected by the re-projection step.

![image](https://user-images.githubusercontent.com/17680388/230248486-e34153c6-7b62-4b41-b7e6-c67df4e6ba42.png)


Have also cleaned up some un-needed OTPS code in one of the new FES2014 funcs.


"
GeoscienceAustralia/dea-notebooks,"Update DEA Notebooks directory structure: https://github.com/GeoscienceAustralia/dea-notebooks/pull/1027
Description: ### Proposed changes
This PR makes it easier for our users to discover our interactive app notebooks by extracting them into their own subfolder in DEA Notebooks. That way, users can easily find content that doesn't require them to have to code.

During the recent DEA Notebooks hackathon, we implemented a new folder structure that maintains our flow from ""simple"" > ""complex"" in alphabetical order:

- **Beginners_guide**
- **DEA_products** (rename from ""DEA_datasets"" to match terminology used on the DEA website and CMI)
- **Interactive_apps** (interactive widgets moved from other folders)
- **How_to_guides** (rename of ""Frequently_used_code"" to move this after ""Interactive apps"" alphabetically)
- **Real_world_examples**

This PR also addresses #1024 by removing the **Scientific_workflows** directory, which has been preserved in a protected branch here: https://github.com/GeoscienceAustralia/dea-notebooks/tree/scientific_workflows_archive

This new structure also support the possible future inclusion of an **Exercises** folder for more tutorial-style content similar to that delivered in the ANU DEA training courses.

### Closes issues (optional)
- Closes Issue #1023 
- Closes Issue #1024



"
GeoscienceAustralia/dea-notebooks,"Remove and archive Scientific workflows folder by saving it to a new branch 🧪: https://github.com/GeoscienceAustralia/dea-notebooks/issues/1024
Description: As this repository matures, we need to ensure that all content on the public facing `develop` or `stable` branches is fully reproducible, so that users don't get confused by code that doesn't run on their systems.

The `Scientific_workflows` folder contains non-reproducible, complex project specific material which would live better in a branch than on the main branches of DEA Notebooks.

We should remove that folder from `develop` and `stable`, and archive it as a new branch (e.g. `scientific_workflows`) so that it can be found easily, but be harder to stumble upon by beginner users.

"
GeoscienceAustralia/dea-notebooks,"MAJOR: Update DEA Notebooks directory structure 📂: https://github.com/GeoscienceAustralia/dea-notebooks/issues/1023
Description: One thing that would be nice would be to extract interactive app notebooks into their own subfolder in DEA Notebooks. That way, users can easily find content that doesn't require them to have to code. 

A structure like this could work while still maintaining our flow from ""simple"" > ""complex"" and alphabetical order:

- **Beginners_guide**
- **DEA_products** (rename from ""DEA_datasets"")
- **Interactive_apps** (interactive widgets moved from other folders)
- **How_to_guides** (rename of ""Frequently_used_code"" to move this after ""Interactive apps"" alphabetically)
- **Real_world_examples**

This could also support the possible future inclusion of an **Exercises** folder for more tutorial-style content that Pablo delivered in his DEA training courses...

"
GeoscienceAustralia/dea-notebooks,"Solve the mystery of why only some DEA Tools modules render in the User Guide... 🕵: https://github.com/GeoscienceAustralia/dea-notebooks/issues/1021
Description: The User Guide renders our DEA Tools Python package here:
https://docs.dea.ga.gov.au/notebooks/Tools/index.html

But this list only includes some of the DEA Tools modules (https://github.com/GeoscienceAustralia/dea-notebooks/tree/develop/Tools/dea_tools), not all. **Why are some missing?**

Last time we looked into this, the theory was it was something to do with this: https://github.com/GeoscienceAustralia/dea-docs/blob/master/conf.py#L30-L89

"
GeoscienceAustralia/dea-notebooks,"Readme update: https://github.com/GeoscienceAustralia/dea-notebooks/pull/1019
Description: ### Proposed changes
Add encouragement to contribute to the readme and a link to the notebooks template




"
GeoscienceAustralia/dea-notebooks,"Draft of Near Real-time Burnt Area Mapping DEA notebook for review: https://github.com/GeoscienceAustralia/dea-notebooks/pull/1018
Description: ### Proposed changes
New notebook.

### Concerns
Kernel crashes when user requests a large AOI and a large number of images are extracted. Mostly occurs in the Baseline load_ard extract. Not sure if I can use Dask to solve as I need to visualise the images in the following plot.

### Checklist (replace `[ ]` with `[x]` to check off)
- [ ] Notebook created using the [DEA-notebooks template](https://github.com/GeoscienceAustralia/dea-notebooks/tree/develop)
- [ ] Remove any unused Python packages from `Load packages`
- [ ] Remove any unused/empty code cells
- [ ] Remove any guidance cells (e.g. `General advice`)
- [ ] Ensure that all code cells follow the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) for code. The `jupyterlab_code_formatter` tool can be used to format code cells to a consistent style: select each code cell, then click `Edit` and then one of the `Apply X Formatter` options (`YAPF` or `Black` are recommended).
- [ ] Include relevant tags in the final notebook cell (refer to the [DEA Tags Index](https://docs.dea.ga.gov.au/genindex.html), and re-use tags if possible)
- [ ] Clear all outputs, run notebook from start to finish, and save the notebook in the state where all cells have been sequentially evaluated
- [ ] Test notebook on both the `NCI` and `DEA Sandbox` (flag if not working as part of PR and ask for help to solve if needed)
- [ ] If applicable, update the `Notebook currently compatible with the NCI|DEA Sandbox environment only` line below the notebook title to reflect the environments the notebook is compatible with




"
GeoscienceAustralia/dea-notebooks,"PR template could include instructions on where to put supplementary data: https://github.com/GeoscienceAustralia/dea-notebooks/issues/1017
Description: When writing notebooks, the PR template could suggest where to put your supplementary data, and that you have checked that the links have been moved to the correct folder


"
GeoscienceAustralia/dea-notebooks,"Ignore markdown and RST files from integration tests: https://github.com/GeoscienceAustralia/dea-notebooks/pull/1015
Description: ### Proposed changes
This PR updates the DEA Notebooks integration tests to ignore any pushes/PRs that modify RST or MD files, or any files in the `.github/` folder (excluding the integration test YML itself):

```
    paths-ignore:
      - '**/*.md' # ignore markdown files
      - '**/*.rst' # ignore restructured text files
      - '.github/**' # ignore anything in .github folder
      - '!.github/workflows/test_notebooks.yml' # except test_notebooks.yml
```

"
GeoscienceAustralia/dea-notebooks,"test with release/stable sandbox image: https://github.com/GeoscienceAustralia/dea-notebooks/pull/1013
Description: ### Proposed changes
test with `sandbox:stable`

### Closes issues (optional)


### Checklist (replace `[ ]` with `[x]` to check off)
- [ ] Notebook created using the [DEA-notebooks template](https://github.com/GeoscienceAustralia/dea-notebooks/tree/develop)
- [ ] Remove any unused Python packages from `Load packages`
- [ ] Remove any unused/empty code cells
- [ ] Remove any guidance cells (e.g. `General advice`)
- [ ] Ensure that all code cells follow the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) for code. The `jupyterlab_code_formatter` tool can be used to format code cells to a consistent style: select each code cell, then click `Edit` and then one of the `Apply X Formatter` options (`YAPF` or `Black` are recommended).
- [ ] Include relevant tags in the final notebook cell (refer to the [DEA Tags Index](https://docs.dea.ga.gov.au/genindex.html), and re-use tags if possible)
- [ ] Clear all outputs, run notebook from start to finish, and save the notebook in the state where all cells have been sequentially evaluated
- [ ] Test notebook on both the `NCI` and `DEA Sandbox` (flag if not working as part of PR and ask for help to solve if needed)
- [ ] If applicable, update the `Notebook currently compatible with the NCI|DEA Sandbox environment only` line below the notebook title to reflect the environments the notebook is compatible with




"
GeoscienceAustralia/dea-notebooks,"simplify the test workflow (#1008): https://github.com/GeoscienceAustralia/dea-notebooks/pull/1012
Description: * simplify the test workflow

* give access to ecr sandbox repo

* remove develop installation of dea-tools

* debug installation dea-tools

* remove writing __version__.py

* build dea-tools as wheel

* resolve annoying volume access issue

* Update terminology

* add comments

* fix the wrong path

---------

### Proposed changes
Include a brief description of the changes being proposed, and why they are necessary.

### Closes issues (optional)
N/A

### Checklist (replace `[ ]` with `[x]` to check off)
- [ ] Notebook created using the [DEA-notebooks template](https://github.com/GeoscienceAustralia/dea-notebooks/tree/develop)
- [ ] Remove any unused Python packages from `Load packages`
- [ ] Remove any unused/empty code cells
- [ ] Remove any guidance cells (e.g. `General advice`)
- [ ] Ensure that all code cells follow the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) for code. The `jupyterlab_code_formatter` tool can be used to format code cells to a consistent style: select each code cell, then click `Edit` and then one of the `Apply X Formatter` options (`YAPF` or `Black` are recommended).
- [ ] Include relevant tags in the final notebook cell (refer to the [DEA Tags Index](https://docs.dea.ga.gov.au/genindex.html), and re-use tags if possible)
- [ ] Clear all outputs, run notebook from start to finish, and save the notebook in the state where all cells have been sequentially evaluated
- [ ] Test notebook on both the `NCI` and `DEA Sandbox` (flag if not working as part of PR and ask for help to solve if needed)
- [ ] If applicable, update the `Notebook currently compatible with the NCI|DEA Sandbox environment only` line below the notebook title to reflect the environments the notebook is compatible with




"
GeoscienceAustralia/dea-notebooks,"Merge develop to stable: https://github.com/GeoscienceAustralia/dea-notebooks/pull/1011
Description: ### Proposed changes
Include a brief description of the changes being proposed, and why they are necessary.

### Closes issues (optional)
- Closes Issue #000
- Closes Issue #000

### Checklist (replace `[ ]` with `[x]` to check off)
- [ ] Notebook created using the [DEA-notebooks template](https://github.com/GeoscienceAustralia/dea-notebooks/tree/develop)
- [ ] Remove any unused Python packages from `Load packages`
- [ ] Remove any unused/empty code cells
- [ ] Remove any guidance cells (e.g. `General advice`)
- [ ] Ensure that all code cells follow the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) for code. The `jupyterlab_code_formatter` tool can be used to format code cells to a consistent style: select each code cell, then click `Edit` and then one of the `Apply X Formatter` options (`YAPF` or `Black` are recommended).
- [ ] Include relevant tags in the final notebook cell (refer to the [DEA Tags Index](https://docs.dea.ga.gov.au/genindex.html), and re-use tags if possible)
- [ ] Clear all outputs, run notebook from start to finish, and save the notebook in the state where all cells have been sequentially evaluated
- [ ] Test notebook on both the `NCI` and `DEA Sandbox` (flag if not working as part of PR and ask for help to solve if needed)
- [ ] If applicable, update the `Notebook currently compatible with the NCI|DEA Sandbox environment only` line below the notebook title to reflect the environments the notebook is compatible with




"
GeoscienceAustralia/dea-notebooks,"Remove accidently committed autoreload in Coastlines notebook: https://github.com/GeoscienceAustralia/dea-notebooks/pull/1010
Description: ### Proposed changes
Include a brief description of the changes being proposed, and why they are necessary.

### Closes issues (optional)
- Closes Issue #000
- Closes Issue #000

### Checklist (replace `[ ]` with `[x]` to check off)
- [ ] Notebook created using the [DEA-notebooks template](https://github.com/GeoscienceAustralia/dea-notebooks/tree/develop)
- [ ] Remove any unused Python packages from `Load packages`
- [ ] Remove any unused/empty code cells
- [ ] Remove any guidance cells (e.g. `General advice`)
- [ ] Ensure that all code cells follow the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) for code. The `jupyterlab_code_formatter` tool can be used to format code cells to a consistent style: select each code cell, then click `Edit` and then one of the `Apply X Formatter` options (`YAPF` or `Black` are recommended).
- [ ] Include relevant tags in the final notebook cell (refer to the [DEA Tags Index](https://docs.dea.ga.gov.au/genindex.html), and re-use tags if possible)
- [ ] Clear all outputs, run notebook from start to finish, and save the notebook in the state where all cells have been sequentially evaluated
- [ ] Test notebook on both the `NCI` and `DEA Sandbox` (flag if not working as part of PR and ask for help to solve if needed)
- [ ] If applicable, update the `Notebook currently compatible with the NCI|DEA Sandbox environment only` line below the notebook title to reflect the environments the notebook is compatible with



"
google/brax,"PPO returns nan with multiple GPU: https://github.com/google/brax/issues/332
Description: PPO training returns nan when using multiple GPU. Forcing t use one GPU works fine. I just ran the exactly same code in training code in  [Brax Training](https://colab.research.google.com/github/google/brax/blob/main/notebooks/training.ipynb). Can somebody help to try it? Thanks!

"
google/brax,"TracerArrayConversionError when jitting env.step: https://github.com/google/brax/issues/321
Description: I am new to brax. When I try to perform an environment step using a jitted version of env.step (using code taken from the brax environments notebook), I get a TracerArrayConversionError. Any idea how I can resolve this?

Code:

```
from brax import envs
from brax import jumpy as jp
import jax
from jax import numpy as jnp

environment = ""reacher""
env = envs.create(env_name=environment)
state = env.reset(rng=jp.random_prngkey(seed=0))

jit_env_step = jax.jit(env.step)
state = jit_env_step(state, jnp.ones((env.action_size,)))
```
Traceback:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback
    return fun(*args, **kwargs)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/jax/_src/pjit.py"", line 235, in cache_miss
    outs, out_flat, out_tree, args_flat = _python_pjit_helper(
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/jax/_src/pjit.py"", line 179, in _python_pjit_helper
    args_flat, _, params, in_tree, out_tree, _ = infer_params_fn(
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/jax/_src/api.py"", line 442, in infer_params
    return pjit.common_infer_params(pjit_info_args, *args, **kwargs)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/jax/_src/pjit.py"", line 515, in common_infer_params
    jaxpr, consts, canonicalized_out_shardings_flat = _pjit_jaxpr(
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/jax/_src/pjit.py"", line 967, in _pjit_jaxpr
    jaxpr, final_consts, global_out_avals = _create_pjit_jaxpr(
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/jax/_src/linear_util.py"", line 301, in memoized_fun
    ans = call(fun, *args)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/jax/_src/pjit.py"", line 925, in _create_pjit_jaxpr
    jaxpr, global_out_avals, consts = pe.trace_to_jaxpr_dynamic(
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/jax/_src/profiler.py"", line 314, in wrapper
    return func(*args, **kwargs)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/jax/interpreters/partial_eval.py"", line 2029, in trace_to_jaxpr_dynamic
    jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/jax/interpreters/partial_eval.py"", line 2046, in trace_to_subjaxpr_dynamic
    ans = fun.call_wrapped(*in_tracers_)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/jax/_src/linear_util.py"", line 165, in call_wrapped
    ans = self.f(*args, **dict(self.params, **kwargs))
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/envs/wrappers.py"", line 138, in step
    state = self.env.step(state, action)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/envs/wrappers.py"", line 110, in step
    state, rewards = jp.scan(f, state, (), self.action_repeat)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/jumpy.py"", line 115, in scan
    carry, y = f(carry, jax.tree_util.tree_unflatten(xs_tree, xs_slice))
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/envs/wrappers.py"", line 107, in f
    nstate = self.env.step(state, action)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/envs/reacher.py"", line 177, in step
    qp, info = self.sys.step(state.qp, action)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/physics/system.py"", line 247, in step
    return step_funs[self.config.dynamics_mode](qp, act)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/physics/system.py"", line 324, in _pbd_step
    (qp, info), _ = jp.scan(substep, (qp, info), (), self.config.substeps // 2)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/jumpy.py"", line 115, in scan
    carry, y = f(carry, jax.tree_util.tree_unflatten(xs_tree, xs_slice))
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/physics/system.py"", line 268, in substep
    dp_a = sum([a.apply(qp, act) for a in self.actuators], zero)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/physics/system.py"", line 268, in <listcomp>
    dp_a = sum([a.apply(qp, act) for a in self.actuators], zero)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/physics/actuators.py"", line 65, in apply
    dang_p, dang_c = jp.vmap(type(self).apply_reduced)(self, act, qp_p, qp_c)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/jumpy.py"", line 87, in _batched
    rets.append(fun(*b_args))
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/physics/actuators.py"", line 107, in apply_reduced
    torque = jp.sum(jp.vmap(jp.multiply)(axis, torque), axis=0)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/jumpy.py"", line 89, in _batched
    return jax.tree_util.tree_map(lambda *x: onp.stack(x), *rets)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/jax/_src/tree_util.py"", line 209, in tree_map
    return treedef.unflatten(f(*xs) for xs in zip(*all_leaves))
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/jax/_src/tree_util.py"", line 209, in <genexpr>
    return treedef.unflatten(f(*xs) for xs in zip(*all_leaves))
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/jumpy.py"", line 89, in <lambda>
    return jax.tree_util.tree_map(lambda *x: onp.stack(x), *rets)
  File ""<__array_function__ internals>"", line 200, in stack
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/numpy/core/shape_base.py"", line 458, in stack
    arrays = [asanyarray(arr) for arr in arrays]
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/numpy/core/shape_base.py"", line 458, in <listcomp>
    arrays = [asanyarray(arr) for arr in arrays]
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/jax/_src/core.py"", line 575, in __array__
    raise TracerArrayConversionError(self)
jax._src.traceback_util.UnfilteredStackTrace: jax._src.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=1/0)>
The error occurred while tracing the function step at /Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/envs/wrappers.py:132 for jit. This concrete value was not available in Python because it depends on the values of the arguments state.qp.rot and action.
See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError

The stack trace below excludes JAX-internal frames.
The preceding is the original exception that occurred, unmodified.

--------------------

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/envs/wrappers.py"", line 138, in step
    state = self.env.step(state, action)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/envs/wrappers.py"", line 110, in step
    state, rewards = jp.scan(f, state, (), self.action_repeat)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/jumpy.py"", line 115, in scan
    carry, y = f(carry, jax.tree_util.tree_unflatten(xs_tree, xs_slice))
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/envs/wrappers.py"", line 107, in f
    nstate = self.env.step(state, action)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/envs/reacher.py"", line 177, in step
    qp, info = self.sys.step(state.qp, action)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/physics/system.py"", line 247, in step
    return step_funs[self.config.dynamics_mode](qp, act)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/physics/system.py"", line 324, in _pbd_step
    (qp, info), _ = jp.scan(substep, (qp, info), (), self.config.substeps // 2)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/jumpy.py"", line 115, in scan
    carry, y = f(carry, jax.tree_util.tree_unflatten(xs_tree, xs_slice))
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/physics/system.py"", line 268, in substep
    dp_a = sum([a.apply(qp, act) for a in self.actuators], zero)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/physics/system.py"", line 268, in <listcomp>
    dp_a = sum([a.apply(qp, act) for a in self.actuators], zero)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/physics/actuators.py"", line 65, in apply
    dang_p, dang_c = jp.vmap(type(self).apply_reduced)(self, act, qp_p, qp_c)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/jumpy.py"", line 87, in _batched
    rets.append(fun(*b_args))
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/physics/actuators.py"", line 107, in apply_reduced
    torque = jp.sum(jp.vmap(jp.multiply)(axis, torque), axis=0)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/jumpy.py"", line 89, in _batched
    return jax.tree_util.tree_map(lambda *x: onp.stack(x), *rets)
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/jumpy.py"", line 89, in <lambda>
    return jax.tree_util.tree_map(lambda *x: onp.stack(x), *rets)
  File ""<__array_function__ internals>"", line 200, in stack
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/numpy/core/shape_base.py"", line 458, in stack
    arrays = [asanyarray(arr) for arr in arrays]
  File ""/Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/numpy/core/shape_base.py"", line 458, in <listcomp>
    arrays = [asanyarray(arr) for arr in arrays]
jax._src.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=1/0)>
The error occurred while tracing the function step at /Users/James/opt/anaconda3/envs/hMPC/lib/python3.9/site-packages/brax/envs/wrappers.py:132 for jit. This concrete value was not available in Python because it depends on the values of the arguments state.qp.rot and action.
See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError
```

I am using brax 0.1.1 and jax 0.4.6.

Thanks.

"
google/brax,"Brax contact modeling results in non-physical behavior (and non-physical gradients): https://github.com/google/brax/issues/317
Description: Hi,

I'm noticing an issue in how Brax handles contact in both the `pbd` and `legacy_spring` dynamics modes, where **both the auto-diff computed gradient and the numerically-estimated gradient** will not agree with the physical behavior of the system.

The setup is in [this Colab notebook](https://colab.research.google.com/drive/17aBHTKA6mfsteQuJ9YS3N6nWapzUdr_D?usp=sharing). I created a simple environment with an elastic ball, a ground plane, and no gravity. The ball is initialized with a constant downwards velocity and the simulation is run for a fixed length (1 second; set to be long enough that the ball bounces once). I then vary the initial height of the ball between 2.9 and 3 meters and observe the final height of the ball.

Thinking about what should happen physically: with no gravity, if we raise the initial height of the ball, then it should take longer to reach the floor and bounce, thus spending less time in the ""upwards moving"" phase and reaching a **lower** final height. However, although this trend (higher initial height -> lower final height) holds **on average**, the final height is not continuous when we simulate it using Brax!

Here's the plot of final height vs initial height with the `pbd` backend:
![image](https://user-images.githubusercontent.com/4432920/223867462-eea829f8-ca01-45a6-be48-355eb95cbd7a.png)

And here's a plot of the final height vs initial height with the `legacy_spring` backend:
![image](https://user-images.githubusercontent.com/4432920/223868115-e7a3bd35-97d0-440b-b11c-672559ff1780.png)

If we were to try to compute the gradient of the final height w.r.t. the initial height (either with `jax.grad` or with some numerical difference scheme), then the gradient would be zero almost everywhere (for `pbd`) or **opposite** the long-run trend (for `legacy_spring`)!

This seems to be the same issue as reported in [this paper](https://arxiv.org/pdf/1910.00935.pdf) (section 4.3). Here's the key figure (note how it matches Brax's behavior for the spring case)
![image](https://user-images.githubusercontent.com/4432920/223865994-79319ab2-8b51-48ba-938f-1700129a60cb.png)

There's also a video showing this issue [here](https://youtu.be/Z1xvAZve9aE?t=180).

That paper suggests adding time-of-collision detection as a remedy; is that something that could be added to Brax? This is in the critical path of my research, and so I would be willing to help implement this feature, but I might need some pointers for where to start.

Thanks for your help! I like Brax and would love to help out.

P.S. it seems like many people have struggled to get analytical policy gradient-based algorithms to work. Could this be the reason?
"
quantopian/qgrid,"""ImportError: cannot import name 'Mapping' from 'collections' (C:\ProgramData\anaconda3\lib\collections\__init__.py)"": https://github.com/quantopian/qgrid/issues/386
Description: ### Environment
Windows 11
Python 3.10.9
pip install qgrid
Jupyter lab packages:
        bqplot v0.5.40 enabled ok (python, bqplot)
        jupyterlab-plotly v5.14.1 enabled ok
        jupyterlab_pygments v0.2.2 enabled ok (python, jupyterlab_pygments)
        @jupyter-widgets/jupyterlab-manager v5.0.7 enabled ok (python, jupyterlab_widgets)
        @voila-dashboards/jupyterlab-preview v2.2.0 enabled ok (python, voila)

### Description of Issue
When attempting to implement python qgrid in a data analysis script, the following error was raised:
""ImportError: cannot import name 'Mapping' from 'collections' (C:\ProgramData\anaconda3\lib\collections\__init__.py)""

I have attempted to reinstall various related packages etc., and I have tried the several answers that relate to it when searched online, but nothing seems to work.  For instance:

I added the following lines to the ""C:\ProgramData\anaconda3\lib\collections\__init__.py""  file

from _collections_abc import Mapping
from _collections_abc import MutableMapping
from _collections_abc import Sequence

so I got by the 'Mapping' package error, only to get an error for the 'Iterable' package.
Fix one error with the package, only for another related error to crop up.

Spent lots of time trying to solve this, to no avail.  Sounds like a promising package, but I can't get it to work.

"
quantopian/qgrid,"Here, working version: https://github.com/quantopian/qgrid/issues/383
Description: I made a working conda version for ipywidget 8 and jupyter lab 3.x

https://anaconda.org/eshard/qgrid



"
quantopian/qgrid,"QGrid is not working with NB-viewer.: https://github.com/quantopian/qgrid/issues/381
Description: ### Environment

* Operating System: Ubuntu 
* Python Version: `3.6.9`
* How did you install Qgrid: `pip`
* Python packages: absl-py==0.13.0
aiobotocore==1.4.2
aiohttp==3.7.4.post0
aiohttp-cors==0.7.0
aiohttp-retry==2.4.6
aioitertools==0.10.0
alembic==1.6.5
altair==4.1.0
altgraph==0.17
appdirs==1.4.4
argon2-cffi==20.1.0
astor==0.8.1
async-generator==1.10
async-timeout==3.0.1
atpublic==2.3
attrs==21.2.0
backcall==0.2.0
backports.zoneinfo==0.2.1
base58==2.1.0
basemap==1.3.6
basemap-data==1.3.2
beautifulsoup4==4.9.3
BentoML==0.13.1
bitarray==1.2.2
black==22.8.0
bleach==3.3.1
blinker==1.5
boto3==1.17.106
botocore==1.20.106
Bottleneck==1.3.2
branca==0.4.2
bravado==11.0.3
bravado-core==5.17.0
Brotli==1.0.9
cached-property==1.5.2
cachetools==4.2.4
Cerberus==1.3.4
certifi==2021.5.30
cffi==1.14.6
chardet==4.0.0
charset-normalizer==2.0.3
click==7.1.2
click-plugins==1.1.1
cligj==0.7.2
colorama==0.4.4
comet-ml==3.28.0
commonmark==0.9.1
configobj==5.0.6
configparser==5.0.2
contextlib2==21.6.0
contextvars==2.4
cycler==0.10.0
cytoolz==0.11.0
dash==2.0.0
dash-bootstrap-components==1.2.1
dash-colorscales==0.0.4
dash-core-components==2.0.0
dash-daq==0.5.0
dash-html-components==2.0.0
dash-table==5.0.0
dataclasses==0.8
dataframe-image==0.1.1
decorator==4.4.2
deepmerge==0.3.0
defusedxml==0.7.1
dictdiffer==0.9.0
diskcache==5.4.0
distro==1.7.0
docker==5.0.0
docutils==0.17.1
dpath==2.0.6
dtale==2.9.1
dulwich==0.20.33
dvc==2.8.1
elasticsearch==7.13.4
entrypoints==0.3
et-xmlfile==1.1.0
eth-abi==2.1.1
eth-account==0.5.6
eth-hash==0.3.2
eth-keyfile==0.5.1
eth-keys==0.3.3
eth-rlp==0.2.1
eth-typing==2.2.2
eth-utils==1.10.0
everett==3.0.0
feather-format==0.4.1
Fiona==1.8.20
Flask==1.1.4
Flask-Compress==1.13
flask-ngrok==0.0.25
flatten-dict==0.4.2
flufl.lock==3.2
folium==0.12.1
fsspec==2021.10.0
ftfy==6.0.3
funcy==1.17
future==0.18.2
gast==0.5.0
Geohash==1.0
geopandas==0.9.0
gitdb==4.0.9
GitPython==3.1.18
grandalf==0.6
grpcio==1.39.0
gunicorn==20.1.0
h5py==3.1.0
hexbytes==0.2.2
htmlmin==0.1.12
humanfriendly==9.2
idna==3.2
idna-ssl==1.1.0
ImageHash==4.2.1
immutables==0.15
importlib-metadata==4.8.3
importlib-resources==5.2.3
ipfshttpclient==0.8.0a2
ipykernel==5.5.5
ipython==7.16.1
ipython-genutils==0.2.0
ipywidgets==7.6.3
itsdangerous==1.1.0
jedi==0.18.0
Jinja2==2.11.3
jmespath==0.10.0
joblib==1.0.1
jsonpointer==2.2
jsonref==0.2
jsonschema==3.2.0
jupyter==1.0.0
jupyter-client==6.1.12
jupyter-console==6.4.0
jupyter-contrib-core==0.3.3
jupyter-contrib-nbextensions==0.5.1
jupyter-core==4.7.1
jupyter-highlight-selected-word==0.2.0
jupyter-latex-envs==1.4.6
jupyter-nbextensions-configurator==0.4.1
jupyterlab-pygments==0.1.2
jupyterlab-widgets==1.0.0
kaleido==0.1.0
Keras==2.2.5
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.2
kiwisolver==1.1.0
lru-dict==1.1.7
lxml==4.8.0
lz4==3.1.10
mailchecker==4.1.17
Mako==1.1.4
mapclassify==2.4.3
Markdown==3.3.4
MarkupSafe==2.0.1
matplotlib==3.3.4
missingno==0.4.2
missingpy==0.2.0
mistune==0.8.4
mmh3==3.0.0
mock==4.0.3
monotonic==1.6
msgpack==1.0.3
multiaddr==0.0.9
multidict==5.1.0
multimethod==1.4
munch==2.5.0
mypy-extensions==0.4.3
nanotime==0.5.2
nbclient==0.5.3
nbconvert==6.0.7
nbformat==5.1.3
neptune-client==0.15.1
neptune-tensorflow-keras==0.9.9
nest-asyncio==1.5.1
netaddr==0.8.0
networkx==2.5.1
notebook==6.4.0
numpy==1.19.5
nvidia-ml-py3==7.352.0
oauthlib==3.2.0
opencv-python==4.5.5.64
openpyxl==3.0.8
packaging==21.0
pandas==1.1.5
pandocfilters==1.4.3
parsimonious==0.8.1
parso==0.8.2
pathspec==0.9.0
patsy==0.5.1
pep517==0.11.0
pexpect==4.8.0
phik==0.12.0
phonenumbers==8.12.49
pickleshare==0.7.5
Pillow==8.4.0
pip-compile-multi==2.4.1
pip-tools==6.2.0
platformdirs==2.4.0
plotly==5.1.0
ply==3.11
prometheus-client==0.11.0
prompt-toolkit==3.0.19
protobuf==3.17.3
psutil==5.8.0
ptyprocess==0.7.0
pyarrow==4.0.1
pyasn1==0.4.8
pycparser==2.20
pycryptodome==3.11.0
pydantic==1.8.2
pydeck==0.6.2
pydot==1.4.2
pyee==8.2.2
pygit2==1.6.1
Pygments==2.9.0
pygtrie==2.4.2
pyinstaller==4.4
pyinstaller-hooks-contrib==2021.2
PyJWT==2.3.0
Pympler==1.0.1
pyparsing==2.4.7
pyppeteer==0.2.6
pyproj==3.0.1
pyrsistent==0.18.0
pyshp==2.3.1
python-benedict==0.25.1
python-dateutil==2.8.2
python-editor==1.0.4
python-fsutil==0.6.1
python-geohash==0.8.5
python-json-logger==2.0.1
python-slugify==6.1.2
pytz==2021.1
pytz-deprecation-shim==0.1.0.post0
PyWavelets==1.1.1
PyYAML==6.0
pyzmq==22.1.0
qgrid==1.3.1
qtconsole==5.1.1
QtPy==1.9.0
requests==2.26.0
requests-oauthlib==1.3.1
requests-toolbelt==0.9.1
rfc3987==1.3.8
rich==12.4.4
rlp==2.0.1
ruamel.yaml==0.17.21
ruamel.yaml.clib==0.2.6
s3fs==2021.10.0
s3transfer==0.4.2
schema==0.7.4
scikit-learn==0.24.2
scipy==1.5.4
seaborn==0.11.2
selenium==3.141.0
semantic-version==2.9.0
semver==2.13.0
Send2Trash==1.7.1
Shapely==1.7.1
shortuuid==1.0.9
shtab==1.5.4
simple-di==0.1.0
simplejson==3.17.6
six==1.16.0
smmap==5.0.0
soupsieve==2.2.1
SQLAlchemy==1.3.24
SQLAlchemy-Utils==0.36.5
squarify==0.4.3
statsmodels==0.12.2
streamlit==1.10.0
strict-rfc3339==0.7
strsimpy==0.2.1
swagger-spec-validator==2.7.4
tabulate==0.8.9
tangled-up-in-unicode==0.1.0
tenacity==8.0.1
tensorboard==1.13.1
tensorflow==1.13.2
tensorflow-estimator==1.13.0
termcolor==1.1.0
terminado==0.10.1
testpath==0.5.0
text-unidecode==1.3
threadpoolctl==2.2.0
toml==0.10.2
tomli==1.2.3
toolz==0.11.1
toposort==1.6
tornado==6.1
tqdm==4.62.2
traitlets==4.3.3
typed-ast==1.5.4
types-dataclasses==0.1.5
typing-extensions==4.1.1
tzdata==2022.2
tzlocal==4.2
urllib3==1.25.11
validators==0.20.0
varint==1.0.2
visions==0.7.1
voluptuous==0.13.1
watchdog==2.1.9
wcwidth==0.2.5
web3==5.24.0
webcolors==1.11.1
webencodings==0.5.1
websocket-client==1.1.0
websockets==9.1
Werkzeug==1.0.1
widgetsnbextension==3.5.1
wrapt==1.14.0
wurlitzer==3.0.2
xarray==0.16.2
xlrd==2.0.1
xmltodict==0.13.0
yarl==1.6.3
zc.lockfile==2.0
zipp==3.6.0

* Jupyter lab packages (if applicable): `$ jupyter labextension list`

### Description of Issue

* What did you expect to happen?
* What happened instead?

### Reproduction Steps

1.The qgrid is working fine on JUpyter notebook locally. I have a nb-viewer server, when I upload it there, it doesnot show me the table for which I use qgrid.
...

### What steps have you taken to resolve this already?
- tried to use older version but doesnot solve my problem.
- trying to find a js file to be locally embedded in notebook.
...

### Anything else?
![image](https://user-images.githubusercontent.com/3126387/211582083-9a40b800-6b82-4362-94ca-bf4c79955f9e.png)

...


"
quantopian/qgrid,"REPO IS NO LONGER MAINTAINED AND DOES NOT SUPPORT JUPYTERLAB>=3.0.0: https://github.com/quantopian/qgrid/issues/380
Description: I just spent quite a while setting up an environment and troubleshooting different issues, when I realized that this repo is no maintained and does not support any recent version of jupyterlab.


"
quantopian/qgrid,"update for ipywidgets 8 compatibility: https://github.com/quantopian/qgrid/issues/376
Description: according to https://github.com/jupyter-widgets/ipywidgets/issues/3031

this line should have its parenthesis removed:

https://github.com/quantopian/qgrid/blob/877b420d3bd83297bbcc97202b914001a85afff2/qgrid/grid.py#L524

This is broken now with `ipywidgets` version 8.0 released yesterday

"
quantopian/qgrid,"qgrid not working in Binder notebook: https://github.com/quantopian/qgrid/issues/374
Description: Hello,

I'm trying to get qgrid to work on a jupyter python notebook that I need to share with some colleagues. I'm new to bynder, and tried to follow some examples, but maybe some things have changed. I always end up with the `Error displaying widget: model not found` message.

The repo is located at https://github.com/CarlosGrohmann/DEMIX_wine_contest

the bynder is this one: https://mybinder.org/v2/gh/CarlosGrohmann/DEMIX_wine_contest/HEAD?labpath=wine_contest.ipynb

This is my environment.yml:
```
name: demix-env
channels:
  - conda-forge
  - defaults
dependencies:
  - jupyterlab
  - python
  - numpy
  - qgrid
  - matplotlib
  - nodejs
  - pip
  - pip:
    - npm
    - ipywidgets
    - pandas
    - seaborn
    - qgrid
    - scipy
    - jupyter_contrib_nbextensions 
```
and this is postBuild

```
#!/bin/bash
jupyter labextension install @ jupyter-widgets / jupyterlab-manager --no-build
jupyter labextension install @ 8080labs / qgrid --no-build

jupyter nbextension enable --py --sys-prefix qgrid
jupyter nbextension enable --py --sys-prefix widgetsnbextension

jupyter lab build --minimize=False
```

Python packages:
```
# packages in environment at /srv/conda/envs/notebook:
#
# Name                    Version                   Build  Channel
_libgcc_mutex             0.1                 conda_forge    conda-forge
_openmp_mutex             4.5                       1_gnu    conda-forge
alembic                   1.7.5              pyhd8ed1ab_0    conda-forge
alsa-lib                  1.2.3                h516909a_0    conda-forge
anyio                     3.5.0           py310hff52083_0    conda-forge
argon2-cffi               21.3.0             pyhd8ed1ab_0    conda-forge
argon2-cffi-bindings      21.2.0          py310h6acc77f_1    conda-forge
asttokens                 2.0.5              pyhd8ed1ab_0    conda-forge
async_generator           1.10                       py_0    conda-forge
attrs                     21.4.0             pyhd8ed1ab_0    conda-forge
babel                     2.9.1              pyh44b312d_0    conda-forge
backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
backports                 1.0                        py_2    conda-forge
backports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    conda-forge
beautifulsoup4            4.10.0             pyha770c72_0    conda-forge
bleach                    4.1.0              pyhd8ed1ab_0    conda-forge
blinker                   1.4                        py_1    conda-forge
brotli                    1.0.9                h7f98852_6    conda-forge
brotli-bin                1.0.9                h7f98852_6    conda-forge
brotlipy                  0.7.0           py310h6acc77f_1003    conda-forge
bzip2                     1.0.8                h7f98852_4    conda-forge
c-ares                    1.18.1               h7f98852_0    conda-forge
ca-certificates           2021.10.8            ha878542_0    conda-forge
certifi                   2021.10.8       py310hff52083_1    conda-forge
certipy                   0.1.3                      py_0    conda-forge
cffi                      1.15.0          py310h0fdd8cc_0    conda-forge
charset-normalizer        2.0.10             pyhd8ed1ab_0    conda-forge
cryptography              36.0.1          py310h685ca39_0    conda-forge
cycler                    0.11.0             pyhd8ed1ab_0    conda-forge
dbus                      1.13.6               h5008d03_3    conda-forge
debugpy                   1.5.1           py310h122e73d_0    conda-forge
decorator                 5.1.1              pyhd8ed1ab_0    conda-forge
defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
entrypoints               0.3             pyhd8ed1ab_1003    conda-forge
executing                 0.8.3              pyhd8ed1ab_0    conda-forge
expat                     2.4.7                h27087fc_0    conda-forge
flit-core                 3.6.0              pyhd8ed1ab_0    conda-forge
font-ttf-dejavu-sans-mono 2.37                 hab24e00_0    conda-forge
font-ttf-inconsolata      3.000                h77eed37_0    conda-forge
font-ttf-source-code-pro  2.038                h77eed37_0    conda-forge
font-ttf-ubuntu           0.83                 hab24e00_0    conda-forge
fontconfig                2.13.96              h8e229c2_2    conda-forge
fonts-conda-ecosystem     1                             0    conda-forge
fonts-conda-forge         1                             0    conda-forge
fonttools                 4.30.0          py310h5764c6d_0    conda-forge
freetype                  2.10.4               h0708190_1    conda-forge
gettext                   0.19.8.1          h73d1719_1008    conda-forge
giflib                    5.2.1                h36c2ea0_2    conda-forge
greenlet                  1.1.2           py310h122e73d_1    conda-forge
gst-plugins-base          1.18.5               hf529b03_3    conda-forge
gstreamer                 1.18.5               h9f60fe5_3    conda-forge
icu                       69.1                 h9c3ff4c_0    conda-forge
idna                      3.3                pyhd8ed1ab_0    conda-forge
importlib-metadata        4.11.2          py310hff52083_0    conda-forge
importlib_resources       5.4.0              pyhd8ed1ab_0    conda-forge
ipykernel                 6.9.1           py310hfdc917e_0    conda-forge
ipython                   8.1.1           py310hff52083_0    conda-forge
ipython_genutils          0.2.0                      py_1    conda-forge
ipywidgets                7.6.5              pyhd8ed1ab_0    conda-forge
jbig                      2.1               h7f98852_2003    conda-forge
jedi                      0.18.1          py310hff52083_0    conda-forge
jinja2                    3.0.3              pyhd8ed1ab_0    conda-forge
jpeg                      9e                   h7f98852_0    conda-forge
json5                     0.9.5              pyh9f0ad1d_0    conda-forge
jsonschema                4.4.0              pyhd8ed1ab_0    conda-forge
jupyter-contrib-core      0.3.3                    pypi_0    pypi
jupyter-contrib-nbextensions 0.5.1                    pypi_0    pypi
jupyter-highlight-selected-word 0.2.0                    pypi_0    pypi
jupyter-latex-envs        1.4.6                    pypi_0    pypi
jupyter-nbextensions-configurator 0.4.1                    pypi_0    pypi
jupyter-offlinenotebook   0.2.2              pyh1d7be83_0    conda-forge
jupyter-resource-usage    0.6.1              pyhd8ed1ab_0    conda-forge
jupyter_client            7.1.2              pyhd8ed1ab_0    conda-forge
jupyter_core              4.9.2           py310hff52083_0    conda-forge
jupyter_server            1.13.4             pyhd8ed1ab_0    conda-forge
jupyter_telemetry         0.1.0              pyhd8ed1ab_1    conda-forge
jupyterhub-base           2.2.1              pyhd8ed1ab_0    conda-forge
jupyterhub-singleuser     2.2.1                hd8ed1ab_0    conda-forge
jupyterlab                3.3.1              pyhd8ed1ab_0    conda-forge
jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge
jupyterlab_server         2.10.3             pyhd8ed1ab_0    conda-forge
jupyterlab_widgets        1.0.2              pyhd8ed1ab_0    conda-forge
kiwisolver                1.3.2           py310h91b1402_1    conda-forge
krb5                      1.19.2               hcc1bbae_3    conda-forge
lcms2                     2.12                 hddcbb42_0    conda-forge
ld_impl_linux-64          2.36.1               hea4e1c9_2    conda-forge
lerc                      3.0                  h9c3ff4c_0    conda-forge
libblas                   3.9.0           13_linux64_openblas    conda-forge
libbrotlicommon           1.0.9                h7f98852_6    conda-forge
libbrotlidec              1.0.9                h7f98852_6    conda-forge
libbrotlienc              1.0.9                h7f98852_6    conda-forge
libcblas                  3.9.0           13_linux64_openblas    conda-forge
libclang                  13.0.1          default_hc23dcda_0    conda-forge
libcurl                   7.81.0               h2574ce0_0    conda-forge
libdeflate                1.10                 h7f98852_0    conda-forge
libedit                   3.1.20191231         he28a2e2_2    conda-forge
libev                     4.33                 h516909a_1    conda-forge
libevent                  2.1.10               h9b69904_4    conda-forge
libffi                    3.4.2                h7f98852_5    conda-forge
libgcc-ng                 11.2.0              h1d223b6_12    conda-forge
libgfortran-ng            11.2.0              h69a702a_13    conda-forge
libgfortran5              11.2.0              h5c6108e_13    conda-forge
libglib                   2.70.2               h174f98d_4    conda-forge
libgomp                   11.2.0              h1d223b6_12    conda-forge
libiconv                  1.16                 h516909a_0    conda-forge
liblapack                 3.9.0           13_linux64_openblas    conda-forge
libllvm13                 13.0.1               hf817b99_2    conda-forge
libnghttp2                1.43.0               h812cca2_1    conda-forge
libnsl                    2.0.0                h7f98852_0    conda-forge
libogg                    1.3.4                h7f98852_1    conda-forge
libopenblas               0.3.18          pthreads_h8fe5266_0    conda-forge
libopus                   1.3.1                h7f98852_1    conda-forge
libpng                    1.6.37               h21135ba_2    conda-forge
libpq                     14.2                 hd57d9b9_0    conda-forge
libsodium                 1.0.18               h36c2ea0_1    conda-forge
libssh2                   1.10.0               ha56f1ee_2    conda-forge
libstdcxx-ng              11.2.0              he4da1e4_12    conda-forge
libtiff                   4.3.0                h542a066_3    conda-forge
libuuid                   2.32.1            h7f98852_1000    conda-forge
libuv                     1.43.0               h7f98852_0    conda-forge
libvorbis                 1.3.7                h9c3ff4c_0    conda-forge
libwebp                   1.2.2                h3452ae3_0    conda-forge
libwebp-base              1.2.2                h7f98852_1    conda-forge
libxcb                    1.13              h7f98852_1004    conda-forge
libxkbcommon              1.0.3                he3ba5ed_0    conda-forge
libxml2                   2.9.12               h885dcf4_1    conda-forge
libzlib                   1.2.11            h36c2ea0_1013    conda-forge
lxml                      4.8.0                    pypi_0    pypi
lz4-c                     1.9.3                h9c3ff4c_1    conda-forge
mako                      1.1.6              pyhd8ed1ab_0    conda-forge
markupsafe                2.1.0           py310h5764c6d_1    conda-forge
matplotlib                3.5.1           py310hff52083_0    conda-forge
matplotlib-base           3.5.1           py310h23f4a51_0    conda-forge
matplotlib-inline         0.1.3              pyhd8ed1ab_0    conda-forge
mistune                   0.8.4           py310h6acc77f_1005    conda-forge
munkres                   1.1.4              pyh9f0ad1d_0    conda-forge
mysql-common              8.0.28               ha770c72_0    conda-forge
mysql-libs                8.0.28               hfa10184_0    conda-forge
nbclassic                 0.3.5              pyhd8ed1ab_0    conda-forge
nbclient                  0.5.10             pyhd8ed1ab_1    conda-forge
nbconvert                 6.4.4           py310hff52083_0    conda-forge
nbformat                  5.1.3              pyhd8ed1ab_0    conda-forge
ncurses                   6.3                  h9c3ff4c_0    conda-forge
nest-asyncio              1.5.4              pyhd8ed1ab_0    conda-forge
nodejs                    17.4.0               h8ca31f7_0    conda-forge
notebook                  6.3.0              pyha770c72_1    conda-forge
npm                       0.1.1                    pypi_0    pypi
nspr                      4.32                 h9c3ff4c_1    conda-forge
nss                       3.74                 hb5efdd6_0    conda-forge
nteract_on_jupyter        2.1.3                      py_0    conda-forge
numpy                     1.22.3          py310h45f3432_0    conda-forge
oauthlib                  3.1.1              pyhd8ed1ab_0    conda-forge
openjpeg                  2.4.0                hb52868f_1    conda-forge
openssl                   1.1.1l               h7f98852_0    conda-forge
optional-django           0.1.0                    pypi_0    pypi
packaging                 21.3               pyhd8ed1ab_0    conda-forge
pamela                    1.0.0                      py_0    conda-forge
pandas                    1.4.1           py310hb5077e9_0    conda-forge
pandoc                    2.17.0.1             h7f98852_0    conda-forge
pandocfilters             1.5.0              pyhd8ed1ab_0    conda-forge
parso                     0.8.3              pyhd8ed1ab_0    conda-forge
pcre                      8.45                 h9c3ff4c_0    conda-forge
pexpect                   4.8.0              pyh9f0ad1d_2    conda-forge
pickleshare               0.7.5                   py_1003    conda-forge
pillow                    9.0.1           py310he619898_2    conda-forge
pip                       22.0.4             pyhd8ed1ab_0    conda-forge
prometheus_client         0.13.0             pyhd8ed1ab_0    conda-forge
prompt-toolkit            3.0.24             pyha770c72_0    conda-forge
psutil                    5.9.0           py310h6acc77f_0    conda-forge
pthread-stubs             0.4               h36c2ea0_1001    conda-forge
ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
pure_eval                 0.2.2              pyhd8ed1ab_0    conda-forge
pycparser                 2.21               pyhd8ed1ab_0    conda-forge
pycurl                    7.44.1          py310ha6c063c_1    conda-forge
pygments                  2.11.2             pyhd8ed1ab_0    conda-forge
pyjwt                     2.3.0              pyhd8ed1ab_1    conda-forge
pyopenssl                 21.0.0             pyhd8ed1ab_0    conda-forge
pyparsing                 3.0.7              pyhd8ed1ab_0    conda-forge
pyqt                      5.12.3          py310hff52083_8    conda-forge
pyqt-impl                 5.12.3          py310h1f8e252_8    conda-forge
pyqt5-sip                 4.19.18         py310h122e73d_8    conda-forge
pyqtchart                 5.12            py310hfcd6d55_8    conda-forge
pyqtwebengine             5.12.1          py310hfcd6d55_8    conda-forge
pyrsistent                0.18.1          py310h6acc77f_0    conda-forge
pysocks                   1.7.1           py310hff52083_4    conda-forge
python                    3.10.2          h85951f9_4_cpython    conda-forge
python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
python-json-logger        2.0.1              pyh9f0ad1d_0    conda-forge
python_abi                3.10                    2_cp310    conda-forge
pytz                      2021.3             pyhd8ed1ab_0    conda-forge
pyyaml                    6.0                      pypi_0    pypi
pyzmq                     22.3.0          py310h675a958_1    conda-forge
qgrid                     1.3.1              pyhd8ed1ab_3    conda-forge
qt                        5.12.9               ha98a1a1_5    conda-forge
readline                  8.1                  h46c0cb4_0    conda-forge
requests                  2.27.1             pyhd8ed1ab_0    conda-forge
ruamel.yaml               0.17.21         py310h6acc77f_0    conda-forge
ruamel.yaml.clib          0.2.6           py310h6acc77f_0    conda-forge
scipy                     1.8.0                    pypi_0    pypi
seaborn                   0.11.2                   pypi_0    pypi
send2trash                1.8.0              pyhd8ed1ab_0    conda-forge
setuptools                60.9.3          py310hff52083_0    conda-forge
six                       1.16.0             pyh6c4a22f_0    conda-forge
sniffio                   1.2.0           py310hff52083_2    conda-forge
soupsieve                 2.3.1              pyhd8ed1ab_0    conda-forge
sqlalchemy                1.4.32          py310h5764c6d_0    conda-forge
sqlite                    3.37.0               h9cd32fc_0    conda-forge
stack_data                0.2.0              pyhd8ed1ab_0    conda-forge
terminado                 0.13.3          py310hff52083_0    conda-forge
testpath                  0.5.0              pyhd8ed1ab_0    conda-forge
tk                        8.6.12               h27826a3_0    conda-forge
tornado                   6.1             py310h6acc77f_2    conda-forge
traitlets                 5.1.1              pyhd8ed1ab_0    conda-forge
typing_extensions         4.0.1              pyha770c72_0    conda-forge
tzdata                    2021e                he74cb21_0    conda-forge
unicodedata2              14.0.0          py310h6acc77f_0    conda-forge
urllib3                   1.26.8             pyhd8ed1ab_1    conda-forge
wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
webencodings              0.5.1                      py_1    conda-forge
websocket-client          1.2.3              pyhd8ed1ab_0    conda-forge
wheel                     0.37.1             pyhd8ed1ab_0    conda-forge
widgetsnbextension        3.5.2           py310hff52083_1    conda-forge
xorg-libxau               1.0.9                h7f98852_0    conda-forge
xorg-libxdmcp             1.1.3                h7f98852_0    conda-forge
xz                        5.2.5                h516909a_1    conda-forge
zeromq                    4.3.4                h9c3ff4c_1    conda-forge
zipp                      3.7.0              pyhd8ed1ab_0    conda-forge
zlib                      1.2.11            h36c2ea0_1013    conda-forge
zstd                      1.5.2                ha95c52a_0    conda-forge
```

Jupyter extensions
```
JupyterLab v3.3.1
/srv/conda/envs/notebook/share/jupyter/labextensions
        jupyter-offlinenotebook v0.2.2 enabled OK
        @jupyter-widgets/jupyterlab-manager v3.0.1 enabled OK (python, jupyterlab_widgets)
        @jupyter-server/resource-usage v0.6.1 enabled OK (python, jupyter-resource-usage)
```

Thanks




"
quantopian/qgrid,"missing 1 required positional argument: 'widget': https://github.com/quantopian/qgrid/issues/372
Description: Hello, i've tried to install qgrid and it seems i am having the below error. Could you please advice?

### Environment

* Operating System: centos 7
* Python Version: Python 3.8.11
* How did you install Qgrid:

`pip install qgrid`
`jupyter nbextension enable --py --sys-prefix widgetsnbextension`
`jupyter labextension install qgrid2`


`jupyter nbextension enable --py --sys-prefix qgrid` - if i try to run this i get the below error:

`
Traceback (most recent call last):
  File ""/opt/app-root/bin/jupyter-nbextension"", line 8, in <module>
    sys.exit(main())
  File ""/opt/app-root/lib64/python3.8/site-packages/jupyter_core/application.py"", line 264, in launch_instance
    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)
  File ""/opt/app-root/lib64/python3.8/site-packages/traitlets/config/application.py"", line 846, in launch_instance
    app.start()
  File ""/opt/app-root/lib64/python3.8/site-packages/notebook/nbextensions.py"", line 983, in start
    super(NBExtensionApp, self).start()
  File ""/opt/app-root/lib64/python3.8/site-packages/jupyter_core/application.py"", line 253, in start
    self.subapp.start()
  File ""/opt/app-root/lib64/python3.8/site-packages/notebook/nbextensions.py"", line 891, in start
    self.toggle_nbextension_python(self.extra_args[0])
  File ""/opt/app-root/lib64/python3.8/site-packages/notebook/nbextensions.py"", line 864, in toggle_nbextension_python
    return toggle(module,
  File ""/opt/app-root/lib64/python3.8/site-packages/notebook/nbextensions.py"", line 477, in enable_nbextension_python
    return _set_nbextension_state_python(True, module, user, sys_prefix,
  File ""/opt/app-root/lib64/python3.8/site-packages/notebook/nbextensions.py"", line 375, in _set_nbextension_state_python
    m, nbexts = _get_nbextension_metadata(module)
  File ""/opt/app-root/lib64/python3.8/site-packages/notebook/nbextensions.py"", line 1117, in _get_nbextension_metadata
    m = import_item(module)
  File ""/opt/app-root/lib64/python3.8/site-packages/traitlets/utils/importstring.py"", line 38, in import_item
    return __import__(parts[0])
  File ""/opt/app-root/lib64/python3.8/site-packages/qgrid/__init__.py"", line 3, in <module>
    from .grid import (
  File ""/opt/app-root/lib64/python3.8/site-packages/qgrid/grid.py"", line 524, in <module>
    @widgets.register()
TypeError: register() missing 1 required positional argument: 'widget'
`

* Python packages: 
`notebook                 6.1.1`
`qgrid                    1.3.1`
`jupyterlab               2.2.9`

* Jupyter lab packages (if applicable):

`
JupyterLab v2.2.9
Known labextensions:
   app dir: /opt/app-root/share/jupyter/lab
        @jupyter-widgets/jupyterlab-manager v2.0.0  enabled  OK
        @jupyterlab/debugger v0.3.1  enabled  OK
        @jupyterlab/git v0.23.3  enabled  OK
        jupyterlab-dash v0.4.0  enabled  OK
        jupyterlab-execute-time v1.1.0  enabled  OK
        jupyterlab-logout v0.5.0  enabled  OK
        jupyterlab-s3-browser v0.11.0-rc.0  enabled  OK
        jupyterlab-system-monitor v0.6.0  enabled  OK
        jupyterlab-theme-toggle v0.5.0  enabled  OK
        jupyterlab-topbar-extension v0.5.0  enabled  OK
        jupyterlab-topbar-text v0.5.1  enabled  OK
        nbdime-jupyterlab v2.0.2  enabled  OK
        qgrid2 v1.1.3  enabled  OK
`
### Description of Issue

Installed qgrid on jupyter and tried to run 

`import qgrid`
I get the below error
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/tmp/ipykernel_251/3618078849.py in <module>
----> 1 import qgrid

/opt/app-root/lib64/python3.8/site-packages/qgrid/__init__.py in <module>
      1 from ._version import version_info, __version__
      2 
----> 3 from .grid import (
      4     enable,
      5     disable,

/opt/app-root/lib64/python3.8/site-packages/qgrid/grid.py in <module>
    522 
    523 
--> 524 @widgets.register()
    525 class QgridWidget(widgets.DOMWidget):
    526     """"""

TypeError: register() missing 1 required positional argument: 'widget'



"
quantopian/qgrid,"AttributeError: 'DataFrame' object has no attribute 'map': https://github.com/quantopian/qgrid/issues/370
Description: ### Environment

* Operating System: MacOS Monterey 
* Python Version: 3.9
* How did you install Qgrid: pip and conda, v1.3.1
* Python packages: `$ pip freeze` or `$ conda list` (please include qgrid, notebook, and jupyterlab versions)
* Jupyter lab packages (if applicable): `$ jupyter labextension list`

### Description of Issue

Issue rendering the dataframe. Exception: `AttributeError: 'DataFrame' object has no attribute 'map'`

* What did you expect to happen?

Render dataframe

* What happened instead?

```
/Applications/Anaconda/anaconda3/lib/python3.9/site-packages/qgrid/grid.py in show_grid(data_frame, show_toolbar, precision, grid_options, column_options, column_definitions, row_edit_callback)
    504 
    505     # create a visualization for the dataframe
--> 506     return QgridWidget(df=data_frame, precision=precision,
    507                        grid_options=grid_options,
    508                        column_options=column_options,

/Applications/Anaconda/anaconda3/lib/python3.9/site-packages/qgrid/grid.py in __init__(self, *args, **kwargs)
    625 
    626         if self.df is not None:
--> 627             self._update_df()
    628 
    629     def _grid_options_default(self):

/Applications/Anaconda/anaconda3/lib/python3.9/site-packages/qgrid/grid.py in _update_df(self)
    818         self._unfiltered_df = self._df.copy()
    819 
--> 820         self._update_table(update_columns=True, fire_data_change_event=False)
    821         self._ignore_df_changed = False
    822 

/Applications/Anaconda/anaconda3/lib/python3.9/site-packages/qgrid/grid.py in _update_table(self, update_columns, triggered_by, scroll_to_row, fire_data_change_event)
    909                 series_to_set = df[sort_column_name]
    910             else:
--> 911                 series_to_set = self._get_col_series_from_df(
    912                     col_name, df, level_vals=True
    913                 ).map(stringify)

/Applications/Anaconda/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py in __getattr__(self, name)
   5485         ):
   5486             return self[name]
-> 5487         return object.__getattribute__(self, name)
   5488 
   5489     def __setattr__(self, name: str, value) -> None:

AttributeError: 'DataFrame' object has no attribute 'map'
```

### Reproduction Steps - 

It happens in my environment with this particular DataFrame. Other DataFrames are rendered fine, no issue with them. 

1.  Unable to share the entire DataFrame. However, I have confirmed that the object is of type: `pandas.core.frame.DataFrame`

...

### What steps have you taken to resolve this already?

...

### Anything else?

...


"
quantopian/qgrid,"Qgrid working with Modern Jupyter Lab (3.2 and beyond): https://github.com/quantopian/qgrid/issues/368
Description: I have cloned qgrid and spent a day learning it well enough to work with an up-to-date Jupyterlab.  I prebuilt the extension and it seems to be working. I am not an expert JS coder, and more of a hacker, but I got it working. If we move over here, we could start maybe cleaning up some of the bugs in the code, but I will need other coders to help!  

https://github.com/JohnOmernik/qgrid



"
quantopian/qgrid,"qgrid incompatible with ipywidgets==8.0.0a6 due to decorator usage: https://github.com/quantopian/qgrid/issues/367
Description: ### Environment

* Operating System: ubuntu 20.04
* Python Version: `$ python --version`: 3.9
* How did you install Qgrid: pip
* Python packages: `$ pip freeze` or `$ conda list` (please include qgrid, notebook, and jupyterlab versions)

qgrid: 1.3.1, ipywidgets 8.0.0a6

* Jupyter lab packages (if applicable): `$ jupyter labextension list`

N/A

### Description of Issue

* What did you expect to happen?

`import qgrid` to work

* What happened instead?

`import qgrid` threw an error in `qgrid/grid.py` line 524

```
@widgets.register() <--- register() missing 1 required positional argument: 'widget'
```

### Reproduction Steps

1. `pip3 install ipywidgets==8.0.0a6`
2. `pip3 install qgrid`
3. `import qgrid` in python 3.9
...

### What steps have you taken to resolve this already?

None

### Anything else?

No


"
quantopian/qgrid,"add_row function breaks when using a numeric index: https://github.com/quantopian/qgrid/issues/364
Description: ### Environment

* Operating System: WIndows 10
* Python Version: 3.8.8
* How did you install Qgrid: pip

### Description of Issue

I am trying to add an empty row at the end of my `qgrid` widget.  The row argument is:

```python
[('ID', ''), ('SOURCE', ''), ('PROJECT', ''), ('NAME', ''), ('STATUS', ''), ('SHAPE', ''), ('TIME', '')]
```

The error is:

```python
C:\notebook_analytics\nemawashi\nemawashi_lib\user_interfaces\jobs.py in add_jobs_to_qgrid_widget(self, job)
    340                 empty_row.append((column_name, ''))
    341             print(empty_row)
--> 342             self.widgets['jobs_qgrid'].add_row(empty_row)
    343         jobs_df = self.widgets['jobs_qgrid'].df
    344         (col_opts, col_defs) = self.__fetch_qgrid_options()

~\Anaconda3\envs\nemawashi\lib\site-packages\qgrid\grid.py in add_row(self, row)
   1617             added_index = self._duplicate_last_row()
   1618         else:
-> 1619             added_index = self._add_row(row)
   1620 
   1621         self._notify_listeners({

~\Anaconda3\envs\nemawashi\lib\site-packages\qgrid\grid.py in _add_row(self, row)
   1666         print(f""dictrow={dict(row)}"")
   1667         print(f""indexname={df.index.name}"")
-> 1668         index_col_val = dict(row)[df.index.name]
   1669 
   1670         # check that the given column names match what

KeyError: None
```

The issue here is that the documentation says the following for the `_add_row` function.

~~~python
def _add_row(self, row):
        """"""
        Append a new row to the end of the DataFrame given a list of 2-tuples
        of (column name, column value). This method will work for DataFrames
        with arbitrary index types.
        """"""
~~~

However, the code actual assumes there is an index name set within the dataframe [here](https://github.com/quantopian/qgrid/blob/master/qgrid/grid.py#L1665).  A dataframe with an integer index will not have an index name.  Either the `_add_row` function needs to be re-written or an option could be added to the `add_row` function to add an empty row.

~~~python
qgrid_widget.add_row(empty=True)
~~~

Adding an empty row is very easy regardless if there is an index or not.

### Reproduction Steps

1. Create a qgrid widget with an integer index
2. Call the `add_row` function with a row argument

### What steps have you taken to resolve this already?

The workaround for me is to add the empty row in pandas and then re-instantiate the `qgrid` widget.

### Anything else?

Please advise on the PR direction folks want and I will create the PR.


"
quantopian/qgrid,"qgrid widget showing blank rows when dispayed within an Output widget: https://github.com/quantopian/qgrid/issues/362
Description: ### Environment

* Operating System: Windows 10
* Python Version: `$ 3.8.8
* How did you install Qgrid: conda
* Python packages: `$ qgrid==1.3.1, jupyter nb==6.3.0

### Description of Issue

When creating a qgrid widget within an Output widget, on a separate Tab widget, the table is created but all of the rows show no data.  However, the data is in the table as the filters show it.

If I display the same qgrid widget on another tab, not in an Output widget, the table displays correctly.

![image](https://user-images.githubusercontent.com/26782653/112749976-6a34bf80-8f8b-11eb-8a94-a50e1d83b85c.png)

The code is essentially:

~~~python
        self.widgets['ui_container'] = ipyw.Output(layout={'border': '1px solid black', 'width': '99%', 'height': '100%'})
        with self.widgets['ui_container']:
            display(self.widgets['current_visual'])
        display(self.widgets['current_visual'])
~~~




"
quantopian/qgrid,"AttributeError: 'DataFrame' object has no attribute 'map' when running qgrid.enable(): https://github.com/quantopian/qgrid/issues/360
Description: ### Environment

* Operating System: Windows 10
* Python Version: `$3.8.5
* How did you install Qgrid: pip
* Python packages: `$ qgrid==1.3.1, jupyter notebook 6.1.4

### Description of Issue

Tried to enable `qgrid` by default for all dataframes in my Jupyter Notebook

### Reproduction Steps

1. qgrid.enable()
2. my_dataframe

### What steps have you taken to resolve this already?
None
...

### Anything else?
Full stack error:
~~~python
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~\AppData\Roaming\Python\Python38\site-packages\IPython\core\formatters.py in __call__(self, obj)
    911                 pass
    912             else:
--> 913                 printer(obj)
    914                 return True
    915             # Finally look for special method names

C:\ProgramData\Anaconda3\lib\site-packages\qgrid\grid.py in _display_as_qgrid(data)
    276 
    277 def _display_as_qgrid(data):
--> 278     display(show_grid(data))
    279 
    280 

C:\ProgramData\Anaconda3\lib\site-packages\qgrid\grid.py in show_grid(data_frame, show_toolbar, precision, grid_options, column_options, column_definitions, row_edit_callback)
    504 
    505     # create a visualization for the dataframe
--> 506     return QgridWidget(df=data_frame, precision=precision,
    507                        grid_options=grid_options,
    508                        column_options=column_options,

C:\ProgramData\Anaconda3\lib\site-packages\qgrid\grid.py in __init__(self, *args, **kwargs)
    625 
    626         if self.df is not None:
--> 627             self._update_df()
    628 
    629     def _grid_options_default(self):

C:\ProgramData\Anaconda3\lib\site-packages\qgrid\grid.py in _update_df(self)
    818         self._unfiltered_df = self._df.copy()
    819 
--> 820         self._update_table(update_columns=True, fire_data_change_event=False)
    821         self._ignore_df_changed = False
    822 

C:\ProgramData\Anaconda3\lib\site-packages\qgrid\grid.py in _update_table(self, update_columns, triggered_by, scroll_to_row, fire_data_change_event)
    909                 series_to_set = df[sort_column_name]
    910             else:
--> 911                 series_to_set = self._get_col_series_from_df(
    912                     col_name, df, level_vals=True
    913                 ).map(stringify)

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\generic.py in __getattr__(self, name)
   5137             if self._info_axis._can_hold_identifiers_and_holds_name(name):
   5138                 return self[name]
-> 5139             return object.__getattribute__(self, name)
   5140 
   5141     def __setattr__(self, name: str, value) -> None:

AttributeError: 'DataFrame' object has no attribute 'map'
~~~
...


"
quantopian/qgrid,"QGrid on Voila: does not display: https://github.com/quantopian/qgrid/issues/358
Description: Hi everyone,
I am having difficulties to render the very great qgrid tables (from dataframe) via Voila.

### Environment

* Operating System: Ubuntu
* Python Version: 3.8.5
* How did you install Qgrid: pip
* Python packages: Latest qgrid

### Description of Issue

* Expected: The dataframe table should be displayed as a QGrid table with controls to edit.
* Output: Blank page. The notebook does not render the dataframe when served with Voila.

### Reproduction Steps

1. Created a dataframe using Pandas
2. Used `qdf = qgrid.show_grid(df, show_toolbar=True);qdf` to render it in the notebook
3. Rendering through `ipywidgets.HBox([qdf], layout={'width': '1000px'})`
4. When served with voila, `$voila test.ipynb`, there is a blank screen
5. Terminal gives the following error
```
[W 12:52:06.423 NotebookApp] 404 GET /nbextensions/nbextensions_configurator/tree_tab/main.js?v=20210311125204 (127.0.0.1) 12.290000ms referer=http://localhost:8888/tree
[W 12:52:10.354 NotebookApp] Notebook WDN Scheduler - qgrid.ipynb is not trusted
[W 12:52:10.417 NotebookApp] 404 GET /nbextensions/nbextensions_configurator/config_menu/main.js?v=20210311125204 (127.0.0.1) 2.400000ms referer=http://localhost:8888/notebooks/WDN%20Scheduler%20-%20qgrid.ipynb
[I 12:52:10.897 NotebookApp] Kernel started: 1159ddc1-a9f9-41fa-8764-7d64b47f1789, name: python3
[W 12:52:10.971 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20210311125204 (127.0.0.1) 3.260000ms referer=http://localhost:8888/notebooks/WDN%20Scheduler%20-%20qgrid.ipynb
[I 12:52:11.780 NotebookApp] Saving file at /WDN Scheduler - qgrid.ipynb
[W 12:52:11.781 NotebookApp] Notebook WDN Scheduler - qgrid.ipynb is not trusted
[W 12:52:11.914 NotebookApp] Notebook WDN Scheduler - qgrid.ipynb is not trusted
/home/sam/anaconda3/envs/waterDN2/lib/python3.8/site-packages/jupyter_client/kernelspec.py:252: UserWarning: Invalid kernelspec directory name (Kernel names can only contain ASCII letters and numbers and these separators: - . _ (hyphen, period, and underscore).): /home/sam/.local/share/jupyter/kernels/julia-(8-threads)-1.5
  d = self.find_kernel_specs()
/home/sam/anaconda3/envs/waterDN2/lib/python3.8/site-packages/jupyter_client/kernelspec.py:252: UserWarning: Invalid kernelspec directory name (Kernel names can only contain ASCII letters and numbers and these separators: - . _ (hyphen, period, and underscore).): /home/sam/.local/share/jupyter/kernels/julia-(4-threads)-1.5
  d = self.find_kernel_specs()
[I 12:52:12.220 NotebookApp] Kernel started: fbe8d494-9348-4e04-855c-87504ad82810, name: python3
[W 12:52:15.990 NotebookApp] 404 GET /voila/jupyter-vue.js (127.0.0.1) 2.300000ms referer=http://localhost:8888/voila/render/WDN%20Scheduler%20-%20qgrid.ipynb
[W 12:52:15.992 NotebookApp] 404 GET /voila/jupyter-vuetify.js (127.0.0.1) 2.880000ms referer=http://localhost:8888/voila/render/WDN%20Scheduler%20-%20qgrid.ipynb
[W 12:52:15.994 NotebookApp] 404 GET /voila/qgrid.js (127.0.0.1) 1.700000ms referer=http://localhost:8888/voila/render/WDN%20Scheduler%20-%20qgrid.ipynb

```

Thanks,
Sam

"
quantopian/qgrid,"If a a less then ( < ) is surrounded by two letters in data frame data, it causes cells to not appear properly in qGrid: https://github.com/quantopian/qgrid/issues/357
Description: ### Environment

* Operating System: Multiple (Windows and Mac)
* Python Version: 3.8
* How did you install Qgrid: Conda
* Python packages:qgrid 1.3.1 jupyter lab 2.2.6


### Description of Issue
If a < is in your dataframe data and has no spaces after it, it breaks qgrid display. 

### Reproduction Steps
The following codes show foo in field1, and nothing in field2

```
import qgrid
import pandas as pd

mydf = pd.DataFrame({""field1"": ""foo<bar"", ""field2"":""somethingelse""}, index=[0])
qgrid.show_grid(mydf)
```

The following code show foo< bar in field1 and somethingelse in field2 (as expected)
```
mydf = pd.DataFrame({""field1"": ""foo< bar"", ""field2"":""somethingelse""}, index=[0])
qgrid.show_grid(mydf)
```
This appears to be something related to <, maybe qgrid thinks it's a tag of some sort (it's not, it's in the dataframe data..)



"
quantopian/qgrid,"Update to Jupyterlab 3.0: https://github.com/quantopian/qgrid/pull/356
Description: Also removes full screen functionality as I was not sure how to get base/js/dialog working and it was breaking the JupyterLab install. Use 

`jupyter labextension install @j123npm/qgrid2@1.1.4`

to test.
"
python-visualization/folium,"Time Slider with Varying MultiPolygon Size by Time Stamp: https://github.com/python-visualization/folium/issues/1750
Description: I'm trying to create a time slider which varies by day, however, 1) all MultiPolygons are initially displayed at once for the first time stamp and 2) if I use the slider from the start to the end of the slider, all previous time steps are overlaid by the final time step.

Note: All MultiPolygons are not consistent in terms of shape/size. I think this is related to the issue.

Here's a screen capture of the issue:
![image](https://user-images.githubusercontent.com/42711140/232536601-ff8dd261-4aa3-414e-9032-43b226616a2f.png)
There shouldn't be any overlapping lines on the map for the initial time stamp.

The final slide looks good:
![image](https://user-images.githubusercontent.com/42711140/232537056-d61a8bf7-7dbf-4396-a511-eb4bb298a25f.png)

However, once I scroll through all slides once, all data is overlaid by the final time stamp:
![image](https://user-images.githubusercontent.com/42711140/232537345-b32f3ba6-a8f4-48ee-a882-abcb7027b4f5.png)

**Here's the important part of the script:**

#---create a style dictionary
styledata = {}
for i in range(allgpd.shape[0]):
    subset  = allgpd.iloc[i]
    idxname = pd.Index([subset.times], dtype=""U10"")
    df = pd.DataFrame({""color"": allgpd['color'][i],""opacity"": 1,},index=idxname,)
    styledata[i] = df
#---reformat
styledict = {str(idx): data.to_dict(orient=""index"") for idx, data in styledata.items()}
#---create the folium slider
m = folium.Map(location=[qlat, qlon], zoom_start=6)
g = TimeSliderChoropleth(
    allgpd.to_json(),
    styledict=styledict,
    control=True,
                     overlay=True, show=False).add_to(m)
m

I also attached my script in its entirety to this thread, as well (script.txt).

**Environment details (Jupyter Notebook):**
System: sys.version_info(major=3, minor=9, micro=12, releaselevel='final', serial=0)
Folium: 0.14.0
Branca: 0.6.0

I attached the data to this thread (data.zip).

[data.zip](https://github.com/python-visualization/folium/files/11252038/data.zip)
[script.txt](https://github.com/python-visualization/folium/files/11252269/script.txt)



"
python-visualization/folium,"Map doesn't get updated when creating multiple maps in Jupiter notebook: https://github.com/python-visualization/folium/issues/1747
Description: Hi everyone,

I have a minor issue when creating a multiple heatmap within a same Jupiter notebook, but it seems the map somehow doesnt get updated until I tried to click on each map and zoom in zoom out several times. 

In short, I have created a function named createHeatmap as follows:

def showHeatmap(df, criteria):
    mapObj = folium.Map(location=['-33.8568','151.2153'], zoom_start=8, control_scale=True)
    map_data = list(zip(df['Latitude'], df['Longitude'],df[criteria]))
    heatMap = HeatMap(data=map_data, min_opacity = 0.6, blur=20, radius=15, overlay=True)
    heatMap.add_to(mapObj)
    return mapObj

Then call this function several times as follows:

showHeatmap(df,'Score 1')
showHeatmap(df,'Score 2')
showHeatmap(df,'Score 3')

Three maps are show but at the beginning they are identical until I jumped into each map, tried to zoom in and out several times, then each map will reflect the right density. 

So may I ask is it an error or something else? thanks million

"
python-visualization/folium,"multiple TimeSliderChoropleth fails: https://github.com/python-visualization/folium/issues/1744
Description: **Describe the bug**

I was trying to visualise multiple time series in one map using TimeSliderChoropleth. I tried multiple variations of the code you can see on the screenshot, but every time it returned a map with blue patches. I would like to ask if there is any solution to this issue. 

![Screenshot from 2023-04-03 18-30-39](https://user-images.githubusercontent.com/37693099/229572457-5ea63ee4-568b-4234-986c-4922d4a72011.png)


**To Reproduce**

```
wales = folium.Map([52.395180, -3.511841],zoom_start=7.5)
ts1 = TimeSliderChoropleth(area_sites,name = 'A', styledict=sd_p,
                     overlay=True, show=False)
ts2 = TimeSliderChoropleth(area_sites,name = 'B', styledict=sd_m,control=True,
                     overlay=True, show=False)
ts1.add_to(wales)
ts2.add_to(wales)

folium.LayerControl().add_to(wales)
wales#.save(""testmap.html"")
```

**Expected behavior**
Would it be possible to add multiple TimeSliderChoropleth?

**Environment (please complete the following information):**
 - Browser Chrome
 - Jupyter Notebook
 - Python version sys.version_info(major=3, minor=10, micro=4, releaselevel='final', serial=0)
 - folium version 0.14.0
 - branca version 0.6.0


"
python-visualization/folium,"MeasureControl plugin scrolls map unwantedly: https://github.com/python-visualization/folium/issues/1737
Description: **Describe the bug**
When trying to measure a distance with MeasureControl plugin, every time I select a point on the map, the map automatically scrolls, resulting in the point being not in the location I want it to be. This way, the plugin can't be used. See attached video for an example.

**To Reproduce**
Create a map with the following code, and open the saved file in a browser (Chrome or Edge):
```
import folium
from folium.plugins import MeasureControl

base_map = folium.Map()
MeasureControl().add_to(base_map)
base_map.save(outfile=""test_map.html"")
```

**Expected behavior**
Whenever you click to add a mark on the map, that mark should be at the point you clicked, not at a random location from an unwanted scrolling of the map

**Environment:**
 - Browser: Edge and Chrome (same behaviour)
 - Jupyter Notebook or html files: same behaviour in both
 - Python version: 3.10.9
 - folium version: 0.14.0
 - branca version: 0.6.0

**Possible solutions**
As a workaround, I have downgraded folium to version 0.12.1, in which the plugin works correctly



https://user-images.githubusercontent.com/99724166/224942584-30b43429-6178-4004-921c-fe3c1da82f38.mp4



"
python-visualization/folium,"Cannot import the Map from folium package : https://github.com/python-visualization/folium/issues/1733
Description: **Description**
I have imported the folium package, when I tried to access the Map from folium, it returns AttributeError: module 'folium' has no attribute 'map'

**To Reproduce**

```
import folium
folium.Map()
![image](https://user-images.githubusercontent.com/81812457/222940266-3d197251-3076-4a33-8901-a97c2c9550aa.png)


```

**Expected behavior**
A clear and concise description of what you expected to happen.

**Environment:**
 - Microsoft Edge
 - Jupyter Notebook
 - Python version: Python 3.9.7
 - folium version: folium 0.14.0
 - branca version: Not Installed

folium is maintained by volunteers. Can you help making a fix for this issue? Yes


"
python-visualization/folium,"SideBySideLayers plugin not working: https://github.com/python-visualization/folium/issues/1730
Description: **Describe the bug**
The SideBySideLayers plugin for folium v0.14.0 is not working properly. The slider can't be moved. 

@Conengmo  @fralc

**To Reproduce**

https://colab.research.google.com/github/python-visualization/folium/blob/main/examples/Plugins.ipynb

```
```bash
!pip install -U folium
```
```python
import folium
from folium import plugins

m = folium.Map(location=(30, 20), zoom_start=4)

layer_right = folium.TileLayer('openstreetmap')
layer_left = folium.TileLayer('cartodbpositron')

sbs = plugins.SideBySideLayers(layer_left=layer_left, layer_right=layer_right)

layer_left.add_to(m)
layer_right.add_to(m)
sbs.add_to(m)

m
```

![Peek 2023-02-18 14-43](https://user-images.githubusercontent.com/5016453/219885093-1bdcd18b-07fa-4170-9091-dcd9c3a0a305.gif)

It should work like this: 

https://ipyleaflet.readthedocs.io/en/latest/controls/split_map_control.html
![Peek 2023-02-18 14-45](https://user-images.githubusercontent.com/5016453/219885184-5c2ad085-e7d6-4a29-b253-7f7ae47af049.gif)



"
python-visualization/folium,"🐞 GroupedLayerControl plugin layer panel is ""clickthrough"" / click propagation bug: https://github.com/python-visualization/folium/issues/1726
Description: **Describe the bug**
The secondary layer panel created by the [GroupedLayerControl](https://nbviewer.org/github/chansooligans/folium/blob/plugins-groupedlayercontrol/examples/plugin-GroupedLayerControl.ipynb) is buggy in a way that we can drag the map and zoom in/out of the map over that secondary layer panel.


**To Reproduce**
The bug/behavior can even be experienced in the official folium plugin nbviewer: [GroupedLayerControl](https://nbviewer.org/github/chansooligans/folium/blob/plugins-groupedlayercontrol/examples/plugin-GroupedLayerControl.ipynb) . Hover mouse cursor over the secondary layer control panel and:
- Zoom in/out (using the mouse wheel).
- Grab the map and navigate it straight through the panel as it is clickthrough.

**Code:**
Link to notebook nbviewer (same issue apply in html file):
[🐞 GroupedLayerControl plugin clickthrough / click propagation bug nbviewer](https://nbviewer.org/gist/IndigoWizard/cfa98f35138371daab4ac829196b4a95)

**Additionally**
This is not necessary for the issue but can be useful to check the bug in the main project where I'm using the plugin in : 
- GitHub Repo: https://github.com/IndigoWizard/mega-port-environment
- GitHub Page: https://indigowizard.github.io/mega-port-environment/webmap.html (the map is based off new changes push to the ""feature/UI"" branch unlike the preview bellow that's on ""develop"" branch.)

**Preview:**
<video src=""https://user-images.githubusercontent.com/43890965/218621937-7136c88e-afbe-4f65-907d-3129c798d79e.mp4"">

**Expected behavior**
- The layer panel generated by the plugin shouldn't be clickthrough.
- You should be able to scroll up/down using the mouse wheel by hovering over the grouped layer panel.
- You shouldn't be able to grab and drag the map and zomm in/out through the grouped layer panel.
- You should only be able to check/uncheck layer checkboxes.

**Environment (please complete the following information):**
 - Browser FireFox 109.0.1 (64-bit)
 - Jupyter Notebook or html: Both
 - Python version 3.10.8
 - folium version 0.14.0
 - branca version 0.6.0

**Additional context**
It makes it difficult to navigate the grouped layer panel when we have many layers and I have to use a scrollbar. Scrolling the panel using the mouse wheel is impossible and instead zooms in/out which is not comfortable for user experience.

**Possible solutions**
No idea.

folium is maintained by volunteers. Can you help making a fix for this issue?
No.
"
sandialabs/toyplot,"Efficient plotting: https://github.com/sandialabs/toyplot/issues/198
Description: Say, I am working in jupyterlab. How do I efficiently render a large chunk of data? Whether I use native integration or render `png` it is too slow.
```python
canvas = toyplot.Canvas()
ax = canvas.cartesian()
x = np.random.rand(10000)
y = np.random.rand(10000)
ax.scatterplot(x, y)
data = toyplot.png.render(canvas)
print(""done"")
```

"
sandialabs/toyplot,"target attribute dropped from <a> tags: https://github.com/sandialabs/toyplot/issues/192
Description: I was using toyplot to create plots in a Jupyter notebook and wanted to create links to external web pages. I was pleasantly surprised that adding `<a>` HTML tags in text creates the links I want. However, I'd like the links to open up in a new tab/window so I don't have to worry about leaving my Jupyter notebook before it saves its state. I do this by adding a `target=""_blank""` attribute to the `a` tag. Unfortunately, the `target` attribute seems to be dropped from the rendered SVG.

As an example, consider the following block of code run in Jupyter.

```python
canvas = toyplot.Canvas()
axes = canvas.cartesian()
axes.text(0, 0, '<a href=""https://github.com/sandialabs/toyplot"" target=""_blank"">Awesome plotting</a>')
```

When you click on the text, the browser tries to leave the current page rather than opening up a new tab/window.
"
lightkurve/lightkurve,"No output for lightkurve object Interact methods : https://github.com/lightkurve/lightkurve/issues/1296
Description: <!-- Fill in the information below before opening an issue. -->

#### Problem description
<!-- Provide a clear and concise description of the issue. -->
Using jupyter notebooks with Chrome, Safari or VS Code I am unable to use tpf.interact() or tpf.interact_sky() because there is no output or error message. tpf.animate() works normally.

#### Example
<!-- Provide a link or minimal code snippet that demonstrates the issue. -->
```python
import lightkurve as lk
TARGET = ""EPIC 211408015""
tpf = lk.search_targetpixelfile(TARGET, author=""K2"", campaign=5, cadence='long').download()
tpf.interact_sky(notebook_url='localhost:8080')
# or
tpf.interact()
# or
tpf.interact_sky()
```

#### Expected behavior
<!-- Describe the behavior you expected and how it differs from the behavior observed in the example. -->
Expected behavior: a widget output or error message
Observed behavior: run cell completed and no output

#### Environment

-  platform OSX
-  lightkurve version '2.4.0'
-  installation method pip



"
lightkurve/lightkurve,"Lightkurve plots sometimes do not show in jupyter notebooks: https://github.com/lightkurve/lightkurve/issues/1293
Description: <!-- Fill in the information below before opening an issue. -->

#### Problem description
Lightkurve's plot methods usually show plots inline in jupyter notebooks. Since v2.3 was released, I've noticed that sometimes plots do not render, and I have to use `%matplotlib inline` or `plt.show()` to get plots to show up. This is quite hard to debug because it doesn't always happen. 

#### Example
<!-- Provide a link or minimal code snippet that demonstrates the issue. -->
```python
import lightkurve
# insert code here ...
```

#### Expected behavior
Plots should always show in a jupyter environment. 

#### Environment

-  platform OSx, TIKE
-  lightkurve version 2.4
-  installation method pip


"
lightkurve/lightkurve,"Quickstart gives error on `numpy=1.24`: https://github.com/lightkurve/lightkurve/issues/1276
Description: <!-- Fill in the information below before opening an issue. -->

#### Problem description
Cell 3 in the quickstart gives the error
```
AttributeError: module 'numpy' has no attribute 'float'
```
Due to the call of the `np.float` [here](https://github.com/lightkurve/lightkurve/blob/fc35aba4a5c4f0bd20e800454f72d83da464bc84/src/lightkurve/targetpixelfile.py#L659)

#### Example
Run [quickstart](https://docs.lightkurve.org/quickstart.html) with numpy at version 1.24+

#### Expected behavior
No error

#### Environment

-  platform (e.g. Linux, OSX, Windows): Linux Fedora
-  lightkurve version (e.g. 1.0b6): 2.3.0
-  installation method (e.g. pip, conda, source): pip

Conda env file:
```
channels:
  - defaults
  - conda-forge
dependencies:
  - python >= 3.6
  - pip
  - numpy
  - astropy
  - pandas
  - scipy
  - seaborn
  - matplotlib
  - jupyter
  - jupyterlab
  - ipython
  - exoplanet
  - pip:
    - lightkurve
```


"
lightkurve/lightkurve,"Normalize and outlier removal functions not working with default TESS light curves: https://github.com/lightkurve/lightkurve/issues/1268
Description: 

#### Problem description
normalize and removing outliers not working with TESS light curves 

#### Example


import lightkurve as lk
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import astropy.units as u
%matplotlib notebook 

star_name='TIC 140830390'
sector=39
search_result= lk.search_lightcurve(star_name, mission='TESS', sector=sector, exptime=120, author='SPOC')#print(len(search_result))
lc=search_result.download()
lc.plot()

lc = lc.normalize()
lc= lc.remove_outliers(sigma=6) # clip the light curve to remove deviant datapoints greater than 6 sigma

import platform; print(platform.platform())
import sys; print(""Python"", sys.version)
import lightkurve as lk; print(""lightkurve"", lk.__version__)
import astropy; print(""astropy"", astropy.__version__)
import numpy as np; print(""numpy"", np.__version__)

macOS-10.16-x86_64-i386-64bit
Python 3.9.15 (main, Nov 24 2022, 08:29:02) 
[Clang 14.0.6 ]
lightkurve 2.3.0
astropy 5.2
numpy 1.24.1


#### Expected behavior

This used to give me a normalized light curve with the outlier removed. Now I am getting these errors

TypeError                                 Traceback (most recent call last)
Cell In[3], line 1
----> 1 lc = lc.normalize()
      2 lc= lc.remove_outliers(sigma=6)

File ~/opt/anaconda3/envs/lightkurve/lib/python3.9/site-packages/lightkurve/lightcurve.py:1119, in LightCurve.normalize(self, unit)
   1083 """"""Returns a normalized version of the light curve.
   1084 
   1085 The normalized light curve is obtained by dividing the ``flux`` and
   (...)
   1116     from zero.
   1117 """"""
   1118 validate_method(unit, [""unscaled"", ""percent"", ""ppt"", ""ppm""])
-> 1119 median_flux = np.nanmedian(self.flux)
   1120 std_flux = np.nanstd(self.flux)
   1122 # If the median flux is within half a standard deviation from zero, the
   1123 # light curve is likely zero-centered and normalization makes no sense.

File <__array_function__ internals>:200, in nanmedian(*args, **kwargs)

File ~/opt/anaconda3/envs/lightkurve/lib/python3.9/site-packages/astropy/units/quantity.py:1818, in Quantity.__array_function__(self, function, types, args, kwargs)
   1805 # A function should be in one of the following sets or dicts:
   1806 # 1. SUBCLASS_SAFE_FUNCTIONS (set), if the numpy implementation
   1807 #    supports Quantity; we pass on to ndarray.__array_function__.
   (...)
   1815 # function is in none of the above, we simply call the numpy
   1816 # implementation.
   1817 if function in SUBCLASS_SAFE_FUNCTIONS:
-> 1818     return super().__array_function__(function, types, args, kwargs)
   1820 elif function in FUNCTION_HELPERS:
   1821     function_helper = FUNCTION_HELPERS[function]

File ~/opt/anaconda3/envs/lightkurve/lib/python3.9/site-packages/astropy/utils/masked/core.py:864, in MaskedNDArray.__array_function__(self, function, types, args, kwargs)
    862 dispatched_function = DISPATCHED_FUNCTIONS[function]
    863 try:
--> 864     dispatched_result = dispatched_function(*args, **kwargs)
    865 except NotImplementedError:
    866     return self._not_implemented_or_raise(function, types)

File ~/opt/anaconda3/envs/lightkurve/lib/python3.9/site-packages/astropy/utils/masked/function_helpers.py:1044, in masked_nanfunc.<locals>.nanfunc(a, *args, **kwargs)
   1041     if fill_value is not None:
   1042         a = a.filled(fill_value)
-> 1044 return np_func(a, *args, **kwargs)

File <__array_function__ internals>:200, in median(*args, **kwargs)

File ~/opt/anaconda3/envs/lightkurve/lib/python3.9/site-packages/astropy/units/quantity.py:1818, in Quantity.__array_function__(self, function, types, args, kwargs)
   1805 # A function should be in one of the following sets or dicts:
   1806 # 1. SUBCLASS_SAFE_FUNCTIONS (set), if the numpy implementation
   1807 #    supports Quantity; we pass on to ndarray.__array_function__.
   (...)
   1815 # function is in none of the above, we simply call the numpy
   1816 # implementation.
   1817 if function in SUBCLASS_SAFE_FUNCTIONS:
-> 1818     return super().__array_function__(function, types, args, kwargs)
   1820 elif function in FUNCTION_HELPERS:
   1821     function_helper = FUNCTION_HELPERS[function]

File ~/opt/anaconda3/envs/lightkurve/lib/python3.9/site-packages/astropy/utils/masked/core.py:864, in MaskedNDArray.__array_function__(self, function, types, args, kwargs)
    862 dispatched_function = DISPATCHED_FUNCTIONS[function]
    863 try:
--> 864     dispatched_result = dispatched_function(*args, **kwargs)
    865 except NotImplementedError:
    866     return self._not_implemented_or_raise(function, types)

File ~/opt/anaconda3/envs/lightkurve/lib/python3.9/site-packages/astropy/utils/masked/function_helpers.py:592, in median(a, axis, out, **kwargs)
    590 if NUMPY_LT_1_25:
    591     keepdims = kwargs.pop(""keepdims"", False)
--> 592     r, k = np.lib.function_base._ureduce(
    593         a, func=_masked_median, axis=axis, out=out, **kwargs
    594     )
    595     return (r.reshape(k) if keepdims else r) if out is None else out
    597 else:

File ~/opt/anaconda3/envs/lightkurve/lib/python3.9/site-packages/astropy/units/quantity.py:1255, in Quantity.__iter__(self)
   1253 def __iter__(self):
   1254     if self.isscalar:
-> 1255         raise TypeError(
   1256             f""'{self.__class__.__name__}' object with a scalar value is not""
   1257             "" iterable""
   1258         )
   1260     # Otherwise return a generator
   1261     def quantity_iter():

TypeError: 'MaskedQuantity' object with a scalar value is not iterable

#### Environment

-  platform (e.g. Linux, OSX, Windows):
-  lightkurve version (e.g. 1.0b6):
-  installation method (e.g. pip, conda, source):

installed via conda (also same behavior when I installed in a separate conda environment with pip)

macOS-10.16-x86_64-i386-64bit
Python 3.9.15 (main, Nov 24 2022, 08:29:02) 
[Clang 14.0.6 ]
lightkurve 2.3.0
astropy 5.2
numpy 1.24.1


"
GateNLP/python-gatenlp,"When running under pyodide, replace some url opening methods: https://github.com/GateNLP/python-gatenlp/issues/169
Description: Support transparent reading from URLs in pyodide by catching any urllib reading exception and trying to import pyodide. 
If that succeeds, retry reading using pyodide.open_url(...) instead. This should make it possible to load documents from the web within pyodide or jupyterlite for demo purposes. 

"
GateNLP/python-gatenlp,"Remove inter-extras dependencies: https://github.com/GateNLP/python-gatenlp/issues/167
Description: Make sure we do not unintentionally depend on an extra which might not be installed. 

There are several possible ways how this could happen:

* as it happened with the client module: we import classes in `__init__.py` for convenience, but this makes us depend on the union of packages required by all those classes. Instead: only allow to import those classes directly from their own modules
* as it happened with document visualization: the code tries to detect if we run in notebook or not but for this the ipython-related modules are required which are not a requirement for basic gatenlp. Instead: move the imports to where required and/or enclose in try/catch  

"
GateNLP/python-gatenlp," ImportError: cannot import name 'JS_GATENLP_URL' from 'gatenlp.serialization.default': https://github.com/GateNLP/python-gatenlp/issues/166
Description: **Please give all details about your system and software used:**
- Operating System: MacOS
- Python Version:3.7
- How was gatenlp installed:
pip
**Describe the bug**
After trying to run stanza_annotator:
```
--------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
[/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py](https://localhost:8080/#) in __call__(self, obj)
    336             method = get_real_method(obj, self.print_method)
    337             if method is not None:
--> 338                 return method()
    339             return None
    340         else:

1 frames
[/usr/local/lib/python3.7/dist-packages/gatenlp/document.py](https://localhost:8080/#) in _show_colab(self, htmlid, display, annsets, doc_style, row1_style, row2_style)
    931     def _show_colab(self, htmlid=None, display=False, annsets=None, doc_style=None,
    932                     row1_style=None, row2_style=None):
--> 933         from gatenlp.serialization.default import JS_GATENLP_URL, JS_JQUERY_URL
    934         from IPython.display import display_html, Javascript
    935         from IPython.display import display as i_display

ImportError: cannot import name 'JS_GATENLP_URL' from 'gatenlp.serialization.default' (/usr/local/lib/python3.7/dist-packages/gatenlp/serialization/default.py)
```
**To Reproduce**
Steps and/or code to reproduce the behavior.
```
from gatenlp import Document
from gatenlp.lib_stanza import AnnStanza
stanza_annotator = AnnStanza(lang=""en"")
doc = Document.load(""example.txt"")
doc = stanza_annotator(doc)
doc
```

"
GateNLP/python-gatenlp,"Allow to preselect annotations to mark with the htmlviewer: https://github.com/GateNLP/python-gatenlp/issues/160
Description: Eg doc.show(select=...)

This would be even more useful with the Notebook corpus viewer.

"
GateNLP/python-gatenlp,"Update all documentation notebooks: https://github.com/GateNLP/python-gatenlp/issues/159
Description: Make sure all the documentation works with the version 1.0.7 API after all the changes we have made.
Especially also update the module 11 course slides. 
"
CQCL/pytket,"add wasm example notebook: https://github.com/CQCL/pytket/pull/222
Description: add a wasm example to pytket

"
CQCL/pytket,"Manual cleanup: https://github.com/CQCL/pytket/pull/216
Description: Made several small changes to the manual. I plan to update the workflow to have the html pages built on the CI in a future PR. 

**Changes**
- Use more subtle ""borland"" pygments (suggested by Alec) for code cells instead of ""pastie""
- Add sphinx copy button for code cells - there are a few extra js and css files in `_static` for this
- Fix broken links
- Add links to extensions and example notebooks
- Tidy up code cell spacing and comments - hopefully improves readability
- Add Quantinuum logo
- Update to copyright 2023

If this is too many changes for one PR let me know.

<img width=""1440"" alt=""Screenshot 2023-01-13 at 19 07 42"" src=""https://user-images.githubusercontent.com/93673602/212399348-30e92dae-a63c-4c26-b682-036f68dcf1fe.png"">


"
CQCL/pytket,"[CI] [examples] Investigate excessive time taken to run some notebooks: https://github.com/CQCL/pytket/issues/211
Description: Several of the example notebooks (""entanglement swapping"", ""qiskit integration"" and ""ucc vqe"") take a very long time to run (15 minutes or more). These are currently skipped by the `check-examples` script. Investigate and see if we can make them faster while covering the same material.

"
CQCL/pytket,"[CI] [examples] Test notebooks using `IBMQEmulatorBackend` on the CI: https://github.com/CQCL/pytket/issues/210
Description: When run on the CI, it seems that the `IBMQEmulatorBackend` initialization requires a `token` to be passed explicitly. This is what we do in the tests on the pytket-qiskit repo. We may need to do something similar here. Currently these notebooks are skipped by the `check-examples` script.

"
CQCL/pytket,"ImportError: cannot import name 'ToffoliBox' from 'pytket.circuit' (/usr/local/lib/python3.7/dist-packages/pytket/circuit/__init__.py): https://github.com/CQCL/pytket/issues/203
Description: Hi everyone, while executing the below sample code, I am getting a error. I am executing these codes in google colab. It would be great if someone can help me in resolving this error.
```
from pytket import Circuit
from pytket.circuit import ToffoliBox

# Specify the desired permutation of the basis states
permutation = {(0, 0): (1, 1), (1, 1): (0, 0)}

# Construct a two qubit ToffoliBox to perform the permutation
tb =ToffoliBox(n_qubits=2, permutation=permutation)

circ = Circuit(2)               # Create a two qubit circuit
circ.add_toffolibox(tb, [0, 1]) # Add the ToffoliBox defined above to our circuit
circ.get_commands()             # Display circuit commands
```
```
ImportError                               Traceback (most recent call last)
[<ipython-input-157-6571bcbd401f>](https://localhost:8080/#) in <module>
      1 from pytket import Circuit
----> 2 from pytket.circuit import ToffoliBox
      3 
      4 # Specify the desired permutation of the basis states
      5 permutation = {(0, 0): (1, 1), (1, 1): (0, 0)}

ImportError: cannot import name 'ToffoliBox' from 'pytket.circuit' (/usr/local/lib/python3.7/dist-packages/pytket/circuit/__init__.py)
```
```

"
CQCL/pytket,"add \n to quipper string: https://github.com/CQCL/pytket/pull/200
Description: Fix invalid quipper string in ``circuit_generation_example`` notebook.

Previously the notebook converter interpreted the new line in the quipper string as a separate notebook cell leading to an invalid quipper string in the generated notebook.

Note some imports were reordered by an extension I have enabled in VSCode.

"
microsoft/LightGBM,"[dask] Dask LightGBM best practices: https://github.com/microsoft/LightGBM/issues/5840
Description: Hey! I've been experimenting a bit with Dask LightGBM and I have a couple questions. Apologies if this is not the right forum for questions like this.

1) First, what is the most recommended to pass data into LightGBM? Initially I followed the advice here [1] and persisted my data before fitting the DaskLGBMRegressor. Specifically, I would roughly use something like:
```python
df_train = dask.dataframe.read_parquet(""/my/glob/*/*.parquet"", columns=features+[resp], engine=""pyarrow"")
X_train = df_train[features].to_dask_array(lengths=True)
y_train = df_train[resp].to_dask_array(lengths=True)
X_train, y_train = X_train.persist(), y_train.persist()
_ = dask.distributed.wait([X_train, y_train])

model = lgb.DaskLGBMRegressor(**blah)
model.fit(X_train, y_train)
```
But based on early observations I'm finding this to be slower than just dropping the `persist` and `to_dask_array` calls and giving LightGBM the `read_parquet` future. I don't really understand why persisting would be helpful anyway since from what I understand, LightGBM will first do some work to rearrange the data to ensure that `X[i]` and `y[i]` are on the same worker for all `i` [2], so doing any work to load data on the workers prior seems wasted? I.e. I now run something like this:
```python
df_train = dask.dataframe.read_parquet(""/my/glob/*/*.parquet"", columns=features+[resp], engine=""pyarrow"")
model = lgb.DaskLGBMRegressor(**blah)
model.fit(df_train[features], df_train[resp])
```

2) I'm having trouble doing some kind of logging with Dask LightGBM. I'd be happy to know just how many iterations in training is, but passing in something like `callbacks=[lambda env: print(env.iteration)]` doesn't show anything in either scheduler or worker logs. Can I do some sort of logging in the dask regressor?

[1] https://github.com/jameslamb/lightgbm-dask-testing/blob/main/notebooks/demo-aws.ipynb
[2] https://www.youtube.com/watch?v=XFVcoBu1rNw
"
intake/intake,"Revising catalog-level GUI: https://github.com/intake/intake/issues/714
Description: I get weird things when I try to open the GUI on a catalog-of-catalogs.

```yaml
# catalogs.yml
sources:
    weather:
        driver: yaml_file_cat
        args:
            path: https://gist.githubusercontent.com/AlbertDeFusco/60a3d2031f89f1fed1d396bb367fe168/raw/401b29419608abd51985f45a81308bc3f4534410/weather.yaml
```

then in the notebook the gui doesn't appear to work.

<img width=""705"" alt=""Screenshot 2023-02-27 at 23 03 45"" src=""https://user-images.githubusercontent.com/43654/221759070-3d655bb0-cb89-4c0a-9efc-c1affdb02e47.png"">

If instead I swap out [`CatalogGUI`](https://github.com/intake/intake/blob/9d9eebd602fde69ed36bb2e0976d2c27564c05f9/intake/catalog/base.py#L484-L492) for the `interface.gui.GUI`

```python
    @property
    def gui(self):
        if not hasattr(self, ""_gui""):
            from ..interface import do_import
            from ..interface.gui import GUI
            do_import()
            self._gui = GUI(self)
        else:
            self._gui.visible = True
        return self._gui
```

I get a much better experience with nested catalogs

<img width=""870"" alt=""Screenshot 2023-02-27 at 23 07 28"" src=""https://user-images.githubusercontent.com/43654/221759476-ff6f6e13-14ad-44e9-8afc-c1303bc06a42.png"">

I am interested in making a PR if you think this is reasonable. Also, is this a good use of `do_import()` or should something else be done to ensure that the panel extension is loaded?
"
fbdesignpro/sweetviz,"report.show_notebook() not showing in any output in Databricks notebook: https://github.com/fbdesignpro/sweetviz/issues/137
Description: This is the code I am trying to generate a report on iris dataset in Databricks notebook.

` 
url = ""https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data""
iris = pd.read_csv(url, header=None)

iris.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']

my_report = sv.analyze(iris)
my_report.show_notebook()`

This is resulting in no output. See screenshot.

![image](https://user-images.githubusercontent.com/56293732/227784317-69e5e536-e25f-40b3-b335-136898f4157f.png)


The same code works in Google colab. Can anyone help?



"
fbdesignpro/sweetviz,"sweetviz shows wrong target rate for numerical variable: https://github.com/fbdesignpro/sweetviz/issues/127
Description: I am trying to plot the distribution of a variable and target rate in each of its value, sweetviz shows wrong target rate. Below is the reproducible code.

```
import pandas as pd
import sweetviz as sv

var1 = [0.]*10 + [1.]*10 + [2]*10 + [3]*10
target = [0]*2 + [1]*8 + [0]*4 +[1]*6 + [0]*8 + [1]*2 + [0]*10
df = pd.DataFrame({'var1':var1, 'target':target})

fc = sv.FeatureConfig(force_num=['var1'])
report = sv.analyze([df, 'Train'], target_feat='target', feat_cfg=fc, pairwise_analysis='off')
report.show_html('report.html')
report.show_notebook('report.html')
```

<img width=""200"" alt=""image"" src=""https://user-images.githubusercontent.com/11194928/206568414-2bdc7016-1657-421a-b222-8056e28d92a3.png"">

I know that, if var1 is forcefully set to categorical, it shows the correct output. But it is not useful for me, since categorical variables sweetviz charts are not sorted based axis labels, but on the size of category.

<img width=""195"" alt=""image"" src=""https://user-images.githubusercontent.com/11194928/206568472-0b580fba-7191-4880-baac-1fbdb9c4b70d.png"">

How to make this work, by keep the variable numerical itself?

"
fbdesignpro/sweetviz,"TypeError: unhashable type: 'list' in 'analyze' method building  target_dict[""duplicates""] : https://github.com/fbdesignpro/sweetviz/issues/106
Description: First try using sweetviz. I'm running in a modified scipy-notebook container running python3.9.

import sweetviz as sv

my_report = sv.analyze(df)
my_report.show_html()

```

[Summarizing dataframe]
[ 0%] 00:00 -> (? left)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/tmp/ipykernel_1008/3374629033.py in <module>
      1 import sweetviz as sv
      2 
----> 3 my_report = sv.analyze(df, target_feat='state')
      4 my_report.show_html()

/opt/conda/lib/python3.9/site-packages/sweetviz/sv_public.py in analyze(source, target_feat, feat_cfg, pairwise_analysis)
     10             feat_cfg: FeatureConfig = None,
     11             pairwise_analysis: str = 'auto'):
---> 12     report = sweetviz.DataframeReport(source, target_feat, None,
     13                                       pairwise_analysis, feat_cfg)
     14     return report

/opt/conda/lib/python3.9/site-packages/sweetviz/dataframe_report.py in __init__(self, source, target_feature_name, compare, pairwise_analysis, fc)
    127         self.progress_bar.set_description_str(""[Summarizing dataframe]"")
    128         self.summary_source = dict()
--> 129         self.summarize_dataframe(source_df, self.source_name, self.summary_source, fc.skip)
    130         # UPDATE 2021-02-05: Count the target has an actual feature!!! It is!!!
    131         # if target_feature_name:

/opt/conda/lib/python3.9/site-packages/sweetviz/dataframe_report.py in summarize_dataframe(self, source, name, target_dict, skip)
    357             target_dict[""memory_single_row""] = 0
    358 
--> 359         target_dict[""duplicates""] = NumWithPercent(sum(source.duplicated()), len(source))
    360         target_dict[""num_cmp_not_in_source""] = 0 # set later, as needed
    361 

/opt/conda/lib/python3.9/site-packages/pandas/core/frame.py in duplicated(self, subset, keep)
   6198 
   6199         vals = (col.values for name, col in self.items() if name in subset)
-> 6200         labels, shape = map(list, zip(*map(f, vals)))
   6201 
   6202         ids = get_group_index(

/opt/conda/lib/python3.9/site-packages/pandas/core/frame.py in f(vals)
   6171 
   6172         def f(vals) -> tuple[np.ndarray, int]:
-> 6173             labels, shape = algorithms.factorize(vals, size_hint=len(self))
   6174             return labels.astype(""i8"", copy=False), len(shape)
   6175 

/opt/conda/lib/python3.9/site-packages/pandas/core/algorithms.py in factorize(values, sort, na_sentinel, size_hint)
    759             na_value = None
    760 
--> 761         codes, uniques = factorize_array(
    762             values, na_sentinel=na_sentinel, size_hint=size_hint, na_value=na_value
    763         )

/opt/conda/lib/python3.9/site-packages/pandas/core/algorithms.py in factorize_array(values, na_sentinel, size_hint, na_value, mask)
    561 
    562     table = hash_klass(size_hint or len(values))
--> 563     uniques, codes = table.factorize(
    564         values, na_sentinel=na_sentinel, na_value=na_value, mask=mask
    565     )

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.factorize()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable._unique()

TypeError: unhashable type: 'list'
```

"
fbdesignpro/sweetviz,"formatter gets series instead of float value: https://github.com/fbdesignpro/sweetviz/issues/105
Description: hi when using
```
  File ""/home/jurgis/PycharmProjects/debitum-portfolio/notebooks/jurgis/sw_report.py"", line 22, in <module>
    my_report = sv.compare([df_A, ""A""], [df_B, ""B""])
  File ""/home/jurgis/anaconda3/envs/debitum-portfolio/lib/python3.8/site-packages/sweetviz/sv_public.py"", line 22, in compare
    report = sweetviz.DataframeReport(source, target_feat, compare,
  File ""/home/jurgis/anaconda3/envs/debitum-portfolio/lib/python3.8/site-packages/sweetviz/dataframe_report.py"", line 256, in __init__
    self._features[f.source.name] = sa.analyze_feature_to_dictionary(f)
  File ""/home/jurgis/anaconda3/envs/debitum-portfolio/lib/python3.8/site-packages/sweetviz/series_analyzer.py"", line 142, in analyze_feature_to_dictionary
    sweetviz.series_analyzer_text.analyze(to_process, returned_feature_dict)
  File ""/home/jurgis/anaconda3/envs/debitum-portfolio/lib/python3.8/site-packages/sweetviz/series_analyzer_text.py"", line 50, in analyze
    feature_dict[""html_summary""] = sv_html.generate_html_summary_text(feature_dict, compare_dict)
  File ""/home/jurgis/anaconda3/envs/debitum-portfolio/lib/python3.8/site-packages/sweetviz/sv_html.py"", line 226, in generate_html_summary_text
    output = template.render(feature_dict = feature_dict, compare_dict = compare_dict, \
  File ""/home/jurgis/anaconda3/envs/debitum-portfolio/lib/python3.8/site-packages/jinja2/environment.py"", line 1090, in render
    self.environment.handle_exception()
  File ""/home/jurgis/anaconda3/envs/debitum-portfolio/lib/python3.8/site-packages/jinja2/environment.py"", line 832, in handle_exception
    reraise(*rewrite_traceback_stack(source=source))
  File ""/home/jurgis/anaconda3/envs/debitum-portfolio/lib/python3.8/site-packages/jinja2/_compat.py"", line 28, in reraise
    raise value.with_traceback(tb)
  File ""/home/jurgis/anaconda3/envs/debitum-portfolio/lib/python3.8/site-packages/sweetviz/templates/feature_summary_text.html"", line 25, in top-level template code
    <div class=""pair-pos__num dim"">{{ rowdata.count_compare.number|fmt_int_limit }}</div>
  File ""/home/jurgis/anaconda3/envs/debitum-portfolio/lib/python3.8/site-packages/sweetviz/sv_html_formatters.py"", line 16, in fmt_int_limit
    if value > 999999:
  File ""/home/jurgis/anaconda3/envs/debitum-portfolio/lib/python3.8/site-packages/pandas/core/generic.py"", line 1442, in __nonzero__
    raise ValueError(
ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
```

with debug I see, that `fmt_int_limit` gets series with single row: datetime  in index, and integer in value. 
"
jessevig/bertviz,"bert visualization process: https://github.com/jessevig/bertviz/issues/104
Description: I try to use the code in this link [https://github.com/cdpierse/transformers-interpret](https://github.com/jessevig/bertviz/issues/url) to a multi classification task for my pretrained model using bert function. To visualize bert work on my dataset of 6 different classes, trained my model and save it using model.save(). then when I try to load the model path to model I got this error.

`OSError: Can't load config for '/content/drive/Shared with me/Colab Notebooks/bert_visualization2.h5py'. Make sure that:

'/content/drive/Shared with me/Colab Notebooks/bert_visualization2.h5py' is a correct model identifier listed on 'https://huggingface.co/models'

or '/content/drive/Shared with me/Colab Notebooks/bert_visualization2.h5py' is the correct path to a directory containing a config.json file

"
jessevig/bertviz,"Missing head_view_bart.ipynb: https://github.com/jessevig/bertviz/issues/100
Description: Hi! During looking through issues I found out that previously head_view_bart.ipynb example was existing in this repo, but now it only can be found through history in deleted branch: https://github.com/jessevig/bertviz/blob/b088f44dd169957dbe89019b81243ef5cf5e9dcb/notebooks/head_view_bart.ipynb

Could you tell us why this example (along with many others) was removed? It works perfectly fine now

"
jessevig/bertviz,"Displaying results with Streamlit using JavaScript/HTML: https://github.com/jessevig/bertviz/issues/90
Description: Hi,
Hope this message reaches you well. I am trying to deploy BertViz in a Streamlit app, which can display a visualization if an HTML/JavaScript string is provided. From the repo code, it seems that the raw HTML/JavaScript was not returned after the visualization was rendered. While I understand this is probably to improve out cleanliness in the Jupyter environment, I hope that `head_view` and `neuron_view` can optionally return the HTML/JavaScript code, so that I can embed the visualization in a web page. An example is available here https://share.streamlit.io/cdpierse/transformers-interpret-streamlit/main/app.py , you can view the source code by clicking the top-right menu. It allows to retrieve the visualization with `_repr_html_()`. 
Thank you!
Charles

"
jessevig/bertviz,"Error while using neuron_view: https://github.com/jessevig/bertviz/issues/88
Description: Hii, The visualisation is fantastic but while using neuron view i am getting error. 
```show(model, 'bert', tokenizer, sentence_b, sentence_a, layer=2, head=0)```

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/tmp/ipykernel_25767/3466440668.py in <module>
      1 sentence_a = ""The cat sat on the mat""
      2 sentence_b = ""The cat lay on the rug""
----> 3 show(model, 'bert', tokenizer, sentence_b, sentence_a, layer=2, head=0)

~/anaconda3/lib/python3.9/site-packages/bertviz/neuron_view.py in show(model, model_type, tokenizer, sentence_a, sentence_b, display_mode, layer, head)
     71     __location__ = os.path.realpath(
     72         os.path.join(os.getcwd(), os.path.dirname(__file__)))
---> 73     attn_data = get_attention(model, model_type, tokenizer, sentence_a, sentence_b, include_queries_and_keys=True)
     74     if model_type == 'gpt2':
     75         bidirectional = False

~/anaconda3/lib/python3.9/site-packages/bertviz/neuron_view.py in get_attention(model, model_type, tokenizer, sentence_a, sentence_b, include_queries_and_keys)
    174     for layer, attn_data in enumerate(attn_data_list):
    175         # Process attention
--> 176         attn = attn_data['attn'][0]  # assume batch_size=1; shape = [num_heads, source_seq_len, target_seq_len]
    177         attn_dict['all'].append(attn.tolist())
    178         if is_sentence_pair:

TypeError: new(): invalid data type 'str'
```
I have trained a mbert model(bert-base-uncased). Here its how I am loading the model
```
from transformers import AutoTokenizer, AutoModel, utils,BertModel,BertTokenizer
model = AutoModel.from_pretrained(model_name, output_attentions=True,from_tf=True)  # Configure model to return attention values
tokenizer = AutoTokenizer.from_pretrained(model_name)
```
All other views are working fantastic in my case. I have noticed from colab notebook that you are importing models from lib provided by your repo. Should I train my model too like that if you have modified it somewhere?

"
jessevig/bertviz,"Unserstanding BART notebook: https://github.com/jessevig/bertviz/issues/84
Description: Hey

I am trying to use Bertviz to debug my BART model (specifically your notebook https://github.com/jessevig/bertviz/blob/master/notebooks/head_view_bart.ipynb ). I wonder if you have any advice on how to visualize the model's focus. Specifically, I am interested how the produced output words are impacted by specific input words.

Cross attention seems to mostly focus on \<s\> and '.' so I am not sure if I understand things correctly.
![Screen Shot 2021-11-16 at 08 43 58](https://user-images.githubusercontent.com/8597349/141996390-a3f6d892-cc09-4362-9f04-86df29979fde.png)




Thank you
Eugene
"
holoviz/panel,"glaciers.ipynb: Failed to load 'https://panel.pyviz.org/_static/logo_stacked.png': https://github.com/holoviz/panel/issues/4656
Description: I'm testing out Panelite built locally from the `main` branch notebooks and from Panel 0.14.4 js.

When running the notebook `gallery/featured/glaciers.ipynb` I get

```bash
JsException: NetworkError: Failed to execute 'send' on 'XMLHttpRequest': Failed to load 'https://panel.pyviz.org/_static/logo_stacked.png'.
```

![image](https://user-images.githubusercontent.com/42288570/232322423-424535a8-6afd-4feb-b6bd-4f1166969252.png)

If I change `https://panel.pyviz.org/_static/logo_stacked.png` to `https://raw.githubusercontent.com/OGGM/oggm/master/docs/_static/logos/oggm_s_alpha.png` there is no issue.

"
holoviz/panel,"Feature/automated panelite tests: https://github.com/holoviz/panel/pull/4655
Description: Going through the notebooks of Panelite to identify which notebooks which to `piplite.install` what. And which notebooks cannot run without errors has been a pain.

I would like to automate the process of testing the Panelite notebooks for errors. This is an attempt.

- [x] Open Panelite (either online or locally)
- [x] Navigate to notebook
- [x] Run all cells
- [x] Determine if an exception is raised
- [x ] Retry up to 3 times if ""Run All"" never finishes. But fail if an exception is raised.
- [x] Minimize flakyness of tests
- [ ] Maximize speed of tests
- [ ] Enable to test new build of Panelite (how do I build panelite in Panel?)
- [x] Functionality to loop through list of relevant notebooks.
- [ ] Make sure 100% of tests pass

Works like below (2x speed)

https://user-images.githubusercontent.com/42288570/232246575-11ee3423-804a-48b0-9833-34c6c2a9b891.mp4
"
poloclub/timbertrek,"saving Rashomon set as JSON?: https://github.com/poloclub/timbertrek/issues/2
Description: Thank you so much for the awesome tool!

How exactly do I generate the JSON file? I don't think the [example notebook](https://github.com/ubc-systopia/treeFarms/blob/main/treefarms/tutorial.ipynb) covers this.

Thanks!

"
poloclub/timbertrek,"Link to example notebook for ""generate yout own JSON"" is broken: https://github.com/poloclub/timbertrek/issues/1
Description: Source URL: https://github.com/poloclub/timbertrek/blob/master/README.md 

> You can use the web demo to explore your own Rashomon Sets! You just need to choose the my own set tab below the tool and upload a JSON file containing all decision paths in your Rashomon Set.

> Check out this [example notebook](https://poloclub.github.io/timbertrek/notebook/retro/notebooks/?path=campas.ipynb) to see how to generate this JSON file. 

This link is broken. I liked the paper, and wanted to see if I could use my own data, but it is unclear how to do this. On the timbertek github.io page, in the widget that lets you pick your own data also, the link to how to do this is broken. it redirects to the readme and the link in the readme is broken.

Thank you
"
QuSTaR/kaleidoscope,"""Segmentation fault (core dumped)"" when running  bloch sphere sample code in a Qiskit 0.30 env: https://github.com/QuSTaR/kaleidoscope/issues/63
Description: Hi,

I have a Qiskit 0.30 env and want to utilize kaleidoscope to get a good presentation. 
the installation of kaleidoscope is successful though it downgrades the numpy library from 1.21.2 to 1.20.3.
But when i attempting to run the sample test code, i immediately get the ""core dumped"" error, see below.

What could be wrong with this setup?

## Below are some of the package version 

(qiskit0.30) [root@rhel84 tutorials]# 
(qiskit0.30) [root@rhel84 tutorials]# conda list | grep qiskit
# packages in environment at /opt/anaconda/envs/qiskit0.30:
qiskit                    0.30.0                   pypi_0    pypi
qiskit-aer                0.9.0                    pypi_0    pypi
qiskit-aqua               0.9.5                    pypi_0    pypi
qiskit-experiments        0.1.0                    pypi_0    pypi
qiskit-finance            0.2.1                    pypi_0    pypi
qiskit-ibmq-provider      0.16.0                   pypi_0    pypi
qiskit-ignis              0.6.0                    pypi_0    pypi
qiskit-machine-learning   0.2.1                    pypi_0    pypi
qiskit-nature             0.2.1                    pypi_0    pypi
qiskit-optimization       0.2.3                    pypi_0    pypi
qiskit-terra              0.18.2                   pypi_0    pypi
qiskit-textbook           0.1.0                    pypi_0    pypi
(qiskit0.30) [root@rhel84 tutorials]# 

(qiskit0.30) [root@rhel84 ~]# conda list | grep plotly
plotly                    5.3.1                    pypi_0    pypi
(qiskit0.30) [root@rhel84 ~]# conda list | grep kaleidoscope
kaleidoscope              0.0.13                   pypi_0    pypi
(qiskit0.30) [root@rhel84 ~]# conda list | grep dash
dash                      2.0.0                    pypi_0    pypi
dash-core-components      2.0.0                    pypi_0    pypi
dash-html-components      2.0.0                    pypi_0    pypi
dash-table                5.0.0                    pypi_0    pypi
(qiskit0.30) [root@rhel84 ~]# 
(qiskit0.30) [root@rhel84 ~]# conda list | grep notebook
notebook                  6.4.4                    pypi_0    pypi
(qiskit0.30) [root@rhel84 ~]# conda list | grep ipywidgets
ipywidgets                7.6.5                    pypi_0    pypi
(qiskit0.30) [root@rhel84 ~]# 


(qiskit0.30) [root@rhel84 tutorials]# python
Python 3.8.11 (default, Aug  6 2021, 10:33:43) 
[GCC 7.5.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> 
>>> import numpy as np
>>> from qiskit import QuantumCircuit
>>> from qiskit.quantum_info import DensityMatrix
>>> import kaleidoscope.qiskit
Segmentation fault (core dumped)
(qiskit0.30) [root@rhel84 tutorials]# 

##  Try again with fault handler

(qiskit0.30) [root@rhel84 tutorials]# python3 -q -X faulthandler
>>> import numpy as np
>>> from qiskit import QuantumCircuit
>>> from qiskit.quantum_info import DensityMatrix
>>> import kaleidoscope.qiskit
Fatal Python error: Segmentation fault

Current thread 0x000003ff9fbf8240 (most recent call first):
  File ""/opt/anaconda/envs/qiskit0.30/lib/python3.8/ctypes/__init__.py"", line 373 in __init__
  File ""/opt/anaconda/envs/qiskit0.30/lib/python3.8/site-packages/llvmlite/binding/ffi.py"", line 185 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 843 in exec_module
  File ""<frozen importlib._bootstrap>"", line 671 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1042 in _handle_fromlist
  File ""/opt/anaconda/envs/qiskit0.30/lib/python3.8/site-packages/llvmlite/binding/dylib.py"", line 3 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 843 in exec_module
  File ""<frozen importlib._bootstrap>"", line 671 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""/opt/anaconda/envs/qiskit0.30/lib/python3.8/site-packages/llvmlite/binding/__init__.py"", line 4 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 843 in exec_module
  File ""<frozen importlib._bootstrap>"", line 671 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""/opt/anaconda/envs/qiskit0.30/lib/python3.8/site-packages/numba/core/config.py"", line 16 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 843 in exec_module
  File ""<frozen importlib._bootstrap>"", line 671 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1042 in _handle_fromlist
  File ""/opt/anaconda/envs/qiskit0.30/lib/python3.8/site-packages/numba/__init__.py"", line 19 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 843 in exec_module
  File ""<frozen importlib._bootstrap>"", line 671 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""/opt/anaconda/envs/qiskit0.30/lib/python3.8/site-packages/kaleidoscope/interactive/bloch/utils.py"", line 19 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 843 in exec_module
  File ""<frozen importlib._bootstrap>"", line 671 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""/opt/anaconda/envs/qiskit0.30/lib/python3.8/site-packages/kaleidoscope/interactive/bloch/bloch2d.py"", line 26 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 843 in exec_module
  File ""<frozen importlib._bootstrap>"", line 671 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""/opt/anaconda/envs/qiskit0.30/lib/python3.8/site-packages/kaleidoscope/interactive/bloch/__init__.py"", line 15 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 843 in exec_module
  File ""<frozen importlib._bootstrap>"", line 671 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 961 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""/opt/anaconda/envs/qiskit0.30/lib/python3.8/site-packages/kaleidoscope/interactive/__init__.py"", line 51 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 843 in exec_module
  File ""<frozen importlib._bootstrap>"", line 671 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""/opt/anaconda/envs/qiskit0.30/lib/python3.8/site-packages/kaleidoscope/__init__.py"", line 26 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 843 in exec_module
  File ""<frozen importlib._bootstrap>"", line 671 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 975 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 961 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 991 in _find_and_load
  File ""<stdin>"", line 1 in <module>
Segmentation fault (core dumped)
(qiskit0.30) [root@rhel84 tutorials]# 



"
zakandrewking/escher,"Using Escher in Google Colab: https://github.com/zakandrewking/escher/issues/366
Description: Hello, I was trying to display the metabolic map in Google Colab using Builder widget, but it was not successful.

Is there any way to solve this problem?
"
MaartenGr/BERTopic,"Custom embedding: https://github.com/MaartenGr/BERTopic/issues/1188
Description: Hi,

I currently have a pretrained encoder and decoder for text summarization (BART-based). And I want to use topic model as part of the loss function. I was wondering if the topic model supports custom embedding that I trained on my own? I saw the tutorial on Google colab uses TransformerDocumentEmbeddings from flair but not sure about using my own pre-trained model.

Thanks in advance!

"
MaartenGr/BERTopic,"Runtime crashes when increasing min_cluster_size : https://github.com/MaartenGr/BERTopic/issues/1180
Description: Hello,

I am working with a very large corpus of around 3M documents. Thus, I wanted to increase the min_cluster_size in HDBSCAN to 500 to decrease the number of topics. Moreover, small topics with only a few documents have no value in my research (I am looking for trending Twitter topics), they only matter if there are about 500 documents related to it.

However, when I ran the .fit_transform on these documents after setting the min_cluster_size of the HDBSCAN algorithm to 500, my runtime crashed in Google Colab. I have no clue what happened because after the runtime crashes I cannot see what happened. It crashes instantly so I cannot see what happens to the resources.

I know that it has to be because the resources, either the GPU RAM or the System RAM, are fully used. However, I do not have enough knowledge to know why that happens when increasing min_cluster_size.

Next, I tried decreasing it from 500 to 100 and got the same error. When setting it to 50, it did work. As parameters for the algorithms, I used all the defaults for UMAP and HDBSCAN (except the min_cluster_size).

The issue is that most of the topics have under 500 documents. Out of 4553, 4124 have less than 500 docs, so only 429 topics are ""useful"". This is a good thing because I don't want 4553 topics (that's too much), but I don't know what to do next. Is it a good idea to use .reduce_topics and set it to around 429, or will this not solve the problem? I could also only use the 429 ""big"" topics and discard the rest but then I lose quite some information I think.

One last question: out of the 3014471 tweets, 1677106 were assigned topic -1. I know HDBSCAN takes noise into account, but this looks like a lot of noise. Tweets are short so maybe the problem lies there, but is there a way to reduce the noise without having no noise at all? I know I could use other algorithms like k-means, but I think the incorporation of noise is quite useful in my analysis, so I was wondering whether there was a ""middle way"".

Thank you in advance!!

"
MaartenGr/BERTopic,"fix hierarchy viz and handle any form of distance matrix: https://github.com/MaartenGr/BERTopic/pull/1173
Description: This fixes #923, fixes #1063 , and fixes #1021 which are just duplicates of the first one. 

I added a utility function to make sure the distance matrix computed by a given distance_function is a condensed 1-D array. If not, the function convert it and return the result. It can be used directly as I did in [`_bertopic.pyL877`](https://github.com/elashrry/BERTopic/blob/fix-hierarchy-viz/bertopic/_bertopic.py#L877):
```
X = validate_distance_matrix(X, embeddings.shape[0])
```


or to wrap around a function as I did in [`_hierarchy.py#L132-L133`](https://github.com/elashrry/BERTopic/blob/fix-hierarchy-viz/bertopic/plotting/_hierarchy.py#L132-L133):

```
distance_function_viz = lambda x: validate_distance_matrix(
        distance_function(x), embeddings.shape[0])
```

I also added this information in the docstring to let the user know what is possible. I didn't find things to changes in the tutorials. 

I added a testing notebook to test the code provided in the mentioned issues. (to be deleted after reviewing the PR)

Let me know if you have any comments! 


"
eli5-org/eli5,"Bug: AttributeError: 'MaskingTextSamplers' object has no attribute 'sampler_params' when using TextExplainer with MaskingTextSamplers: https://github.com/eli5-org/eli5/issues/27
Description: When using the default values for TextExplainer, it fails reutrning:

```---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
File ~\.conda\envs\eli5-pip\lib\site-packages\IPython\core\formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude)
    970     method = get_real_method(obj, self.print_method)
    972     if method is not None:
--> 973         return method(include=include, exclude=exclude)
    974     return None
    975 else:

File ~\.conda\envs\eli5-pip\lib\site-packages\sklearn\base.py:629, in BaseEstimator._repr_mimebundle_(self, **kwargs)
    627 def _repr_mimebundle_(self, **kwargs):
    628     """"""Mime bundle used by jupyter kernels to display estimator""""""
--> 629     output = {""text/plain"": repr(self)}
    630     if get_config()[""display""] == ""diagram"":
    631         output[""text/html""] = estimator_html_repr(self)

File ~\.conda\envs\eli5-pip\lib\site-packages\sklearn\base.py:279, in BaseEstimator.__repr__(self, N_CHAR_MAX)
    271 # use ellipsis for sequences with a lot of elements
    272 pp = _EstimatorPrettyPrinter(
    273     compact=True,
    274     indent=1,
    275     indent_at_name=True,
    276     n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW,
    277 )
--> 279 repr_ = pp.pformat(self)
    281 # Use bruteforce ellipsis when there are a lot of non-blank characters
    282 n_nonblank = len("""".join(repr_.split()))

File ~\.conda\envs\eli5-pip\lib\pprint.py:157, in PrettyPrinter.pformat(self, object)
    155 def pformat(self, object):
    156     sio = _StringIO()
--> 157     self._format(object, sio, 0, 0, {}, 0)
    158     return sio.getvalue()

File ~\.conda\envs\eli5-pip\lib\pprint.py:174, in PrettyPrinter._format(self, object, stream, indent, allowance, context, level)
    172     self._readable = False
    173     return
--> 174 rep = self._repr(object, context, level)
    175 max_width = self._width - indent - allowance
    176 if len(rep) > max_width:

File ~\.conda\envs\eli5-pip\lib\pprint.py:454, in PrettyPrinter._repr(self, object, context, level)
    453 def _repr(self, object, context, level):
--> 454     repr, readable, recursive = self.format(object, context.copy(),
    455                                             self._depth, level)
    456     if not readable:
    457         self._readable = False

File ~\.conda\envs\eli5-pip\lib\site-packages\sklearn\utils\_pprint.py:189, in _EstimatorPrettyPrinter.format(self, object, context, maxlevels, level)
    188 def format(self, object, context, maxlevels, level):
--> 189     return _safe_repr(
    190         object, context, maxlevels, level, changed_only=self._changed_only
    191     )

File ~\.conda\envs\eli5-pip\lib\site-packages\sklearn\utils\_pprint.py:452, in _safe_repr(object, context, maxlevels, level, changed_only)
    448 for k, v in items:
    449     krepr, kreadable, krecur = saferepr(
    450         k, context, maxlevels, level, changed_only=changed_only
    451     )
--> 452     vrepr, vreadable, vrecur = saferepr(
    453         v, context, maxlevels, level, changed_only=changed_only
    454     )
    455     append(""%s=%s"" % (krepr.strip(""'""), vrepr))
    456     readable = readable and kreadable and vreadable

File ~\.conda\envs\eli5-pip\lib\site-packages\sklearn\utils\_pprint.py:440, in _safe_repr(object, context, maxlevels, level, changed_only)
    438 recursive = False
    439 if changed_only:
--> 440     params = _changed_params(object)
    441 else:
    442     params = object.get_params(deep=False)

File ~\.conda\envs\eli5-pip\lib\site-packages\sklearn\utils\_pprint.py:93, in _changed_params(estimator)
     89 def _changed_params(estimator):
     90     """"""Return dict (param_name: value) of parameters that were given to
     91     estimator with non-default values.""""""
---> 93     params = estimator.get_params(deep=False)
     94     init_func = getattr(estimator.__init__, ""deprecated_original"", estimator.__init__)
     95     init_params = inspect.signature(init_func).parameters

File ~\.conda\envs\eli5-pip\lib\site-packages\sklearn\base.py:211, in BaseEstimator.get_params(self, deep)
    209 out = dict()
    210 for key in self._get_param_names():
--> 211     value = getattr(self, key)
    212     if deep and hasattr(value, ""get_params""):
    213         deep_items = value.get_params().items()

AttributeError: 'MaskingTextSamplers' object has no attribute 'sampler_params'
```

Or alternative non jupyter run:
```
C:\Users\admin\.conda\envs\eli5-pip\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.
  warnings.warn(
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\admin\.conda\envs\eli5-pip\lib\site-packages\sklearn\base.py"", line 279, in __repr__
    repr_ = pp.pformat(self)
  File ""C:\Users\admin\.conda\envs\eli5-pip\lib\pprint.py"", line 157, in pformat
    self._format(object, sio, 0, 0, {}, 0)
  File ""C:\Users\admin\.conda\envs\eli5-pip\lib\pprint.py"", line 174, in _format
    rep = self._repr(object, context, level)
  File ""C:\Users\admin\.conda\envs\eli5-pip\lib\pprint.py"", line 454, in _repr
    repr, readable, recursive = self.format(object, context.copy(),
  File ""C:\Users\admin\.conda\envs\eli5-pip\lib\site-packages\sklearn\utils\_pprint.py"", line 189, in format
    return _safe_repr(
  File ""C:\Users\admin\.conda\envs\eli5-pip\lib\site-packages\sklearn\utils\_pprint.py"", line 452, in _safe_repr
    vrepr, vreadable, vrecur = saferepr(
  File ""C:\Users\admin\.conda\envs\eli5-pip\lib\site-packages\sklearn\utils\_pprint.py"", line 440, in _safe_repr
    params = _changed_params(object)
  File ""C:\Users\admin\.conda\envs\eli5-pip\lib\site-packages\sklearn\utils\_pprint.py"", line 93, in _changed_params
    params = estimator.get_params(deep=False)
  File ""C:\Users\admin\.conda\envs\eli5-pip\lib\site-packages\sklearn\base.py"", line 211, in get_params
    value = getattr(self, key)
AttributeError: 'MaskingTextSamplers' object has no attribute 'sampler_params'
```

This seems to be more of an issue relating to sklearn rather than to eli5 itself. However, an easy fix would be to include the sampler_params as an attribute in the MaskingTextSamplers class. 

Do let me know if there is something that I have done wrong here. 

"
eli5-org/eli5,"Permutation importance show_weights unreadable in Jupyter dark: https://github.com/eli5-org/eli5/issues/13
Description: When the Jupyter theme is set to dark, it is impossible to see the the results from show_weights. Can we have the ability to set a dark or light switch so this can be useable no matter what theme someone is using?

I am using eli5 0.11.0

<img width=""293"" alt=""Screen Shot 2021-04-25 at 1 52 55 PM"" src=""https://user-images.githubusercontent.com/7563201/115983778-8c8f1c80-a5cd-11eb-84b7-4244799d418c.png"">

"
graphistry/pygraphistry,"[BUG] register() fails when both username & api provided: https://github.com/graphistry/pygraphistry/issues/453
Description: **Describe the bug**
When providing an api key, if a username is also provided, an unexpected error appears about not having set a password

Provided a confusing onboarding for a google auth user

**To Reproduce**

```python
graphistry.register(api=3, username=...., personal_key_id='pkey_id', personal_key_secret='pkey_secret')
```

**Expected behavior**
Should work

**Actual behavior**

Error about missing password


**PyGraphistry API client environment**

Jupyter, latest: 0.28.7



"
graphistry/pygraphistry,"[BUG] render=False does not propagate url_params: https://github.com/graphistry/pygraphistry/issues/448
Description: **Describe the bug**
Render equals false URL_params values do not seem to get set such as play=0 . This prevents core workflows like specifying a custom layout for embedded web apps.

**To Reproduce**
See https://graphistry-community.slack.com/archives/C014ESCDDU0/p1677651935706109?thread_ts=1677651935.706109&cid=C014ESCDDU0

**Expected behavior**
What should have happened

**Actual behavior**
What did happen

**Screenshots**
If applicable, any screenshots to help explain the issue

**Browser environment (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Graphistry GPU server environment**
 - Where run [e.g., Hub, AWS, on-prem]
 - If self-hosting, Graphistry Version [e.g. 0.14.0, see bottom of a viz or login dashboard]
- If self-hosting, any OS/GPU/driver versions

**PyGraphistry API client environment**
 - Where run [e.g., Graphistry 2.35.9 Jupyter]
 - Version [e.g. 0.14.0, print via `graphistry.__version__`]
 - Python Version [e.g. Python 3.7.7] 

**Additional context**
Add any other context about the problem here.

"
movingpandas/movingpandas,"date/time consistency and time zone handling: https://github.com/movingpandas/movingpandas/issues/303
Description: If you have a question, rather than a bug report, please post in the [discussions](https://github.com/anitagraser/movingpandas/discussions).
Otherwise, please check: 

- [x] I have checked that this issue has not already been reported.
- [x] I have confirmed this bug exists on the latest version of MovingPandas.
- [x] (optional) I have confirmed this bug exists on the main branch of MovingPandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example
[Binder link to example](https://mybinder.org/v2/gh/bamacgabhann/PublicTransportTracking/mpd_stop_points?labpath=ptt.ipynb)

#### Problem description

TrajectoryStopDetector is not working for me after updating to 0.15. The problem appears to be with pandas, not mpd, but a change to the mpd code would solve the problem. I've tried this on my Windows machine, WSL Ubuntu on the same machine, and in Binder, so I don't think it's just me, but I can't say that with 100% certainty.

The issue is in the get_stop_points() method of the TrajectoryStopDetector, specifically in converting the timedelta object to total seconds. 
```python
        if len(stops) > 0:
            stop_pts[""duration_s""] = (
                stop_pts[""end_time""] - stop_pts[""start_time""]
            ).dt.total_seconds()
```
This was working with 0.11, but running with 0.15 now produces:

```python
detector = mpd.TrajectoryStopDetector(track)  # track is a mpd.Trajectory - see the Binder link for working example
stationary_points = detector.get_stop_points(min_duration=timedelta(seconds=15), max_diameter=30)

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[4], line 2
      1 detector = mpd.TrajectoryStopDetector(track)
----> 2 stationary_points = detector.get_stop_points(min_duration=timedelta(seconds=15), max_diameter=30)

File /srv/conda/envs/notebook/lib/python3.9/site-packages/movingpandas/trajectory_stop_detector.py:182, in TrajectoryStopDetector.get_stop_points(self, max_diameter, min_duration)
    179     stop_pts.at[stop.id, ""traj_id""] = stop.parent.id
    181 if len(stops) > 0:
--> 182     stop_pts[""duration_s""] = (
    183         stop_pts[""end_time""] - stop_pts[""start_time""]
    184     ).dt.total_seconds()
    185     stop_pts[""traj_id""] = stop_pts[""traj_id""].astype(type(stop.parent.id))
    187 return stop_pts

File /srv/conda/envs/notebook/lib/python3.9/site-packages/pandas/core/generic.py:5989, in NDFrame.__getattr__(self, name)
   5982 if (
   5983     name not in self._internal_names_set
   5984     and name not in self._metadata
   5985     and name not in self._accessors
   5986     and self._info_axis._can_hold_identifiers_and_holds_name(name)
   5987 ):
   5988     return self[name]
-> 5989 return object.__getattribute__(self, name)

File /srv/conda/envs/notebook/lib/python3.9/site-packages/pandas/core/accessor.py:224, in CachedAccessor.__get__(self, obj, cls)
    221 if obj is None:
    222     # we're accessing the attribute of the class, i.e., Dataset.geo
    223     return self._accessor
--> 224 accessor_obj = self._accessor(obj)
    225 # Replace the property with the accessor object. Inspired by:
    226 # https://www.pydanny.com/cached-property.html
    227 # We need to use object.__setattr__ because we overwrite __setattr__ on
    228 # NDFrame
    229 object.__setattr__(obj, self._name, accessor_obj)

File /srv/conda/envs/notebook/lib/python3.9/site-packages/pandas/core/indexes/accessors.py:580, in CombinedDatetimelikeProperties.__new__(cls, data)
    577 elif is_period_dtype(data.dtype):
    578     return PeriodProperties(data, orig)
--> 580 raise AttributeError(""Can only use .dt accessor with datetimelike values"")

AttributeError: Can only use .dt accessor with datetimelike values
```
The problem [does appear](https://github.com/pandas-dev/pandas/issues/46582) to be with pandas, specifically that the .dt accessor is not recognising timedelta (and other similar) objects as datetime-like values.

However, upstream issue or not, it still isn't working for me - and if it's not just me, it might not be ideal to leave this broken until pandas fixes .dt handling of timedeltas, especially since they're concentrating on the pyarrow backend right now (which doesn't handle timedeltas either: https://github.com/pandas-dev/pandas/issues/52284).

#### Expected Output

df with ""duration_s"" column in seconds

#### Fix

The Binder link above takes you to a branch of a repo I'm working on where I've left the error un-fixed. I have fixed it on main in that repo - a very dirty fix where I threw in a .py with a subclass of TrajectoryStopDetector and a working version of the get_stop_points method.

I will happily submit a pull request, if you like. My fix is to change

```python
        if len(stops) > 0:
            stop_pts[""duration_s""] = (
                stop_pts[""end_time""] - stop_pts[""start_time""]
            ).dt.total_seconds()
```
to
```python
        if len(stops) > 0:
            stop_pts[""duration_s""] = (
                stop_pts[""end_time""] - stop_pts[""start_time""]
            )
            stop_pts[""duration_s""] = stop_pts[""duration_s""].apply(lambda x: x.seconds)
```
[Binder link to ipynb in working repo branch](https://mybinder.org/v2/gh/bamacgabhann/PublicTransportTracking/main?labpath=ptt.ipynb)
[Link to Repo](https://github.com/bamacgabhann/PublicTransportTracking/)

#### Output of ``movingpandas.show_versions()``

<details>

MovingPandas 0.15.rc1

SYSTEM INFO
-----------
python     : 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]
executable : /home/breandan/miniconda3/envs/ptt/bin/python
machine    : Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.35

GEOS, GDAL, PROJ INFO
---------------------
GEOS       : None
GEOS lib   : None
GDAL       : 3.6.3
GDAL data dir: /home/breandan/miniconda3/envs/ptt/share/gdal
PROJ       : 9.1.1
PROJ data dir: /home/breandan/miniconda3/envs/ptt/share/proj

PYTHON DEPENDENCIES
-------------------
geopandas  : 0.12.2
pandas     : 2.0.0
fiona      : 1.9.2
numpy      : 1.23.5
shapely    : 2.0.1
rtree      : 1.0.1
pyproj     : 3.5.0
matplotlib : 3.7.1
mapclassify: 2.5.0
geopy      : 2.3.0
holoviews  : 1.15.4
hvplot     : 0.8.3
geoviews   : 1.9.6
stonesoup  : 0.1b12

</details>

"
merqurio/neo4jupyter,"physics: https://github.com/merqurio/neo4jupyter/issues/16
Description: Hi

Is there a way to change, adapt the physics, by passing parameters?  This would be great for my use case, but I didn't manage to do it. Somehow the code looks like there are DEFAULT_PHYSICS but changing this constant, had no influence on the output. 

Secondly, I wrote an adapted function to use neo4jupyter with the python graph data science package (gds.run_cypher). If you are interested, I would love to commit the code. 

Thank you!

"
merqurio/neo4jupyter,"Export as an html file: https://github.com/merqurio/neo4jupyter/issues/15
Description: Is there any way to export the result on jupyter notebook as html？I need it badly and I would really appreciate it if this can be solved

"
merqurio/neo4jupyter,"TypeError: 'LabelSetView' object is not callable: https://github.com/merqurio/neo4jupyter/issues/13
Description: I thought this was solved, but it does not actually seem to be. 
Is there a chance that a newer version of py2neo has a breaking change?

It seems to have something to do with the labels attribute. In fact... the query in neo4jupyter.py does not seem to be actually returning any objects (not sure the proper terminology here):

![image](https://user-images.githubusercontent.com/10867313/78417177-7a1dfb80-75e4-11ea-8d1a-7ded34af198f.png)



"
merqurio/neo4jupyter,"TypeError: 'LabelSetView' object is not callable: https://github.com/merqurio/neo4jupyter/issues/12
Description: This error comes up when I try to execute neo4jupyter.draw 
(from the demo code). 

"
merqurio/neo4jupyter,"Jupyter Lab: neo4jupyter.init_notebook_mode() -> Javascript Error: require is not defined: https://github.com/merqurio/neo4jupyter/issues/7
Description: neo4jupyter (v 0.1.2) doesn't seem to work in Jupyter Lab:

import neo4jupyter
neo4jupyter.init_notebook_mode()

Javascript Error: require is not defined




"
merqurio/neo4jupyter,"Update neo4jupyter.py and remove properties attribute for node: https://github.com/merqurio/neo4jupyter/pull/6
Description: py2neo.data.Node does not have the properties attribute in Neo4j 3.3.5 anymore. Also node.labels is not callable anymore.

"
merqurio/neo4jupyter,"added draw_subgraph for displaying subgraphs : https://github.com/merqurio/neo4jupyter/pull/4
Description: Here are the changes I suggested to enable subgraph presentation

`
from py2neo import Graph, Node, Relationship, NodeSelector, Subgraph, remote
import neo4jupyter
neo4jupyter.init_notebook_mode()

class Knows(Relationship): pass
class Dates(Relationship): pass
class Despises(Relationship): pass

class Person(Node): pass

sam = Node('Person', Name = 'Sam')
bob = Node('Person', Name = 'Bob')
george = Node('Person', Name = 'George')
suzy = Node('Person', Name = 'Suzy')

sg = Knows(bob, sam)
sg |= Knows(sam, suzy) 
sg |= Dates(bob, suzy)
sg |= Despises(george, bob)

neo4jupyter.draw_subgraph(sg, options = {""Person"": ""Name""})
`


"
merqurio/neo4jupyter,"Suggestion for handling subgraphs: https://github.com/merqurio/neo4jupyter/issues/3
Description: I have need of presenting a subgraph (py2neo.Subgraph) and this package is close to what I would like to do.  (thank you for publishing it on github)  Have code to enhance to present subgraph in cases where you want to show what will be merged or want to select a subset of nodes out of a graph for presentation.  Are you interested in taking new pull requests?  (i'm new to that process, but thought I would give it a go if you're interested.)

![screen shot 2018-04-20 at 21 30 39](https://user-images.githubusercontent.com/5431447/39079100-33f23912-44e2-11e8-865f-4a9d2f100761.png)

Example code of proposed function draw_subgraph

```python
from py2neo import Graph, Node, Relationship, NodeSelector, Subgraph, remote
import neo4jupyter
import vis
neo4jupyter.init_notebook_mode()
class Knows(Relationship): pass
class Dates(Relationship): pass
class Despises(Relationship): pass
class Person(Node): pass
sam = Node('Person', Name = 'Sam')
bob = Node('Person', Name = 'Bob')
george = Node('Person', Name = 'George')
suzy = Node('Person', Name = 'Suzy')

sg = Knows(bob, sam)
sg |= Knows(sam, suzy) 
sg |= Dates(bob, suzy)
sg |= Despises(george, bob)
neo4jupyter.draw_subgraph(sg, options = {""Person"": ""Name""})`
```


"
merqurio/neo4jupyter,"Automate the init_notebook_mode: https://github.com/merqurio/neo4jupyter/issues/2
Description: Automatically run on `draw() ` if it's not been previously runned.

"
merqurio/neo4jupyter,"Package setup and scripts: https://github.com/merqurio/neo4jupyter/pull/1
Description: Hi @merqurio !

Recently I installed the package and found some issues to run it in jupyter. I've made some chages in package installation and also changed the script to generate the embedded js/html.

Could please review it and maybe update the package in pypi?
"
pixiedust/pixiedust,"pixie debugger in Jupyter showing weird characters: https://github.com/pixiedust/pixiedust/issues/798
Description: ### Expected behavior

The debugger should display this code properly:
```
%%pixie_debugger

import pandas as pd
import numpy as np

starter = np.zeros( (10,10))
```

### Actual behavior

Instead, it displays weird characters everywhere:
```
[0m[1;32mdef[0m [0mpixie_run[0m[1;33m([0m[1;33m)[0m[1;33m:[0m[1;33m[0m[1;33m[0m[0m
[0m    [1;32mimport[0m [0mpdb[0m[1;33m[0m[1;33m[0m[0m
[0m    [0mpdb[0m[1;33m.[0m[0mset_trace[0m[1;33m([0m[1;33m)[0m[1;33m[0m[1;33m[0m[0m
[0m[1;33m[0m[0m
[1;33m    [1;32mimport[0m [0mpandas[0m [1;32mas[0m [0mpd[0m[1;33m[0m[1;33m[0m[0m
[0m    [1;32mimport[0m [0mnumpy[0m [1;32mas[0m [0mnp[0m[1;33m[0m[1;33m[0m[0m
[0m[1;33m[0m[0m
[0m    [0mstarter[0m [1;33m=[0m [0mnp[0m[1;33m.[0m[0mzeros[0m[1;33m([0m [1;33m([0m[1;36m10[0m[1;33m,[0m[1;36m10[0m[1;33m)[0m[1;33m)[0m[1;33m[0m[1;33m[0m[0m
[0m[1;33m[0m[0m
```

### Steps to reproduce the behavior

Ubuntu 20.04
I just installed pixiedust with `pip install pixiedust`, and I got version `1.1.19`.
After `import pixidust`, it tells me:
```
Pixiedust database opened successfully

Pixiedust version 1.1.19
```

Note: Issue has already been reported here by another user: https://stackoverflow.com/questions/63960340/pixie-debugger-in-jupyter-showing-weird-characters


"
pixiedust/pixiedust,"ESC sequences not rendering properly in debugger: https://github.com/pixiedust/pixiedust/issues/793
Description: I'm not sure if pixiedust debugger is still under active development?
Anyway, I have pip-installed it under termux running jupyter notebooks on Android. 

### Expected behavior
The expected behavior would be to see code lines in the debugger, following a cell, decorated with %%pixie_debugger magic

### Actual behavior
The actual behavior is that the debugger's display is full of what I guess are ESC sequences(?) Likely these are not rendered properly(?) See attached jpg.

### Steps to reproduce the behavior
Just invoke the debugger on any cell of the notebook

###
![srcsht](https://user-images.githubusercontent.com/2843926/95899504-f57d1880-0d90-11eb-8eb1-f1473deead63.jpg)


"
pixiedust/pixiedust,"SSL: CERTIFICATE_VERIFY_FAILED: https://github.com/pixiedust/pixiedust/issues/789
Description: Hi, 

`import pixiedust` gives me this warning

```
Unable to check latest version <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]
```

Thanks

ps: on 

* mac OS 10.15.5
* `notebook 6.0.3`

"
pixiedust/pixiedust,"AttributionError in Jupyter Notebook on Windows: https://github.com/pixiedust/pixiedust/issues/788
Description: ### Expected behavior
I am experimenting with pixiedust 1.1.8 in Jupyter Notebook on Windows 10, which I suppose should run fine.
### Actual behavior
However, I instead get the following:

`\nAttributeError: module 'time' has no attribute 'clock'\n`

Could someone advise on what could be the issue, please? 
### Here is my code
`import pandas as pd`
`import pixiedust`

`df = {""age"":[50,12,34,53,22,44], ""height"":[4.5,5.0,6.0,5.5,4.7,5.9], ""name"":['peter','mike','john','james','frank','paul']}`

`dataset = pd.DataFrame(df)`

`display(dataset)`

"
pixiedust/pixiedust,"I can not stop the debugger, it runs infinite time: https://github.com/pixiedust/pixiedust/issues/786
Description: Date: Jan 23, 2020  
pixiedust version: 1.1.18    
platform: jupyter notebook (on Google Chrome on macOS Catalina)  

**problem**
In jupyter notebook, I can use the pixiedust debugger with cell magic `%%pixie_debugger`  and it works.
But after done debugging, I can not stop the debugger.

```
Kernel > Interrupt # does not work

I need to restart the kernel or stop the whole jupyter notebook to stop one cell.

```


**Question** 
1. How to stop the debugging session in a cell without restarting the whole notebook?
2.  The problem is for jupyter notebook ( I have not tested in IBM Watson Cloud notebook).
       Is the debugging functionality only available for IBM cloud, and does not work in free version in jupyter notebook?


"
pixiedust/pixiedust,"Unable to View Spark Job Monitor in Watson Studio Jupyter Notebooks: https://github.com/pixiedust/pixiedust/issues/780
Description: As a `<user type>`, I want to `<task>` so that `<goal>` (make this the title)

### Expected behavior
It should show the spark job monitor

### Actual behavior
it shows a error : Spark Job Progress Monitoring cannot be started on DSX

### Steps to reproduce the behavior


"
pixiedust/pixiedust,"got Exception in thread ""main"" java.io.IOException: No FileSystem for scheme: C: https://github.com/pixiedust/pixiedust/issues/779
Description: As a USER, I want to user pixiedust in jupyter of windows 10 so that I got the “exception in thread ""main"" java.io.IOException: No FileSystem for scheme: C”。

### Expected behavior
jupyter kernel pythonwithpixiedustspark24 should be able handle path: C:/spark-2.4.4-bin-hadoop2.7/python/pyspark/shell.py

### Actual behavior
Exception in thread ""main"" java.io.IOException: No FileSystem for scheme: C
        at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)
                                .......
[IPKernelApp] WARNING | Unknown error in handling PYTHONSTARTUP file C:/spark-2.4.4-bin-hadoop2.7/python/pyspark/shell.py:

### Steps to reproduce the behavior
1. start from command line: jupyter lab
2. create a new ipynb page, input following:
**from pyspark import SparkContext
sc = SparkContext(""local"", ""First App"")**
3. run them. You may see the exception printed in the console of jupyter 
4.  _switch to original kernel ""Python3"", these two lines can be run without error_.

### environment
Windows 10 (only one hard disk C:\)
Anaconda python (3.7.3)
spark 2.44 with prebuilt hadoop 2.7 (winutil.exe included)
scala 2.11.8

### tried but not working:
set windows enviornment variable: 
**SPARK_HOME=C:/spark-2.4.4-bin-hadoop2.7**
change line 512 of createKernel.py as: 
**""PYTHONSTARTUP"": ""file:///{0}/python/pyspark/shell.py"".format(self.spark_home)**,


"
pixiedust/pixiedust,"As a user, I want to set a breakpoint so that jupyter breaks in another python file: https://github.com/pixiedust/pixiedust/issues/774
Description: I can't  set break point in a file which the function is not shown in the jupyter note book. How can I do it?

"
pixiedust/pixiedust,"Chart does not work in local jupyter notebook (only table works): https://github.com/pixiedust/pixiedust/issues/773
Description: I was trying to use this moduel `pixiedust` in jupyter-notebook in my computer.

I was having problem visualization the data.

When I tried to use seaborn, it was not working good, I reinstalled seaborn, now its running good.

I am closing the issue.


"
pycaret/pycaret,"[BUG]: IDE Displaying problem: https://github.com/pycaret/pycaret/issues/3466
Description: ### pycaret version checks

- [X] I have checked that this issue has not already been reported [here](https://github.com/pycaret/pycaret/issues).

- [X] I have confirmed this bug exists on the [latest version](https://github.com/pycaret/pycaret/releases) of pycaret.

- [X] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https://github.com/pycaret/pycaret.git@master).


### Issue Description

I am currently running Pycaret in VSCode and PyCharm with all Jupyter extensions installed. However, I am facing an issue where the markdown table is not displaying properly. Instead of showing the table, it only displays the object name.

I have tried to configure the jupyter kernel to show the markdown table properly, but I still couldn't solve the issue.


### Reproducible Example

```python
from pycaret.datasets import get_data
juice = get_data('juice')
from pycaret.classification import *
exp_name = setup(data = juice,  target = 'Purchase')
```


### Expected Behavior

![Screenshot from 2023-04-08 01-53-03](https://user-images.githubusercontent.com/85236337/230689661-05c52a76-3e8f-4d6e-8b6a-86b1f403418a.png)


### Actual Results

```python-traceback
Empty
```


### Installed Versions

<details>
'3.0.0.rc9'
I try also with
'3.0.0'
</details>


"
pycaret/pycaret,"Transformed data bigger than the original data.: https://github.com/pycaret/pycaret/issues/3456
Description: ### pycaret version checks

- [X] I have checked that this issue has not already been reported [here](https://github.com/pycaret/pycaret/issues).

- [X] I have confirmed this bug exists on the [latest version](https://github.com/pycaret/pycaret/releases) of pycaret.

- [X] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https://github.com/pycaret/pycaret.git@master).


### Issue Description

Running setup function on google colaboratory with parameter preprocess set on False, I obtain a Transformed data shape bigger than original data shape in the number of rows.

I wold like to know why and how can I just use my validation set

### Reproducible Example

```python
!pip install --upgrade pycaret
!pip install shap

import pandas as pd
import plotly.express as px
from pycaret.classification import *
from sklearn.model_selection import train_test_split
import shap 

X_train = pd.read_csv('X_train.csv')
X_val = pd.read_csv('X_val.csv')

y_train = X_train['target']
y_val = X_val['target']

numeric_columns = X_train.select_dtypes(include=['float', 'int']).columns.tolist()
numeric_columns = [column for column in numeric_columns if column != 'target']

clf = setup(data = X_train, preprocess = False,  target='conspiracy', session_id=123, numeric_features = numeric_columns, use_gpu = True, test_data = X_val)
```


### Expected Behavior

| Description | Value
Session id | 123
Target | target
Target typ | Binary
Original data shape | (10823, 506)
Transformed data shape | (10823, 506)
Transformed train set shape | (10281, 506)
Transformed test set shape | (542, 506)
Numeric features | 505

### Actual Results

```python-traceback
| Description | Value
Session id | 123
Target | target
Target typ | Binary
Original data shape | (10823, 506)
Transformed data shape | (11907, 506)
Transformed train set shape | (10823, 506)
Transformed test set shape | (1084, 506)
Numeric features | 505
```


### Installed Versions

<details>
PyCaret required dependencies:
                 pip: 22.0.4
          setuptools: 67.6.1
             pycaret: 3.0.0
             IPython: 7.34.0
          ipywidgets: 7.7.1
                tqdm: 4.65.0
               numpy: 1.22.4
              pandas: 1.4.4
              jinja2: 3.1.2
               scipy: 1.10.1
              joblib: 1.2.0
             sklearn: 1.2.2
                pyod: 1.0.9
            imblearn: 0.10.1
   category_encoders: 2.6.0
            lightgbm: 3.3.5
               numba: 0.56.4
            requests: 2.27.1
          matplotlib: 3.7.1
          scikitplot: 0.3.7
         yellowbrick: 1.5
              plotly: 5.13.1
             kaleido: 0.2.1
         statsmodels: 0.13.5
              sktime: 0.16.1
               tbats: 1.1.2
            pmdarima: 2.0.3
              psutil: 5.9.4

PyCaret optional dependencies:
                shap: 0.41.0
           interpret: Not installed
                umap: Not installed
    pandas_profiling: 3.2.0
  explainerdashboard: Not installed
             autoviz: Not installed
           fairlearn: Not installed
             xgboost: 1.7.4
            catboost: Not installed
              kmodes: Not installed
             mlxtend: 0.14.0
       statsforecast: Not installed
        tune_sklearn: Not installed
                 ray: Not installed
            hyperopt: 0.2.7
              optuna: Not installed
               skopt: Not installed
              mlflow: Not installed
              gradio: Not installed
             fastapi: Not installed
             uvicorn: Not installed
              m2cgen: Not installed
           evidently: Not installed
               fugue: Not installed
           streamlit: Not installed
             prophet: 1.1.2
</details>

"
visgl/deck.gl,"[Bug] Unexpected label fading while using CollisionFilterExtension: https://github.com/visgl/deck.gl/issues/7824
Description: ### Description

While using `TextLayer` with `CollisionFilterExtension`, when set textLayer property `getTextAnchor` to 'start' or 'end', the label is fading.


https://user-images.githubusercontent.com/926754/231077034-6650ef98-f610-4f3c-8f85-d85f796773f4.mp4



### Flavors

- [X] Script tag
- [ ] React
- [ ] Python/Jupyter notebook
- [ ] MapboxOverlay
- [ ] GoogleMapsOverlay
- [ ] CartoLayer
- [ ] ArcGIS

### Expected Behavior

_No response_

### Steps to Reproduce

The behavior described can be produced on [codepen](https://codepen.io/hellocreep/pen/wvYaRPW)

### Environment

- Framework version: 8.9.7
- Browser: Chrome Version 112.0.5615.49 (Official Build) (x86_64)
- OS: macOS Monterey 12.6.3


### Logs

_No response_

"
visgl/deck.gl,"H3ClusterLayer hex cell colour unaffected by count field value: https://github.com/visgl/deck.gl/issues/7820
Description: ### Description

I've named the field with values `count` and I'm using it to affect the colour of each hexagon.

### Flavors

- [ ] Script tag
- [ ] React
- [X] Python/Jupyter notebook
- [ ] MapboxOverlay
- [ ] GoogleMapsOverlay
- [ ] CartoLayer
- [ ] ArcGIS

### Expected Behavior

So far the whole chart is coming out red instead of a gradient between red and yellow.

### Steps to Reproduce

```bash
$ cat aus_h3.duckgl.json
```

```json
[{""count"": ""12"", ""hexIds"": [""82beeffffffffff""]}, {""count"": ""35"", ""hexIds"": [""82be77fffffffff""]}, {""count"": ""51"", ""hexIds"": [""82b917fffffffff""]}, {""count"": ""32"", ""hexIds"": [""82bf4ffffffffff""]}, {""count"": ""93"", ""hexIds"": [""82be67fffffffff""]}, {""count"": ""51"", ""hexIds"": [""82c997fffffffff""]}, {""count"": ""13"", ""hexIds"": [""82be5ffffffffff""]}, {""count"": ""11"", ""hexIds"": [""82bed7fffffffff""]}, {""count"": ""52"", ""hexIds"": [""82be47fffffffff""]}, {""count"": ""9"", ""hexIds"": [""82c987fffffffff""]}, {""count"": ""13"", ""hexIds"": [""82b9a7fffffffff""]}, {""count"": ""26"", ""hexIds"": [""82a737fffffffff""]}, {""count"": ""38"", ""hexIds"": [""82be8ffffffffff""]}, {""count"": ""3"", ""hexIds"": [""829d77fffffffff""]}, {""count"": ""85"", ""hexIds"": [""82be0ffffffffff""]}, {""count"": ""12"", ""hexIds"": [""82b9b7fffffffff""]}, {""count"": ""23"", ""hexIds"": [""82be6ffffffffff""]}, {""count"": ""2"", ""hexIds"": [""82b84ffffffffff""]}, {""count"": ""6"", ""hexIds"": [""829d4ffffffffff""]}, {""count"": ""6"", ""hexIds"": [""82b85ffffffffff""]}, {""count"": ""7"", ""hexIds"": [""82bec7fffffffff""]}, {""count"": ""32"", ""hexIds"": [""82be57fffffffff""]}, {""count"": ""2"", ""hexIds"": [""82a7affffffffff""]}, {""count"": ""30"", ""hexIds"": [""82a727fffffffff""]}, {""count"": ""6"", ""hexIds"": [""82a787fffffffff""]}, {""count"": ""21"", ""hexIds"": [""82bee7fffffffff""]}, {""count"": ""10"", ""hexIds"": [""82b847fffffffff""]}, {""count"": ""5"", ""hexIds"": [""82a617fffffffff""]}, {""count"": ""6"", ""hexIds"": [""82a6a7fffffffff""]}, {""count"": ""7"", ""hexIds"": [""8294effffffffff""]}, {""count"": ""17"", ""hexIds"": [""82bef7fffffffff""]}, {""count"": ""1"", ""hexIds"": [""8294e7fffffffff""]}, {""count"": ""6"", ""hexIds"": [""82a78ffffffffff""]}, {""count"": ""13"", ""hexIds"": [""82a79ffffffffff""]}, {""count"": ""3"", ""hexIds"": [""82b877fffffffff""]}, {""count"": ""5"", ""hexIds"": [""82a797fffffffff""]}, {""count"": ""28"", ""hexIds"": [""82be4ffffffffff""]}, {""count"": ""7"", ""hexIds"": [""829487fffffffff""]}, {""count"": ""4"", ""hexIds"": [""82bedffffffffff""]}, {""count"": ""2"", ""hexIds"": [""82945ffffffffff""]}, {""count"": ""10"", ""hexIds"": [""82b997fffffffff""]}, {""count"": ""4"", ""hexIds"": [""82b9affffffffff""]}, {""count"": ""9"", ""hexIds"": [""829c27fffffffff""]}, {""count"": ""16"", ""hexIds"": [""82a707fffffffff""]}, {""count"": ""3"", ""hexIds"": [""829d07fffffffff""]}, {""count"": ""8"", ""hexIds"": [""82c9b7fffffffff""]}, {""count"": ""2"", ""hexIds"": [""8294affffffffff""]}, {""count"": ""5"", ""hexIds"": [""829d5ffffffffff""]}, {""count"": ""5"", ""hexIds"": [""829d57fffffffff""]}, {""count"": ""1"", ""hexIds"": [""82b80ffffffffff""]}, {""count"": ""11"", ""hexIds"": [""82beaffffffffff""]}, {""count"": ""2"", ""hexIds"": [""82b8b7fffffffff""]}, {""count"": ""1"", ""hexIds"": [""829497fffffffff""]}, {""count"": ""7"", ""hexIds"": [""829d27fffffffff""]}, {""count"": ""2"", ""hexIds"": [""82a7a7fffffffff""]}, {""count"": ""6"", ""hexIds"": [""82b887fffffffff""]}, {""count"": ""7"", ""hexIds"": [""829457fffffffff""]}, {""count"": ""4"", ""hexIds"": [""82c99ffffffffff""]}, {""count"": ""2"", ""hexIds"": [""8294cffffffffff""]}, {""count"": ""4"", ""hexIds"": [""82b88ffffffffff""]}, {""count"": ""3"", ""hexIds"": [""82b98ffffffffff""]}, {""count"": ""7"", ""hexIds"": [""82b837fffffffff""]}, {""count"": ""9"", ""hexIds"": [""829d0ffffffffff""]}, {""count"": ""2"", ""hexIds"": [""8294c7fffffffff""]}, {""count"": ""6"", ""hexIds"": [""829d2ffffffffff""]}, {""count"": ""2"", ""hexIds"": [""829d47fffffffff""]}, {""count"": ""3"", ""hexIds"": [""82b867fffffffff""]}, {""count"": ""1"", ""hexIds"": [""82b807fffffffff""]}, {""count"": ""5"", ""hexIds"": [""82b8a7fffffffff""]}, {""count"": ""2"", ""hexIds"": [""829d67fffffffff""]}, {""count"": ""1"", ""hexIds"": [""82a717fffffffff""]}, {""count"": ""2"", ""hexIds"": [""82b82ffffffffff""]}, {""count"": ""1"", ""hexIds"": [""829c6ffffffffff""]}, {""count"": ""2"", ""hexIds"": [""829c2ffffffffff""]}, {""count"": ""1"", ""hexIds"": [""8294dffffffffff""]}, {""count"": ""1"", ""hexIds"": [""82d897fffffffff""]}, {""count"": ""8"", ""hexIds"": [""82b86ffffffffff""]}, {""count"": ""1"", ""hexIds"": [""82b91ffffffffff""]}, {""count"": ""3"", ""hexIds"": [""82948ffffffffff""]}, {""count"": ""3"", ""hexIds"": [""829c4ffffffffff""]}, {""count"": ""5"", ""hexIds"": [""82b897fffffffff""]}, {""count"": ""1"", ""hexIds"": [""82b89ffffffffff""]}, {""count"": ""1"", ""hexIds"": [""829c07fffffffff""]}, {""count"": ""1"", ""hexIds"": [""82b937fffffffff""]}, {""count"": ""1"", ""hexIds"": [""82949ffffffffff""]}, {""count"": ""1"", ""hexIds"": [""82b99ffffffffff""]}, {""count"": ""1"", ""hexIds"": [""82b987fffffffff""]}, {""count"": ""1"", ""hexIds"": [""8294d7fffffffff""]}, {""count"": ""1"", ""hexIds"": [""82b8dffffffffff""]}, {""count"": ""1"", ""hexIds"": [""829ce7fffffffff""]}, {""count"": ""15"", ""hexIds"": [""82becffffffffff""]}, {""count"": ""13"", ""hexIds"": [""82be1ffffffffff""]}, {""count"": ""1"", ""hexIds"": [""82b827fffffffff""]}]
```

```python
import pandas as pd
import pydeck
df = pd.read_json('aus_h3.duckgl.json')
h3_layer = pydeck.Layer(
    ""H3ClusterLayer"",
    df,
    pickable=True,
    stroked=True,
    filled=True,
    extruded=False,
    get_hexagons=""hexIds"",
    get_fill_color=""[255, (count / %d) * 255, 0]"" % df['count'].max(),
    get_line_color=[255, 255, 255],
    line_width_min_pixels=2,
)
view_state = pydeck.ViewState(latitude=-25.7773677126431,
                              longitude=135.084939479828,
                              zoom=4,
                              bearing=0,
                              pitch=45)
pydeck.Deck(
    layers=[h3_layer],
    initial_view_state=view_state,
    tooltip={""text"": ""Density: {count}""}
).to_html(""aus_h3.duckgl.html"")
```

### Environment

- Framework version: pydeck==0.8.0
- Browser: Brave
- OS: macOS


### Logs

_No response_

"
visgl/deck.gl,"Two hexagons fail to appear: https://github.com/visgl/deck.gl/issues/7819
Description: ### Description

The hex columns aren't rendering. The map of Australia is visible but there is nothing on it.

### Flavors

- [ ] Script tag
- [ ] React
- [X] Python/Jupyter notebook
- [ ] MapboxOverlay
- [ ] GoogleMapsOverlay
- [ ] CartoLayer
- [ ] ArcGIS

### Expected Behavior

I should see hexagon columns appearing on the map.

### Steps to Reproduce

```
$ cat aus_h3.duckgl.json
```

```json
[{""count"": 666583, ""hexIds"": [""82b887fffffffff""]}, {""count"": 615902, ""hexIds"": [""82b8a7fffffffff""]}]
```


```python
import pandas as pd
import pydeck


df = pd.read_json('aus_h3.duckgl.json')

h3_layer = pydeck.Layer(
    ""H3ClusterLayer"",
    df,
    pickable=True,
    stroked=True,
    filled=True,
    extruded=False,
    get_hexagons=""hexIds"",
    get_fill_color=""[255, 255, 0]"",
    get_line_color=[255, 255, 255],
    line_width_min_pixels=2,
)

view_state = pydeck.ViewState(latitude=-25.7773677126431,
                              longitude=135.084939479828,
                              zoom=4,
                              bearing=0,
                              pitch=45)

pydeck.Deck(
    layers=[h3_layer],
    initial_view_state=view_state,
    tooltip={""text"": ""Count: {count}""}
).to_html(""aus_h3.duckgl.html"")
```

### Environment

- Framework version: pydeck==0.8.0
- Browser: Brave
- OS: macOS


### Logs

```
deck: Using Carto base maps
mapbox-gl.js:29 This page appears to be missing CSS declarations for Mapbox GL JS, which may cause the map to display incorrectly. Please ensure your page includes mapbox-gl.css, as described in https://www.mapbox.com/mapbox-gl-js/api/.
x @ mapbox-gl.js:29
```

"
visgl/deck.gl,"[Bug] Interleaving error after upgrading from 8.8 to 8.9: https://github.com/visgl/deck.gl/issues/7818
Description: ### Description

Hello,

Using 8.9.7, setting interleaved to true while using react-map-gl causes the following errors:
1. deck.gl layers disappear
2. The map crashes when re-rendering with the following error: ""can't access property ""stop"", this.animationLoop is null""

I updated directly from 8.8 to 8.9.7, so this could've occurred at any point in the interim.

**Edit**: The break occurs somewhere between 8.8.13 (where it works) and 8.8.27

### Flavors

- [ ] Script tag
- [X] React
- [ ] Python/Jupyter notebook
- [ ] MapboxOverlay
- [ ] GoogleMapsOverlay
- [ ] CartoLayer
- [ ] ArcGIS

### Expected Behavior

_No response_

### Steps to Reproduce

 https://codesandbox.io/s/deckgl-interleaving-8-9-jyb8pl


```
export function App() {
  const [viewState, setViewState] = useState({
    latitude: 40,
    longitude: -74.5,
    zoom: 12
  });
  const onMoveEnd = useCallback(
    debounce(({ viewState }) => {
      setViewState(viewState);
    }, 1000),
    []
  );

  const scatterplotLayer = new ScatterplotLayer({
    id: ""my-scatterplot"",
    data: [{ position: [-74.5, 40], size: 100 }],
    getPosition: (d) => d.position,
    getRadius: (d) => d.size,
    getFillColor: [255, 0, 0]
  });

  return (
    <Map
      initialViewState={viewState}
      onMoveEnd={onMoveEnd}
      style={{ width: ""100vw"", height: ""100vh"" }}
      terrain={{ source: ""mapbox-dem"", exaggeration: 1 }}
      mapStyle=""mapbox://styles/mapbox/light-v9""
      mapboxAccessToken=""API_KEY""
    >
      <DeckGLOverlay layers={[scatterplotLayer]} interleaved={true} />
    </Map>
  );
}
```


### Environment

- Framework version:
- Browser:
- OS:


### Logs

_No response_

"
visgl/deck.gl,"[Bug] SimpleMeshLayer + Tile3DLayer opacity: https://github.com/visgl/deck.gl/issues/7807
Description: ### Description

Hi,

It seems there is some bug in rendering Tile3DLayer when combined with SimpleMeshLayer.

When I set 0 opacity on Tile3DLayer, I can't see SimpleMeshLayer behind. It's somehow cut by Tile3DLayer.

![image](https://user-images.githubusercontent.com/23722745/228880050-dbed431e-ad14-4066-862f-e6f953c7a156.png)

![image](https://user-images.githubusercontent.com/23722745/228880113-e53fc4a0-c1df-4ef2-bb36-e26825f830e5.png)


### Flavors

- [ ] Script tag
- [ ] React
- [ ] Python/Jupyter notebook
- [ ] MapboxOverlay
- [ ] GoogleMapsOverlay
- [ ] CartoLayer
- [ ] ArcGIS

### Expected Behavior

See whole SimpleMeshLayer

### Steps to Reproduce

Just set 0/1 opacity on Tile3DLayer.

https://codesandbox.io/s/deck-gl-3d-tiles-opacity-84gs6u?file=/src/App.js:1335-1346

### Environment

- Framework version: 8.9.4
- Browser: Arc 0.95.1
- OS: MacOs 13.2.1


### Logs

_No response_

"
visgl/deck.gl,"[Bug]Firefox crashes on Radeon Pro 560 while using MaskExtension: https://github.com/visgl/deck.gl/issues/7805
Description: ### Description

My MacBook Pro have two graphic cards: 
<img width=""586"" alt=""image"" src=""https://user-images.githubusercontent.com/536786/228738663-3edc2629-3500-4355-b004-9e00d98d5236.png"">

When I visiit https://deck.gl/examples/mask-extension, firefox crashes if I'm using Radeon Pro 560. It works fine with Intel HD Graphics 630.

### Flavors

- [ ] Script tag
- [ ] React
- [ ] Python/Jupyter notebook
- [ ] MapboxOverlay
- [ ] GoogleMapsOverlay
- [ ] CartoLayer
- [ ] ArcGIS

### Expected Behavior

_No response_

### Steps to Reproduce

1. Switch to Radeon Pro 560 with [gfxCardStatus](https://gfx.io/)
2. Open firefox, visit https://deck.gl/examples/mask-extension


### Environment

- Framework version: 8.9
- Browser: Firefox 111.0.1 (64-bit)
- OS: MacOS 12.4
- Hardware:
<img width=""586"" alt=""image"" src=""https://user-images.githubusercontent.com/536786/228738663-3edc2629-3500-4355-b004-9e00d98d5236.png"">

### Logs

_No response_

"
visgl/deck.gl,"[Bug] Type error when passing a PathStyleExtension object to a PathLayer: https://github.com/visgl/deck.gl/issues/7804
Description: ### Description

Hello! I'm seeing a typescript issue when trying to pass a `PathStyleExtension` object to `PathLayer`'s `extensions` property. I'm importing both `PathStyleExtension` and `PathLayer` from their respective `/typed` modules. I'm using deck.gl/core, deck.gl/layers and deck.gl/extensions all on v8.9.4. I'm defining the extension object like so: `{ offset: true }`, but I run into the same type error even if I fail to define any parameters in the object.

```
const extensions = new PathStyleExtension({ offset: true });

  const pathLayer = new PathLayer({
    extensions: [extensions] // type error
  });

Type 'PathStyleExtension' is not assignable to type 'PathStyleExtension & LayerExtension<undefined>'.
  Type 'PathStyleExtension' is not assignable to type 'LayerExtension<undefined>'.
    Types of property 'opts' are incompatible.
      Type 'PathStyleExtensionOptions' is not assignable to type 'undefined'.
```

<img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/14063740/228618486-52cf015d-764b-4ace-b2d9-215ac9346c3b.png"">


I can bypass this typescript error by simply importing the `PathLayer` from the non-`/typed` module, and the extension and layer behave as expected. However, I would strongly prefer to continue using the fully-typed `PathLayer`.

### Flavors

- [ ] Script tag
- [ ] React
- [ ] Python/Jupyter notebook
- [ ] MapboxOverlay
- [ ] GoogleMapsOverlay
- [ ] CartoLayer
- [ ] ArcGIS

### Expected Behavior

I expect typescript to accept the `PathStyleExtension` type as I've defined it and passed it to the `PathLayer` `extensions` property.

### Steps to Reproduce

Reproducible codesandbox: https://codesandbox.io/s/path-style-extension-type-error-xwg6vq?file=/src/App.tsx

```
import { PathLayer } from '@deck.gl/layers/typed';
import { PathStyleExtension } from '@deck.gl/extensions/typed';

export default function App() {
  const extensions = new PathStyleExtension({ offset: true });

  const pathLayer = new PathLayer({
    extensions: [extensions] // type error - Type 'PathStyleExtension' is not assignable to type 'PathStyleExtension & LayerExtension<undefined>'.
  });

  return (
    <div>
      <h1>Hello CodeSandbox</h1>
      <h2>Start editing to see some magic happen!</h2>
    </div>
  );
}
```

### Environment

- Framework version: deck.gl/core@8.9.4, deck.gl/layer@8.9.4, deck.gl/extensions@8.9.4
- Browser: Chrome 110.0
- OS: macOS 13.2.1


### Logs

_No response_

"
visgl/deck.gl,"[Bug] viewManager is Null, when MapboxGL resize: https://github.com/visgl/deck.gl/issues/7801
Description: ### Description

When Mapbox resize, Deck did not determine whether the object exists




### Flavors

- [ ] Script tag
- [ ] React
- [ ] Python/Jupyter notebook
- [X] MapboxOverlay
- [ ] GoogleMapsOverlay
- [ ] CartoLayer
- [ ] ArcGIS

### Expected Behavior

_No response_

### Steps to Reproduce

> 
>   // Public API
>   // Check if a redraw is needed
>   // Returns `false` or a string summarizing the redraw reason
>   // opts.clearRedrawFlags (Boolean) - clear the redraw flag. Default `true`
>   needsRedraw(opts = {clearRedrawFlags: false}) {
>      if (this.props._animate) {
>        return 'Deck._animate';
>      }
>      let redraw = this._needsRedraw;
>      if (opts.clearRedrawFlags) {
>       this._needsRedraw = false;
>     }
>     const viewManagerNeedsRedraw = this.viewManager.needsRedraw(opts);
>     const layerManagerNeedsRedraw = this.layerManager.needsRedraw(opts);
>     const effectManagerNeedsRedraw = this.effectManager.needsRedraw(opts);
>     const deckRendererNeedsRedraw = this.deckRenderer.needsRedraw(opts);
>     redraw =
>       redraw ||
>       viewManagerNeedsRedraw ||
>       layerManagerNeedsRedraw ||
>       effectManagerNeedsRedraw ||
>       deckRendererNeedsRedraw;
>     return redraw;
>   }


### Environment

- Framework version: deck.gl@8.4.20
- Browser: Chrome 111.0
- OS: w10


### Logs

_No response_
"
keras-team/keras,"data cardinality error: https://github.com/keras-team/keras/issues/18013
Description: I am trying to run simple CNN o a mnist dataset. The objective is to divide the image into equal blocks and execute CNN separately on each block. first I resize mnist images into 4by4. Then, split it into 2by2 blocks i.e. equal to 4 equal parts of each image. In the end, run models through 4 parts and concatenate their output. The execution of four models on different blocks will produce a single output(label)of the image jointly. During the fit, it throws an error, ValueError: Input arrays should have the same number of samples as target arrays. Found 4 input samples and 60000 target samples. I checked to reshape. But did not work. During fit, if I remove * from data_train, then it will be a different error. The complete code is given.

import tensorflow as tf
import seaborn as sns
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train=x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)
x_train=x_train / 255.0
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)
x_test=x_test/255.0

x_train=tf.image.resize(x_train, (4,4))
x_test=tf.image.resize(x_test, (4,4))

#creating blocks..
def blockshaped(arr, nrows, ncols):
""""""
Return an array of shape (n, nrows, ncols) where
n * nrows * ncols = arr.size

If arr is a 2D array, the returned array should look like n subblocks with
each subblock preserving the ""physical"" layout of arr.
""""""
h, w = arr.shape
assert h % nrows == 0, f""{h} rows is not evenly divisible by {nrows}""
assert w % ncols == 0, f""{w} cols is not evenly divisible by {ncols}""
return (arr.reshape(h//nrows, nrows, -1, ncols)
           .swapaxes(1,2)
           .reshape(-1, nrows, ncols))

x_train=x_train.numpy()
x_test=x_test.numpy()

x_train = [blockshaped(x_train[i,:,:,0],2,2) for i in range(len(x_train))]
x_test = [blockshaped(x_test[i,:,:,0],2,2) for i in range(len(x_test))]

y_train = tf.one_hot(y_train.astype(np.int32), depth=10)
y_test = tf.one_hot(y_test.astype(np.int32), depth=10)

#appending train images in four parts
Data_train=[]
for i in range(4):
Data_train.append([])
for j in range(len(x_train)):
Data_train[i].append(np.ndarray.flatten(x_train[j][i]))

Data_test=[]
for i in range(4):
Data_test.append([])
for j in range(len(x_test)):
Data_test[i].append(np.ndarray.flatten(x_test[j][i]))

Data_train=np.asarray(Data_train)

Data_train=Data_train.reshape(-1,4, 2,2)

model_input1 = tf.keras.layers.Input(shape=(2,2))
model1 = tf.keras.layers.Dense(64)(model_input1)
model1 = tf.keras.Model(inputs=model_input1, outputs=model1)

model_input2 = tf.keras.layers.Input(shape=(2,2))
model2 = tf.keras.layers.Dense(64)(model_input2)
model2 = tf.keras.Model(inputs=model_input2, outputs=model2)

model_input3 = tf.keras.layers.Input(shape=(2,2))
model3 = tf.keras.layers.Dense(64)(model_input3)
model3 = tf.keras.Model(inputs=model_input3, outputs=model3)

model_input4 = tf.keras.layers.Input(shape=(2,2))
model4 = tf.keras.layers.Dense(64)(model_input4)
model4 = tf.keras.Model(inputs=model_input4, outputs=model4)

#kernel_model_list.append(model)
#kernel_model_list_input.append(model.input)
#kernel_model_list_output.append(model.output)

#print(kernel_model_list_input[0], kernel_model_list_input[1])

z_input = tf.keras.layers.Concatenate()([model1.output, model2.output, model3.output, model4.output])
z = tf.keras.layers.Dense(32, activation=""relu"")(z_input)
z = tf.keras.layers.Dense(num_classes, activation='softmax')(z)
final_network_model = tf.keras.Model(inputs=[model1.input, model2.input, model3.input, model4.input], outputs=z)

final_network_model.compile(optimizer=tf.keras.optimizers.RMSprop(epsilon=1e-08), loss='categorical_crossentropy',
metrics=['acc'])

history = final_network_model.fit([*Data_train], np.array(y_train),
batch_size=batch_size,
epochs=epochs)

I know the error is due to number of samples of x train and y train. But the main thing is that i resize image first into 4by4. Then created it 2by2 parts of each image. Therefore, there will be four parts of each image, which will be processed independently and then concatenated to create an output. the problem at my end still not worked.

https://colab.research.google.com/drive/1Zdqbt8fKnC5xyNQ-61E5m4gHxmZLAtG0#scrollTo=i7n_d-7SnUmI

x_train has shape (60000,4,2,2) i.e. 60000 images and each image has 2by2 of 4 parts. So the combination of four parts will be equal to one target.


"
PatrikHlobil/Pandas-Bokeh,"[Bug] AttributeError: unexpected attribute 'plot_width' : https://github.com/PatrikHlobil/Pandas-Bokeh/issues/128
Description: Hello,

plot_bokeh worked for me last week, but it seems that there were some updates in pandas or somewhere else and it stopped working. This code from the example (or any other)

```
import numpy as np

np.random.seed(42)
df = pd.DataFrame({""Google"": np.random.randn(1000)+0.2, 
                   ""Apple"": np.random.randn(1000)+0.17}, 
                   index=pd.date_range('1/1/2000', periods=1000))
df = df.cumsum()
df = df + 50
df.plot_bokeh(kind=""line"", plot_width=None) 
```

produces the following error:

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Input In [45], in <cell line: 9>()
      7 df = df.cumsum()
      8 df = df + 50
----> 9 df.plot_bokeh(kind=""line"", plot_width=None)

File /opt/conda/lib/python3.10/site-packages/pandas_bokeh/plot.py:1785, in FramePlotMethods.__call__(self, *args, **kwargs)
   1784 def __call__(self, *args, **kwargs):
-> 1785     return plot(self.df, *args, **kwargs)

File /opt/conda/lib/python3.10/site-packages/pandas_bokeh/plot.py:439, in plot(df_in, x, y, kind, figsize, use_index, title, legend, logx, logy, xlabel, ylabel, xticks, yticks, xlim, ylim, fontsize_title, fontsize_label, fontsize_ticks, fontsize_legend, color, colormap, category, histogram_type, stacked, weights, bins, normed, cumulative, show_average, plot_data_points, plot_data_points_size, number_format, disable_scientific_axes, show_figure, return_html, panning, zooming, sizing_mode, toolbar_location, hovertool, hovertool_string, rangetool, vertical_xlabel, x_axis_location, webgl, reuse_plot, **kwargs)
    432     xlabelname = (
    433         figure_options[""x_axis_label""]
    434         if figure_options.get(""x_axis_label"", """") != """"
    435         else ""x""
    436     )
    438 # Create Figure for plotting:
--> 439 p = figure(**figure_options)
    440 if ""x_axis_type"" not in figure_options:
    441     figure_options[""x_axis_type""] = None

File /opt/conda/lib/python3.10/site-packages/bokeh/plotting/_figure.py:184, in figure.__init__(self, *arg, **kw)
    182 for name in kw.keys():
    183     if name not in names:
--> 184         self._raise_attribute_error_with_matches(name, names | opts.properties())
    186 super().__init__(*arg, **kw)
    188 self.x_range = get_range(opts.x_range)

File /opt/conda/lib/python3.10/site-packages/bokeh/core/has_props.py:369, in HasProps._raise_attribute_error_with_matches(self, name, properties)
    366 if not matches:
    367     matches, text = sorted(properties), ""possible""
--> 369 raise AttributeError(f""unexpected attribute {name!r} to {self.__class__.__name__}, {text} attributes are {nice_join(matches)}"")

AttributeError: unexpected attribute 'plot_width' to figure, similar attributes are outer_width, width or min_width
```

I use Python 3.10 in Jupyter Notebooks, pandas_bokeh installed with pip.

It seems that other people started having the same problem quite recently:

https://stackoverflow.com/questions/74280959/attributeerror-unexpected-attribute-plot-width-to-figure-similar-attributes



"
PatrikHlobil/Pandas-Bokeh,"[Feature Request] Multiple plots in a single plot.: https://github.com/PatrikHlobil/Pandas-Bokeh/issues/121
Description: Being able to simply do `data_frame.plot()` with Pandas-Bokeh is very handy and a huge productivity boost.  Though currently there is no way to do multiple plots in a single plot to my understanding.  An alternative is using bare Bokesh (https://stackoverflow.com/questions/70436454/pandas-bokeh-how-to-plot-two-pd-series-in-a-single-plot-in-colab) and it requires substantially more boilerplates.


"
PatrikHlobil/Pandas-Bokeh,"Constant lines are splited: https://github.com/PatrikHlobil/Pandas-Bokeh/issues/117
Description: **Environment**
Pandas 1.3.3
Python 3.9.7
pandas_bokeh 0.5.5

**Problem**
When drawing constant lines in with `pandas-bokeh` it looks like there are multiple shorter lines instead on large on. When using a zoom tool this looks very odd.

```
import numpy as np
import pandas as pd
import pandas_bokeh

pandas_bokeh.output_notebook()
pd.set_option(""plotting.backend"", ""pandas_bokeh"")

df = pd.DataFrame({'x':7}, index=pd.RangeIndex(20))
df['y'] = df.index.values
df.loc[10:15,'y'] = 10
p = df.plot(kind='line')
```
![bokeh_plot (1)](https://user-images.githubusercontent.com/68053396/142411879-58b63203-5824-4f46-a1a6-83bab1cfcb66.png)

This problem was first mentioned on [SO](https://stackoverflow.com/questions/70017564/constant-values-invisible-in-zoomed-out-bokeh-line-plot).


"
PatrikHlobil/Pandas-Bokeh,"multi y axis charts: https://github.com/PatrikHlobil/Pandas-Bokeh/issues/114
Description: Is it possible to create multi y axis charts like this:

![image](https://user-images.githubusercontent.com/574156/132017382-6c31f8d9-a523-4c3b-9424-296c8d8f7684.png)

or like this?

![image](https://user-images.githubusercontent.com/574156/132017282-d32a8c21-0dcb-49a4-9a88-1801751ab76f.png)

At the moment I can generate only this:

![image](https://user-images.githubusercontent.com/574156/132018929-d0bc43dc-17ca-43a0-941a-da8d86b9a143.png)

with following code:

```
import pandas_bokeh
pandas_bokeh.output_notebook()

dfx = pd.DataFrame({
    'Month': ['2021-01', '2021-02', '2021-03', '2021-04'],
    'Total': [1500, 1200, 1700, 1600],
    'Average': [75.0, 63.15, 73.91, 76.19],
})
dfx.plot_bokeh(x=""Month"",
               title=""Example"",
               line_width=4,
               ylabel = ""Totals"",
               colormap=[""#0000ff99"", ""#ff000099""])
```

"
PatrikHlobil/Pandas-Bokeh,"avascript Error: Model 'AllLabels' does not exist. This could be due to a widget or a custom model not being registered before first usage.: https://github.com/PatrikHlobil/Pandas-Bokeh/issues/112
Description: Dear All. I have installed Pandas-Bokeh to test its functionality. At the first change, I find that the following error returns. 
`Javascript Error: Model 'AllLabels' does not exist. This could be due to a widget or a custom model not being registered before first usage.`
What could be the reason? I will appreciate help

I operate with LinuxMint20.2, Jupyter Lab 3.0.16 and Pandas-Bokeh 0.5

This is the basic code:
`import pandas as pd
import pandas_bokeh
pandas_bokeh.output_notebook()

data.plot_bokeh(kind = ""line"") `
"
slundberg/shap,"Use text template to explain sequential model for recommendations: https://github.com/slundberg/shap/issues/2898
Description: I use RecBole lib to implement sequential model [BERT4Rec](https://recbole.io/docs/user_guide/model/sequential/bert4rec.html).  

Rec model input: list of items.
Rec model output: array with shape (all_item_num)

I tried to change the standard input of the model to a text input to fit the multi-class text classification problem.
For this I made my own function using the [tutorial](https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/sentiment_analysis/Using%20custom%20functions%20and%20tokenizers.html).

You can run it on kaggle: https://www.kaggle.com/code/payngay/anime-recommender-recbole-with-shap

```python
def f(sequences):
    print(sequences)
    # future output 2d-array
    out = []
    proc_sequences = []
    # i don't know if it's correct to change strokes in this way
    for seq in sequences:
        seq = seq.replace('...', '')
        seq = seq.split()
        seq = list(map(int, seq))
        proc_sequences.append(seq)
    # applying model for each sequence with closed items
    for seq in proc_sequences:
        if len(seq) == 0:
            seq = [0]
            len_seq = 0
        len_seq = len(seq)
        input_inter = Interaction({
            'user_id': torch.tensor([1]),
            'anime_id_list': torch.tensor([seq]),
            'item_length': torch.tensor([len_seq]),
            })
        with torch.no_grad():
            scores = model.full_sort_predict(input_inter)
            # predictions for 10 items for simplify output
            out.append(scores[0][:10].numpy())
    return np.array(out)


masker = shap.maskers.Text(r""\W"") # this will create a basic whitespace tokenizer
explainer = shap.Explainer(f, masker)

shap_values = explainer(['5 13 23 43'])
print(shap_values)
shap.plots.text(shap_values)
```
### Every input array for func f when i calling explainer 
```
['...']
['5 13 23 43']
['5 13 ...' '...23 43']
['5 ...' '...13 ...' '5 ...23 43' '...13 23 43' '5 13 23 ...' '5 13 ...43']
['...23 ...' '...43']
```
### Explainer output:
```
.values =
array([[[-7.29364038e-01,  6.33913875e-01,  3.06767166e-01,
          5.47838673e-01, -4.79293227e-01,  5.97867481e-01,
          4.52940829e-01, -4.58241999e-02,  6.36546075e-01,
          4.77575630e-01],
        [ 5.61337829e-01, -2.03750122e+00, -1.11684850e+00,
         -1.86921732e+00,  8.70344520e-01, -1.09398686e+00,
         -1.92754234e+00, -4.23363656e-01, -1.78751212e+00,
         -1.63428947e+00],
        [-4.06678915e-02,  6.12927675e-02,  2.17219517e-02,
          5.36474288e-02, -3.40525508e-02,  4.00626063e-02,
          5.31172156e-02, -5.63263893e-04,  5.68146110e-02,
          3.87389213e-02],
        [-3.82922888e-02,  6.39134645e-02,  2.30076090e-02,
          5.62215149e-02, -3.38643193e-02,  4.03074622e-02,
          5.62291741e-02,  1.09529495e-03,  5.86182475e-02,
          4.09799069e-02]]])

.base_values =
array([[-7.06466484,  3.53171921,  0.71773797,  3.05890965, -3.16552186,
         1.5206027 ,  2.77057242, -1.25324082,  2.98390007,  1.62703848]])

.data =
(array(['5 ', '13 ', '23 ', '43'], dtype=object),)
```

### Questions:
What are these dots in [input arrays for func f when i calling explainer](#every-input-array-for-func-f-when-i-calling-explainer)?
Is it correct to predict values just by throwing out these dots?
Are there ways to solve the problem of interpreting predictions in a better way?


"
slundberg/shap,"The SHAP explanations do not sum up to the model's output!: https://github.com/slundberg/shap/issues/2875
Description: Hi I have tried running the below script initially on my jupyetr notebook and ran without any errors:

background = X_train[np.random.choice(X_train.shape[0], 10, replace=False)]

# explain predictions of the model on three images
e = shap.DeepExplainer(covid_CRmodel, np.expand_dims(background,axis=-1))
# ...or pass tensors directly
# e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)
X_test_expand =  np.expand_dims(X_test,axis=-1)
shap_values = e.shap_values(X_test_expand[5:7])


I'm trying to update it onot the Django interface that I'm developing and I'm receiving an error response like this  - ""The SHAP explanations do not sum up to the model's output! This is either because of a rounding error or because an operator in your computation graph was not fully supported. If the sum difference of 2.694559 is significant compared the scale of your model outputs please post as a github issue, with a reproducable example if possible so we can debug it.""

I have also tried all the suggestions like changing check_additivity=False etc but still unable to resolve it. Let me know of any clues.

"
slundberg/shap,"question to tensorflow decision forests: https://github.com/slundberg/shap/issues/2869
Description: hi,

are there plans to support https://github.com/tensorflow/decision-forests?

would like to plot a Layered violin plot as shown here:
https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Scatter%20Density%20vs.%20Violin%20Plot%20Comparison.html#Layered-violin-plot
"
InsightLab/PyMove,"Add files via upload: https://github.com/InsightLab/PyMove/pull/225
Description: ## Description

-   A brief summary of the four introductory notebooks.


"
InsightLab/PyMove,"Add files via upload: https://github.com/InsightLab/PyMove/pull/218
Description: ## Description

A notebook containing the summary of the first four notebooks.


"
InsightLab/PyMove,"Add files via upload: https://github.com/InsightLab/PyMove/pull/217
Description: ## Description

A quick and concise overview of some PyMove modules.
I included the way to import the library in the Google Colab environment.

"
InsightLab/PyMove,"ImportError: cannot import name 'FilePathOrBuffer' from 'pandas._typing': https://github.com/InsightLab/PyMove/issues/216
Description: **Describe the bug**
Importing pymove raises `ImportError: cannot import name 'FilePathOrBuffer' from 'pandas._typing'`

**To Reproduce**
Steps to reproduce the behavior:
1. Conda install pymove
2. Run import pymove 

```
(pymove) PS C:\Users\anita> python
Python 3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:34:17) [MSC v.1929 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pymove
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""H:\miniconda3\envs\pymove\lib\site-packages\pymove\__init__.py"", line 13, in <module>
    from .core.pandas import PandasMoveDataFrame
  File ""H:\miniconda3\envs\pymove\lib\site-packages\pymove\core\pandas.py"", line 47, in <module>
    from pymove.utils.trajectories import shift
  File ""H:\miniconda3\envs\pymove\lib\site-packages\pymove\utils\trajectories.py"", line 23, in <module>
    from pandas._typing import FilePathOrBuffer
ImportError: cannot import name 'FilePathOrBuffer' from 'pandas._typing' (H:\miniconda3\envs\pymove\lib\site-packages\pandas\_typing.py)
```


**Desktop (please complete the following information):**
 - OS: Win10

```
(pymove) PS C:\Users\anita> conda list
# packages in environment at H:\miniconda3\envs\pymove:
#
# Name                    Version                   Build  Channel
argon2-cffi               21.3.0             pyhd8ed1ab_0    conda-forge
argon2-cffi-bindings      21.2.0          py310he2412df_2    conda-forge
asttokens                 2.0.5              pyhd8ed1ab_0    conda-forge
attrs                     21.4.0             pyhd8ed1ab_0    conda-forge
backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
backports                 1.0                        py_2    conda-forge
backports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    conda-forge
beautifulsoup4            4.11.1             pyha770c72_0    conda-forge
bleach                    5.0.0              pyhd8ed1ab_0    conda-forge
bokeh                     2.4.2           py310h5588dad_1    conda-forge
branca                    0.4.2              pyhd8ed1ab_0    conda-forge
brotli                    1.0.9                h8ffe710_7    conda-forge
brotli-bin                1.0.9                h8ffe710_7    conda-forge
brotlipy                  0.7.0           py310he2412df_1004    conda-forge
bzip2                     1.0.8                h8ffe710_4    conda-forge
ca-certificates           2021.10.8            h5b45459_0    conda-forge
certifi                   2021.10.8       py310h5588dad_2    conda-forge
cffi                      1.15.0          py310hcbf9ad4_0    conda-forge
charset-normalizer        2.0.12             pyhd8ed1ab_0    conda-forge
click                     8.1.2           py310h5588dad_0    conda-forge
cloudpickle               2.0.0              pyhd8ed1ab_0    conda-forge
colorama                  0.4.4              pyh9f0ad1d_0    conda-forge
convertdate               2.4.0              pyhd8ed1ab_0    conda-forge
cryptography              36.0.2          py310ha857299_1    conda-forge
cycler                    0.11.0             pyhd8ed1ab_0    conda-forge
cytoolz                   0.11.2          py310he2412df_2    conda-forge
dask                      2022.4.1           pyhd8ed1ab_0    conda-forge
dask-core                 2022.4.1           pyhd8ed1ab_0    conda-forge
debugpy                   1.6.0           py310h8a704f9_0    conda-forge
decorator                 5.1.1              pyhd8ed1ab_0    conda-forge
defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
distributed               2022.4.1           pyhd8ed1ab_0    conda-forge
entrypoints               0.4                pyhd8ed1ab_0    conda-forge
executing                 0.8.3              pyhd8ed1ab_0    conda-forge
flit-core                 3.7.1              pyhd8ed1ab_0    conda-forge
folium                    0.12.1.post1       pyhd8ed1ab_1    conda-forge
fonttools                 4.32.0          py310he2412df_0    conda-forge
freetype                  2.10.4               h546665d_1    conda-forge
fsspec                    2022.3.0           pyhd8ed1ab_0    conda-forge
geohash2                  1.1                        py_0    conda-forge
geojson                   2.5.0                      py_0    conda-forge
geos                      3.10.2               h39d44d4_0    conda-forge
heapdict                  1.0.1                      py_0    conda-forge
hijri-converter           2.2.3              pyhd8ed1ab_0    conda-forge
holidays                  0.13               pyhd8ed1ab_0    conda-forge
idna                      3.3                pyhd8ed1ab_0    conda-forge
importlib-metadata        4.11.3          py310h5588dad_1    conda-forge
importlib_resources       5.7.1              pyhd8ed1ab_0    conda-forge
intel-openmp              2022.0.0          h57928b3_3663    conda-forge
ipykernel                 6.13.0          py310hbbfc1a7_0    conda-forge
ipython                   8.2.0           py310h5588dad_0    conda-forge
ipython_genutils          0.2.0                      py_1    conda-forge
ipywidgets                7.7.0              pyhd8ed1ab_0    conda-forge
jbig                      2.1               h8d14728_2003    conda-forge
jedi                      0.18.1          py310h5588dad_1    conda-forge
jinja2                    3.1.1              pyhd8ed1ab_0    conda-forge
joblib                    1.1.0              pyhd8ed1ab_0    conda-forge
jpeg                      9e                   h8ffe710_0    conda-forge
jsonschema                4.4.0              pyhd8ed1ab_0    conda-forge
jupyter_client            7.2.2              pyhd8ed1ab_1    conda-forge
jupyter_core              4.9.2           py310h5588dad_0    conda-forge
jupyterlab_pygments       0.2.2              pyhd8ed1ab_0    conda-forge
jupyterlab_widgets        1.1.0              pyhd8ed1ab_0    conda-forge
kiwisolver                1.4.2           py310h476a331_1    conda-forge
korean_lunar_calendar     0.2.1              pyh9f0ad1d_0    conda-forge
lcms2                     2.12                 h2a16943_0    conda-forge
lerc                      3.0                  h0e60522_0    conda-forge
libblas                   3.9.0              14_win64_mkl    conda-forge
libbrotlicommon           1.0.9                h8ffe710_7    conda-forge
libbrotlidec              1.0.9                h8ffe710_7    conda-forge
libbrotlienc              1.0.9                h8ffe710_7    conda-forge
libcblas                  3.9.0              14_win64_mkl    conda-forge
libdeflate                1.10                 h8ffe710_0    conda-forge
libffi                    3.4.2                h8ffe710_5    conda-forge
liblapack                 3.9.0              14_win64_mkl    conda-forge
libpng                    1.6.37               h1d00b33_2    conda-forge
libsodium                 1.0.18               h8d14728_1    conda-forge
libtiff                   4.3.0                hc4061b1_3    conda-forge
libwebp                   1.2.2                h57928b3_0    conda-forge
libwebp-base              1.2.2                h8ffe710_1    conda-forge
libxcb                    1.13              hcd874cb_1004    conda-forge
libzlib                   1.2.11            h8ffe710_1014    conda-forge
locket                    0.2.0                      py_2    conda-forge
lz4                       4.0.0           py310h7b86b54_1    conda-forge
lz4-c                     1.9.3                h8ffe710_1    conda-forge
m2w64-gcc-libgfortran     5.3.0                         6    conda-forge
m2w64-gcc-libs            5.3.0                         7    conda-forge
m2w64-gcc-libs-core       5.3.0                         7    conda-forge
m2w64-gmp                 6.1.0                         2    conda-forge
m2w64-libwinpthread-git   5.0.0.4634.697f757               2    conda-forge
markupsafe                2.1.1           py310he2412df_1    conda-forge
matplotlib-base           3.5.1           py310h79a7439_0    conda-forge
matplotlib-inline         0.1.3              pyhd8ed1ab_0    conda-forge
mistune                   0.8.4           py310he2412df_1005    conda-forge
mkl                       2022.0.0           h0e2418a_796    conda-forge
mplleaflet                0.0.5                      py_4    conda-forge
msgpack-python            1.0.3           py310h476a331_1    conda-forge
msys2-conda-epoch         20160418                      1    conda-forge
munkres                   1.1.4              pyh9f0ad1d_0    conda-forge
nbclient                  0.6.0              pyhd8ed1ab_0    conda-forge
nbconvert                 6.5.0              pyhd8ed1ab_0    conda-forge
nbconvert-core            6.5.0              pyhd8ed1ab_0    conda-forge
nbconvert-pandoc          6.5.0              pyhd8ed1ab_0    conda-forge
nbformat                  5.3.0              pyhd8ed1ab_0    conda-forge
nest-asyncio              1.5.5              pyhd8ed1ab_0    conda-forge
notebook                  6.4.10             pyha770c72_0    conda-forge
numpy                     1.22.3          py310hcae7c84_2    conda-forge
openjpeg                  2.4.0                hb211442_1    conda-forge
openssl                   1.1.1n               h8ffe710_0    conda-forge
packaging                 21.3               pyhd8ed1ab_0    conda-forge
pandas                    1.4.2           py310hf5e1058_1    conda-forge
pandoc                    2.18                 h57928b3_0    conda-forge
pandocfilters             1.5.0              pyhd8ed1ab_0    conda-forge
parso                     0.8.3              pyhd8ed1ab_0    conda-forge
partd                     1.2.0              pyhd8ed1ab_0    conda-forge
pickleshare               0.7.5                   py_1003    conda-forge
pillow                    9.1.0           py310h767b3fd_2    conda-forge
pip                       22.0.4             pyhd8ed1ab_0    conda-forge
prometheus_client         0.14.1             pyhd8ed1ab_0    conda-forge
prompt-toolkit            3.0.29             pyha770c72_0    conda-forge
psutil                    5.9.0           py310he2412df_1    conda-forge
pthread-stubs             0.4               hcd874cb_1001    conda-forge
pure_eval                 0.2.2              pyhd8ed1ab_0    conda-forge
pycparser                 2.21               pyhd8ed1ab_0    conda-forge
pygments                  2.11.2             pyhd8ed1ab_0    conda-forge
pymeeus                   0.5.10             pyhd8ed1ab_0    conda-forge
pymove                    3.0.0              pyhd8ed1ab_0    conda-forge
pyopenssl                 22.0.0             pyhd8ed1ab_0    conda-forge
pyparsing                 3.0.8              pyhd8ed1ab_0    conda-forge
pyrsistent                0.18.1          py310he2412df_1    conda-forge
pysocks                   1.7.1           py310h5588dad_5    conda-forge
python                    3.10.4          h9a09f29_0_cpython    conda-forge
python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
python-fastjsonschema     2.15.3             pyhd8ed1ab_0    conda-forge
python_abi                3.10                    2_cp310    conda-forge
pytz                      2022.1             pyhd8ed1ab_0    conda-forge
pywin32                   303             py310he2412df_0    conda-forge
pywinpty                  2.0.5           py310h00ffb61_1    conda-forge
pyyaml                    6.0             py310he2412df_4    conda-forge
pyzmq                     22.3.0          py310h73ada01_2    conda-forge
requests                  2.27.1             pyhd8ed1ab_0    conda-forge
scikit-learn              1.0.2           py310h4dafddf_0    conda-forge
scipy                     1.8.0           py310h33db832_1    conda-forge
send2trash                1.8.0              pyhd8ed1ab_0    conda-forge
setuptools                62.1.0          py310h5588dad_0    conda-forge
shapely                   1.8.0           py310h3578588_6    conda-forge
six                       1.16.0             pyh6c4a22f_0    conda-forge
sortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge
soupsieve                 2.3.1              pyhd8ed1ab_0    conda-forge
sqlite                    3.38.2               h8ffe710_0    conda-forge
stack_data                0.2.0              pyhd8ed1ab_0    conda-forge
tbb                       2021.5.0             h2d74725_1    conda-forge
tblib                     1.7.0              pyhd8ed1ab_0    conda-forge
terminado                 0.13.3          py310h5588dad_1    conda-forge
threadpoolctl             3.1.0              pyh8a188c0_0    conda-forge
tinycss2                  1.1.1              pyhd8ed1ab_0    conda-forge
tk                        8.6.12               h8ffe710_0    conda-forge
toolz                     0.11.2             pyhd8ed1ab_0    conda-forge
tornado                   6.1             py310he2412df_3    conda-forge
tqdm                      4.64.0             pyhd8ed1ab_0    conda-forge
traitlets                 5.1.1              pyhd8ed1ab_0    conda-forge
typing_extensions         4.2.0              pyha770c72_0    conda-forge
tzdata                    2022a                h191b570_0    conda-forge
ucrt                      10.0.20348.0         h57928b3_0    conda-forge
unicodedata2              14.0.0          py310he2412df_1    conda-forge
urllib3                   1.26.9             pyhd8ed1ab_0    conda-forge
vc                        14.2                 hb210afc_6    conda-forge
vs2015_runtime            14.29.30037          h902a5da_6    conda-forge
wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
webencodings              0.5.1                      py_1    conda-forge
wheel                     0.37.1             pyhd8ed1ab_0    conda-forge
widgetsnbextension        3.6.0           py310h5588dad_0    conda-forge
win_inet_pton             1.1.0           py310h5588dad_4    conda-forge
winpty                    0.4.3                         4    conda-forge
xorg-libxau               1.0.9                hcd874cb_0    conda-forge
xorg-libxdmcp             1.1.3                hcd874cb_0    conda-forge
xz                        5.2.5                h62dcd97_1    conda-forge
yaml                      0.2.5                h8ffe710_2    conda-forge
zeromq                    4.3.4                h0e60522_1    conda-forge
zict                      2.1.0              pyhd8ed1ab_0    conda-forge
zipp                      3.8.0              pyhd8ed1ab_0    conda-forge
zlib                      1.2.11            h8ffe710_1014    conda-forge
zstd                      1.5.2                h6255e5f_0    conda-forge
```



"
InsightLab/PyMove,"Folium throwing a bunch of errors: https://github.com/InsightLab/PyMove/issues/209
Description: Hi,

I'm just trying out PyMove because it seems like it could bring a lot to my current project.
But I can't get any folium functionality to work, though.

Here's an extract of what I'm doing:

```python
df.head()
```
<html>
<body>
<!--StartFragment-->

absolute_time | lat | lng | id
-- | -- | -- | --
2021-09-09 10:32:27.000 | 45.818459 | 3.139887 | 54
2021-09-09 10:32:28.000 | 45.818459 | 3.139887 | 54
2021-09-09 10:32:29.044 | 45.818459 | 3.139887 | 54
2021-09-09 10:32:30.000 | 45.818459 | 3.139887 | 54
2021-09-09 10:32:31.000 | 45.818459 | 3.139887 | 54

<!--EndFragment-->
</body>
</html>

```python
move_df = MoveDataFrame(data=df, latitude='lat', longitude='lng', datetime='absolute_time')

from pymove import folium as f
f.plot_trajectories(move_df)
```
```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-159-487305dba7d4> in <module>()
      1 from pymove import folium as f
----> 2 f.plot_trajectories(move_df)

2 frames
/usr/local/lib/python3.7/dist-packages/pymove/visualization/folium.py in plot_trajectories(move_data, n_rows, lat_origin, lon_origin, zoom_start, legend, base_map, tile, save_as_html, color, color_by_id, filename)
    993 
    994     _add_trajectories_to_map(
--> 995         mv_df, items, base_map, legend, save_as_html, filename
    996     )
    997 

/usr/local/lib/python3.7/dist-packages/pymove/visualization/folium.py in _add_trajectories_to_map(move_data, items, base_map, legend, save_as_html, filename)
    896         mv = move_data[move_data[TRAJ_ID] == _id]
    897 
--> 898         _add_begin_end_markers_to_map(mv, base_map, color, _id)
    899 
    900         folium.PolyLine(

/usr/local/lib/python3.7/dist-packages/pymove/visualization/folium.py in _add_begin_end_markers_to_map(move_data, base_map, color, _id)
    837         popup='Início',
    838         icon=plugins.BeautifyIcon(
--> 839             icon='play', icon_shape='marker', background_color=color or 'green'
    840         )
    841     ).add_to(points)

TypeError: __init__() got an unexpected keyword argument 'color'
```

Or if I just try to plot the bounding box, I get a different error.

```python
f.plot_bbox(move_df.get_bbox(), color='blue')
```
```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-161-03377951b622> in <module>()
----> 1 f.plot_bbox(move_df.get_bbox(), color='blue')

6 frames
/usr/local/lib/python3.7/dist-packages/pymove/visualization/folium.py in plot_bbox(bbox_tuple, base_map, tiles, color, save_as_html, filename)
   1688         base_map = folium.Map(tiles=tiles)
   1689     base_map.fit_bounds(
-> 1690         [[bbox_tuple[0], bbox_tuple[1]], [bbox_tuple[2], bbox_tuple[3]]]
   1691     )
   1692     points_ = [

/usr/local/lib/python3.7/dist-packages/folium/folium.py in fit_bounds(self, bounds, padding_top_left, padding_bottom_right, padding, max_zoom)
    411             'Choropleth class, which has the same arguments. See the example '
    412             'notebook \'GeoJSON_and_choropleth\' for how to do this.',
--> 413             FutureWarning
    414         )
    415         from folium.features import Choropleth

/usr/local/lib/python3.7/dist-packages/folium/map.py in __init__(self, bounds, padding_top_left, padding_bottom_right, padding, max_zoom)
    456         Bounding box specified as two points [southwest, northeast]
    457     padding_top_left: (x, y) point, default None
--> 458         Padding in the top left corner. Useful if some elements in
    459         the corner, such as controls, might obscure objects you're zooming
    460         to.

/usr/lib/python3.7/json/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)
    229         cls is None and indent is None and separators is None and
    230         default is None and not sort_keys and not kw):
--> 231         return _default_encoder.encode(obj)
    232     if cls is None:
    233         cls = JSONEncoder

/usr/lib/python3.7/json/encoder.py in encode(self, o)
    197         # exceptions aren't as detailed.  The list call should be roughly
    198         # equivalent to the PySequence_Fast that ''.join() would do.
--> 199         chunks = self.iterencode(o, _one_shot=True)
    200         if not isinstance(chunks, (list, tuple)):
    201             chunks = list(chunks)

/usr/lib/python3.7/json/encoder.py in iterencode(self, o, _one_shot)
    255                 self.key_separator, self.item_separator, self.sort_keys,
    256                 self.skipkeys, _one_shot)
--> 257         return _iterencode(o, 0)
    258 
    259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,

/usr/lib/python3.7/json/encoder.py in default(self, o)
    177 
    178         """"""
--> 179         raise TypeError(f'Object of type {o.__class__.__name__} '
    180                         f'is not JSON serializable')
    181 

TypeError: Object of type float16 is not JSON serializable
```


I'm using python version 3.7, `PyMove` version 3.0.0, and `folium` version 0.8.3

"
InsightLab/PyMove,"Update docs: https://github.com/InsightLab/PyMove/pull/206
Description: ## Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
-   [ ] Bug fix (non-breaking change which fixes an issue)
-   [ ] New feature (non-breaking change which adds functionality)
-   [ ] Breaking change (fix or feature that would cause existing functionality to change)
-   [ ] I have read the **CONTRIBUTING** document.
-   [ ] My code follows the code style of this project (see `.code-style.md`).
-   [ ] All new and existing tests passed (see `.testing.md`).
-   [ ] I have added tests to cover my changes.
-   [ ] My change requires a change to the documentation.
-   [ ] I have updated the documentation accordingly (see `.documentation.md`).

## Description

-   What is the current behavior? (You can also link to an open issue here)
    <!--- Discuss the new functionality or bug --->

-   What is the new behavior (if this is a feature change)?
  Updated notebooks examples
"
tkrabel/bamboolib,"How can I activate bamboolib inside my jupyer lab environment under a proper running venv?: https://github.com/tkrabel/bamboolib/issues/47
Description: ### Environment

* Operating System: Microsoft Windows 10 Pro, 10.0.19044 Build 19044
* Python Version: 3.10.11
* How did you install bamboolib: Installed bamboolib with pip install bamboolib inside a virtual environment build with venv
* Python packages: `Package                           Version
	--------------------------------- ---------
	aiofiles                          22.1.0
	aiosqlite                         0.18.0
	anyio                             3.6.2
	appdirs                           1.4.4
	argon2-cffi                       21.3.0
	argon2-cffi-bindings              21.2.0
	arrow                             1.2.3
	astor                             0.8.1
	astroid                           2.15.1
	asttokens                         2.2.1
	attrs                             22.2.0
	Babel                             2.12.1
	backcall                          0.2.0
	bamboolib                         1.30.19
	beautifulsoup4                    4.12.1
	bleach                            6.0.0
	certifi                           2022.12.7
	cffi                              1.15.1
	charset-normalizer                3.1.0
	colorama                          0.4.6
	comm                              0.1.3
	contourpy                         1.0.7
	cryptography                      40.0.1
	cycler                            0.11.0
	debugpy                           1.6.6
	decorator                         5.1.1
	defusedxml                        0.7.1
	dill                              0.3.6
	executing                         1.2.0
	fastjsonschema                    2.16.3
	fonttools                         4.39.3
	fqdn                              1.5.1
	htmlmin                           0.1.12
	idna                              3.4
	ImageHash                         4.3.1
	ipykernel                         6.22.0
	ipyslickgrid                      0.0.3
	ipython                           8.12.0
	ipython-genutils                  0.2.0
	ipywidgets                        7.7.5
	isoduration                       20.11.0
	isort                             5.12.0
	jedi                              0.18.2
	Jinja2                            3.1.2
	joblib                            1.2.0
	json5                             0.9.11
	jsonpointer                       2.3
	jsonschema                        4.17.3
	jupyter_client                    8.1.0
	jupyter-contrib-core              0.4.2
	jupyter_core                      5.3.0
	jupyter-events                    0.6.3
	jupyter-nbextensions-configurator 0.6.1
	jupyter_server                    2.5.0
	jupyter_server_fileid             0.8.0
	jupyter_server_terminals          0.4.4
	jupyter_server_ydoc               0.8.0
	jupyter-ydoc                      0.2.3
	jupyterlab                        3.6.3
	jupyterlab-pygments               0.2.2
	jupyterlab_server                 2.22.0
	jupyterlab-widgets                1.1.4
	kiwisolver                        1.4.4
	lazy-object-proxy                 1.9.0
	MarkupSafe                        2.1.2
	matplotlib                        3.6.3
	matplotlib-inline                 0.1.6
	mccabe                            0.7.0
	mistune                           2.0.5
	multimethod                       1.9.1
	nbclassic                         0.5.5
	nbclient                          0.7.3
	nbconvert                         7.3.0
	nbformat                          5.8.0
	nest-asyncio                      1.5.6
	networkx                          3.0
	notebook                          6.5.3
	notebook_shim                     0.2.2
	numpy                             1.23.5
	packaging                         23.0
	pandas                            1.5.3
	pandas-profiling                  3.6.6
	pandasgui                         0.2.14
	pandocfilters                     1.5.0
	parso                             0.8.3
	patsy                             0.5.3
	phik                              0.12.3
	pickleshare                       0.7.5
	Pillow                            9.5.0
	pip                               23.0.1
	platformdirs                      3.2.0
	plotly                            5.14.0
	ppscore                           1.3.0
	prometheus-client                 0.16.0
	prompt-toolkit                    3.0.38
	psutil                            5.9.4
	pure-eval                         0.2.2
	pyarrow                           11.0.0
	pycparser                         2.21
	pydantic                          1.10.7
	Pygments                          2.14.0
	pylint                            2.17.1
	pynput                            1.7.6
	pyparsing                         3.0.9
	PyQt5                             5.15.9
	PyQt5-Qt5                         5.15.2
	PyQt5-sip                         12.11.1
	PyQtWebEngine                     5.15.6
	PyQtWebEngine-Qt5                 5.15.2
	pyrsistent                        0.19.3
	python-dateutil                   2.8.2
	python-json-logger                2.0.7
	pytz                              2023.3
	PyWavelets                        1.4.1
	pywin32                           306
	pywinpty                          2.0.10
	PyYAML                            6.0
	pyzmq                             25.0.2
	qtstylish                         0.1.5
	requests                          2.28.2
	rfc3339-validator                 0.1.4
	rfc3986-validator                 0.1.1
	scikit-learn                      1.2.2
	scipy                             1.9.3
	seaborn                           0.12.2
	Send2Trash                        1.8.0
	setuptools                        65.5.0
	six                               1.16.0
	sniffio                           1.3.0
	soupsieve                         2.4
	stack-data                        0.6.2
	statsmodels                       0.13.5
	tangled-up-in-unicode             0.2.0
	tenacity                          8.2.2
	terminado                         0.17.1
	threadpoolctl                     3.1.0
	tinycss2                          1.2.1
	toml                              0.10.2
	tomli                             2.0.1
	tomlkit                           0.11.7
	tornado                           6.2
	tqdm                              4.64.1
	traitlets                         5.9.0
	typeguard                         2.13.3
	typing_extensions                 4.5.0
	uri-template                      1.2.0
	urllib3                           1.26.15
	visions                           0.7.5
	wcwidth                           0.2.6
	webcolors                         1.13
	webencodings                      0.5.1
	websocket-client                  1.5.1
	widgetsnbextension                3.6.4
	wordcloud                         1.8.2.2
	wrapt                             1.15.0
	xlrd                              2.0.1
	y-py                              0.5.9
	ydata-profiling                   4.1.2
	ypy-websocket                     0.8.2` 
* If bamboolib is used with JupyterLab: JupyterLab extensions: `jupyterlab-plotly v5.14.0 enabled ok  
    jupyterlab_pygments v0.2.2 enabled ok (python, jupyterlab_pygments)  
    @jupyter-widgets/jupyterlab-manager v3.1.4 enabled ok (python, jupyterlab_widgets)`  

* If bamboolib is used with Jupyter Notebook: It's running as expected
### Description of Issue

* What did you expect to happen?
Inside my venv with the installed version on bamboolib I want to be able to execute bamboolib inside jupyter lab.

* What happened instead?
Inside my venv  with the installed version on bamboolib I'm getting this error:
´Error displaying widget: model not found` 

### What steps have you taken to resolve this already?

```
import bamboolib as bam
bam.test_setup()
```

Testing Jupyter extensions:
ipywidgets works

ipywidgets:


ipyslickgrid:
Python library version is 0.0.3

Error displaying widget: model not found


plotly:
Python library version is 5.14.0


bamboolib:
Python library version is 1.30.19
Needed version of Jupyter extension is 1.30.0

Error displaying widget: model not found


JupyterLab and Labextensions:


### Anything else?
I'm looking forward to your hint to get bamboolib running inside jupyter lab.
Thx
...

"
tkrabel/bamboolib,"Create new column formula, replacement error: https://github.com/tkrabel/bamboolib/issues/45
Description: ### Environment

* Operating System: Linux
* Python Version: Python 3.9.16
* How did you install bamboolib: pip
* Python packages: 
```
Package                   Version
------------------------- ---------
aiofiles                  22.1.0
aiosqlite                 0.18.0
ansi                      0.3.6
anyio                     3.6.2
argon2-cffi               21.3.0
argon2-cffi-bindings      21.2.0
arrow                     1.2.3
astroid                   2.13.5
asttokens                 2.2.1
astunparse                1.6.3
attrs                     22.2.0
Babel                     2.11.0
backcall                  0.2.0
bamboolib                 1.30.19
beautifulsoup4            4.11.2
binaryornot               0.4.4
bleach                    6.0.0
boto3                     1.26.47
botocore                  1.29.63
certifi                   2022.12.7
cffi                      1.15.1
chardet                   5.1.0
charset-normalizer        3.1.0
cleverdict                1.9.2
click                     8.1.3
click-plugins             1.1.1
cligj                     0.7.2
colorama                  0.4.6
comm                      0.1.2
conda-pack                0.7.0
cookiecutter              2.1.1
cryptography              39.0.0
cx-Oracle                 8.3.0
cycler                    0.11.0
damenu                    0.3.1
DAutils                   0.3
debugpy                   1.6.6
decorator                 5.1.1
defusedxml                0.7.1
descartes                 1.1.0
dill                      0.3.6
et-xmlfile                1.1.0
executing                 1.2.0
fastjsonschema            2.16.2
Fiona                     1.9.0
flake8                    6.0.0
fonttools                 4.38.0
fqdn                      1.5.1
fsspec                    2023.1.0
future                    0.18.3
geopandas                 0.12.2
gitdb                     4.0.10
GitPython                 3.1.30
graphviz                  0.20.1
greenlet                  2.0.2
idna                      3.4
importlib-metadata        6.0.0
ipykernel                 6.21.1
ipyslickgrid              0.0.3
ipython                   8.9.0
ipython-genutils          0.2.0
ipywidgets                7.7.2
isoduration               20.11.0
isort                     5.12.0
jedi                      0.18.2
Jinja2                    3.1.2
jinja2-time               0.2.0
jmespath                  1.0.1
joblib                    1.2.0
json5                     0.9.11
jsonpointer               2.3
jsonschema                4.17.3
jupyter_client            8.0.3
jupyter_core              5.2.0
jupyter-events            0.5.0
jupyter_server            2.0.6
jupyter_server_fileid     0.6.0
jupyter-server-mathjax    0.2.6
jupyter_server_terminals  0.4.4
jupyter_server_ydoc       0.6.1
jupyter-ydoc              0.2.2
jupyterlab                3.5.2
jupyterlab-git            0.41.0
jupyterlab-pygments       0.2.2
jupyterlab_server         2.19.0
jupyterlab-widgets        1.1.1
kiwisolver                1.4.4
lazy-object-proxy         1.9.0
MarkupSafe                2.1.2
matplotlib                3.5.3
matplotlib-inline         0.1.6
mccabe                    0.7.0
mistune                   2.0.5
munch                     2.5.0
nbclassic                 0.5.1
nbclient                  0.7.2
nbconvert                 7.2.9
nbdime                    3.1.1
nbformat                  5.7.3
nest-asyncio              1.5.6
notebook                  6.5.2
notebook_shim             0.2.2
numpy                     1.24.1
openpyxl                  3.0.10
packaging                 23.0
pandarallel               1.6.4
pandas                    1.5.2
pandocfilters             1.5.0
parso                     0.8.3
patsy                     0.5.3
pexpect                   4.8.0
pickleshare               0.7.5
Pillow                    9.4.0
pip                       22.3.1
platformdirs              3.0.0
plotly                    5.11.0
ppscore                   1.3.0
prometheus-client         0.16.0
prompt-toolkit            3.0.36
psutil                    5.9.4
ptyprocess                0.7.0
pure-eval                 0.2.2
pyarrow                   10.0.1
pycodestyle               2.10.0
pycparser                 2.21
pyflakes                  3.0.1
pyflowchart               0.2.3
Pygments                  2.14.0
pylint                    2.15.10
pyparsing                 3.0.9
pyproj                    3.4.1
pyrsistent                0.19.3
python-dateutil           2.8.2
python-dotenv             0.21.0
python-highcharts         0.4.2
python-json-logger        2.0.4
python-slugify            8.0.1
pytz                      2022.7.1
PyYAML                    6.0
pyzmq                     25.0.0
requests                  2.28.2
rfc3339-validator         0.1.4
rfc3986-validator         0.1.1
s3fs                      0.4.2
s3transfer                0.6.0
scikit-learn              1.2.1
scipy                     1.10.0
Send2Trash                1.8.0
setuptools                65.6.3
shapely                   2.0.1
simple-term-menu          1.5.2
six                       1.16.0
smmap                     5.0.0
sniffio                   1.3.0
soupsieve                 2.4
SQLAlchemy                1.4.46
sqlalchemy-vertica-python 0.5.10
stack-data                0.6.2
statsmodels               0.13.5
tenacity                  8.1.0
terminado                 0.17.1
text-unidecode            1.3
threadpoolctl             3.1.0
tinycss2                  1.2.1
toml                      0.10.2
tomli                     2.0.1
tomlkit                   0.11.6
tornado                   6.2
tqdm                      4.64.1
traitlets                 5.9.0
typing_extensions         4.4.0
uri-template              1.2.0
urllib3                   1.26.14
vertica-python            1.2.0
verticapy                 0.12.0
wcwidth                   0.2.6
webcolors                 1.12
webencodings              0.5.1
websocket-client          1.5.0
wheel                     0.37.1
widgetsnbextension        3.6.1
wrapt                     1.14.1
xlrd                      2.0.1
xlwt                      1.3.0
y-py                      0.5.5
ypy-websocket             0.8.2
zipp                      3.12.0
```

* If bamboolib is used with JupyterLab: 
```
        jupyterlab-plotly v5.11.0 enabled OK
        jupyterlab_pygments v0.2.2 enabled OK (python, jupyterlab_pygments)
        nbdime-jupyterlab v2.1.1 enabled OK
        @jupyterlab/git v0.41.0 enabled OK (python, jupyterlab-git)
        @jupyter-widgets/jupyterlab-manager v3.1.1 enabled OK (python, jupyterlab_widgets)
```
### Description of Issue

My DataFrame has Columns,    A B C D E 

Create New Column Formula :  newCol = A * 10

Because the replacement happens ""naive"", I'm left with pieces of the template in the error:
```
df['newCol'] = BAM_PLABAM_PLACdf['E']HOLBAM_PLACdf['E']HOLDdf['E']R_3_df['E']R_2_df['E']HOLBAM_PLACdf['E']HOLDdf['E']R_3_df['E']R_1_AM_PLABAM_PLACdf['E']HOLBAM_PLACdf['E']HOLDdf['E']R_3_df['E']R_2_df['E']HOLBAM_PLACdf['E']HOLDdf['E']R_3_df['E']R_0_ * 10
```

### Reproduction Steps

1. Have a table where Column names are single capital letters, A B D E .. 
2. Create any ""New Column Formula"" using those columns.


### What steps have you taken to resolve this already?


### Anything else?

...

"
tkrabel/bamboolib,"Installation fails: https://github.com/tkrabel/bamboolib/issues/44
Description: ### Environment

* Operating System: Ubuntu 22.04.1 LTS
* Python Version: Python 3.7.13
* How did you install bamboolib: Conda

### Description of Issue

* What did you expect to happen?  I tried to install bamboolib from a fresh ubntu 22.04.01 installation using the conda instructions provided
* What happened instead? The installation failed

### Reproduction Steps

1.  Install a fresh installation of the latest Ubuntu and update
2. Install Anaconda and update
3. Follow the instruction for Bamboolib installation via conda found here : https://docs.bamboolib.8080labs.com/documentation/how-tos/installation-and-setup/install-bamboolib#installation-with-anaconda-virtual-environment
4. Note that the following step returns an error: python -m bamboolib install_nbextensions

The error is as follows:  

Trying to install bamboolib nbextension...
Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/envs/bamboolib/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/ubuntu/anaconda3/envs/bamboolib/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/ubuntu/.local/lib/python3.7/site-packages/bamboolib/__main__.py"", line 38, in <module>
    install_nbextensions()
  File ""/home/ubuntu/.local/lib/python3.7/site-packages/bamboolib/setup/_extensions.py"", line 42, in install_nbextensions
    nbextensions.install_nbextension_python(extension, user=True)
  File ""/home/ubuntu/anaconda3/envs/bamboolib/lib/python3.7/site-packages/notebook/nbextensions.py"", line 203, in install_nbextension_python
    m, nbexts = _get_nbextension_metadata(module)
  File ""/home/ubuntu/anaconda3/envs/bamboolib/lib/python3.7/site-packages/notebook/nbextensions.py"", line 1107, in _get_nbextension_metadata
    m = import_item(module)
  File ""/home/ubuntu/anaconda3/envs/bamboolib/lib/python3.7/site-packages/traitlets/utils/importstring.py"", line 38, in import_item
    return __import__(parts[0])
ModuleNotFoundError: No module named 'plotlywidget'





...

### What steps have you taken to resolve this already?

...

### Anything else?

...

"
tkrabel/bamboolib,"installation fails on building wheel for cryptography: https://github.com/tkrabel/bamboolib/issues/42
Description: ### Environment

* Operating System: windows 11
* Python Version: 3.9.12
* How did you install bamboolib: pip inside conda base environment (i have also tried with standard python install on windows which failed the same way)
* Python packages: fresh install of python with only bamboolib installed
* If bamboolib is used with JupyterLab: JupyterLab extensions: see above
* If bamboolib is used with Jupyter Notebook: Notebook extension: `jupyter nbextension list`

### Description of Issue

* Usually it installs fine and was previously working on same machine with windows 11 and python 3.9. Re-installed windows then fresh python and it fails every time. 
 When installing bamboolib it fails first time stating MS visual c++ required. This is downloaded through build tools from link suggested in python. c++ version is latest community edition with recommended install options (i have also tried with earlier versions of c++ (2015,17 and 19 - same issue). 



### What steps have you taken to resolve this already?
One error says in error files (below) is to do with openssl - after adding openssl ""include"" and ""lib"" files to respective python folders, this error goes away but is left with the ""C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.32.31326\\bin\\HostX86\\x64\\cl.exe' failed with exit code 2"" error. I have researched extensively with no solution. I have tried updating pip, installing wheel, forcing version of cryptography to 2.9.2 - no solution. Now when trying to install cryptography using pip it fails with an error too. 
...

### Anything else?

error code:

 Building wheel for cryptography (PEP 517) ... error
  ERROR: Command errored out with exit status 1:
   command: 'C:\Users\Nate\anaconda3\python.exe' 'C:\Users\Nate\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py' build_wheel 'C:\Users\Nate\AppData\Local\Temp\tmpv9lm2if3'
       cwd: C:\Users\Nate\AppData\Local\Temp\pip-install-9esg215_\cryptography_e5268f4a0055458e81a4a24c916c972a
  Complete output (142 lines):
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build\lib.win-amd64-cpython-39
  creating build\lib.win-amd64-cpython-39\cryptography
  copying src\cryptography\exceptions.py -> build\lib.win-amd64-cpython-39\cryptography
  copying src\cryptography\fernet.py -> build\lib.win-amd64-cpython-39\cryptography
  copying src\cryptography\utils.py -> build\lib.win-amd64-cpython-39\cryptography
  copying src\cryptography\__about__.py -> build\lib.win-amd64-cpython-39\cryptography
  copying src\cryptography\__init__.py -> build\lib.win-amd64-cpython-39\cryptography
  creating build\lib.win-amd64-cpython-39\cryptography\hazmat
  copying src\cryptography\hazmat\_der.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat
  copying src\cryptography\hazmat\_oid.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat
  copying src\cryptography\hazmat\__init__.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat
  creating build\lib.win-amd64-cpython-39\cryptography\x509
  copying src\cryptography\x509\base.py -> build\lib.win-amd64-cpython-39\cryptography\x509
  copying src\cryptography\x509\certificate_transparency.py -> build\lib.win-amd64-cpython-39\cryptography\x509
  copying src\cryptography\x509\extensions.py -> build\lib.win-amd64-cpython-39\cryptography\x509
  copying src\cryptography\x509\general_name.py -> build\lib.win-amd64-cpython-39\cryptography\x509
  copying src\cryptography\x509\name.py -> build\lib.win-amd64-cpython-39\cryptography\x509
  copying src\cryptography\x509\ocsp.py -> build\lib.win-amd64-cpython-39\cryptography\x509
  copying src\cryptography\x509\oid.py -> build\lib.win-amd64-cpython-39\cryptography\x509
  copying src\cryptography\x509\__init__.py -> build\lib.win-amd64-cpython-39\cryptography\x509
  creating build\lib.win-amd64-cpython-39\cryptography\hazmat\backends
  copying src\cryptography\hazmat\backends\interfaces.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends
  copying src\cryptography\hazmat\backends\__init__.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends
  creating build\lib.win-amd64-cpython-39\cryptography\hazmat\bindings
  copying src\cryptography\hazmat\bindings\__init__.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\bindings
  creating build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives
  copying src\cryptography\hazmat\primitives\cmac.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives
  copying src\cryptography\hazmat\primitives\constant_time.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives
  copying src\cryptography\hazmat\primitives\hashes.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives
  copying src\cryptography\hazmat\primitives\hmac.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives
  copying src\cryptography\hazmat\primitives\keywrap.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives  copying src\cryptography\hazmat\primitives\padding.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives  copying src\cryptography\hazmat\primitives\poly1305.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives
  copying src\cryptography\hazmat\primitives\__init__.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives
  creating build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\aead.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\backend.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\ciphers.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\cmac.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\decode_asn1.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\dh.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\dsa.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\ec.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\ed25519.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\ed448.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\encode_asn1.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\hashes.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\hmac.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\ocsp.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\poly1305.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\rsa.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\utils.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\x25519.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\x448.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\x509.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  copying src\cryptography\hazmat\backends\openssl\__init__.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\backends\openssl
  creating build\lib.win-amd64-cpython-39\cryptography\hazmat\bindings\openssl
  copying src\cryptography\hazmat\bindings\openssl\binding.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\bindings\openssl
  copying src\cryptography\hazmat\bindings\openssl\_conditional.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\bindings\openssl
  copying src\cryptography\hazmat\bindings\openssl\__init__.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\bindings\openssl
  creating build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\asymmetric
  copying src\cryptography\hazmat\primitives\asymmetric\dh.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\asymmetric
  copying src\cryptography\hazmat\primitives\asymmetric\dsa.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\asymmetric
  copying src\cryptography\hazmat\primitives\asymmetric\ec.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\asymmetric
  copying src\cryptography\hazmat\primitives\asymmetric\ed25519.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\asymmetric
  copying src\cryptography\hazmat\primitives\asymmetric\ed448.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\asymmetric
  copying src\cryptography\hazmat\primitives\asymmetric\padding.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\asymmetric
  copying src\cryptography\hazmat\primitives\asymmetric\rsa.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\asymmetric
  copying src\cryptography\hazmat\primitives\asymmetric\utils.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\asymmetric
  copying src\cryptography\hazmat\primitives\asymmetric\x25519.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\asymmetric
  copying src\cryptography\hazmat\primitives\asymmetric\x448.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\asymmetric
  copying src\cryptography\hazmat\primitives\asymmetric\__init__.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\asymmetric
  creating build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\ciphers
  copying src\cryptography\hazmat\primitives\ciphers\aead.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\ciphers
  copying src\cryptography\hazmat\primitives\ciphers\algorithms.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\ciphers
  copying src\cryptography\hazmat\primitives\ciphers\base.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\ciphers
  copying src\cryptography\hazmat\primitives\ciphers\modes.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\ciphers
  copying src\cryptography\hazmat\primitives\ciphers\__init__.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\ciphers
  creating build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\kdf
  copying src\cryptography\hazmat\primitives\kdf\concatkdf.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\kdf
  copying src\cryptography\hazmat\primitives\kdf\hkdf.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\kdf
  copying src\cryptography\hazmat\primitives\kdf\kbkdf.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\kdf
  copying src\cryptography\hazmat\primitives\kdf\pbkdf2.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\kdf
  copying src\cryptography\hazmat\primitives\kdf\scrypt.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\kdf
  copying src\cryptography\hazmat\primitives\kdf\x963kdf.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\kdf
  copying src\cryptography\hazmat\primitives\kdf\__init__.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\kdf
  creating build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\serialization
  copying src\cryptography\hazmat\primitives\serialization\base.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\serialization
  copying src\cryptography\hazmat\primitives\serialization\pkcs12.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\serialization
  copying src\cryptography\hazmat\primitives\serialization\ssh.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\serialization
  copying src\cryptography\hazmat\primitives\serialization\__init__.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\serialization
  creating build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\twofactor
  copying src\cryptography\hazmat\primitives\twofactor\hotp.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\twofactor
  copying src\cryptography\hazmat\primitives\twofactor\totp.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\twofactor
  copying src\cryptography\hazmat\primitives\twofactor\utils.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\twofactor
  copying src\cryptography\hazmat\primitives\twofactor\__init__.py -> build\lib.win-amd64-cpython-39\cryptography\hazmat\primitives\twofactor
  running egg_info
  writing src\cryptography.egg-info\PKG-INFO
  writing dependency_links to src\cryptography.egg-info\dependency_links.txt
  writing requirements to src\cryptography.egg-info\requires.txt
  writing top-level names to src\cryptography.egg-info\top_level.txt
  reading manifest file 'src\cryptography.egg-info\SOURCES.txt'
  reading manifest template 'MANIFEST.in'
  no previously-included directories found matching 'docs\_build'
  warning: no previously-included files found matching 'vectors'
  warning: no previously-included files matching '*' found under directory 'vectors'
  warning: no previously-included files found matching 'azure-pipelines.yml'
  warning: no previously-included files found matching '.azure-pipelines'
  warning: no previously-included files found matching '.travis.yml'
  warning: no previously-included files found matching '.travis'
  warning: no previously-included files matching '*' found under directory '.azure-pipelines'
  warning: no previously-included files matching '*' found under directory '.travis'
  warning: no previously-included files found matching 'release.py'
  warning: no previously-included files found matching '.coveragerc'
  warning: no previously-included files found matching 'codecov.yml'
  warning: no previously-included files found matching 'dev-requirements.txt'
  warning: no previously-included files found matching 'rtd-requirements.txt'
  warning: no previously-included files found matching 'tox.ini'
  adding license file 'LICENSE'
  adding license file 'LICENSE.APACHE'
  adding license file 'LICENSE.BSD'
  adding license file 'LICENSE.PSF'
  adding license file 'AUTHORS.rst'
  writing manifest file 'src\cryptography.egg-info\SOURCES.txt'
  running build_ext
  generating cffi module 'build\\temp.win-amd64-cpython-39\\Release\\_padding.c'
  creating build\temp.win-amd64-cpython-39
  creating build\temp.win-amd64-cpython-39\Release
  generating cffi module 'build\\temp.win-amd64-cpython-39\\Release\\_constant_time.c'
  generating cffi module 'build\\temp.win-amd64-cpython-39\\Release\\_openssl.c'
  building '_openssl' extension
  creating build\temp.win-amd64-cpython-39\Release\build
  creating build\temp.win-amd64-cpython-39\Release\build\temp.win-amd64-cpython-39
  creating build\temp.win-amd64-cpython-39\Release\build\temp.win-amd64-cpython-39\Release
  ""C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.32.31326\bin\HostX86\x64\cl.exe"" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\Users\Nate\anaconda3\include -IC:\Users\Nate\anaconda3\Include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.32.31326\include"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\um"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\cppwinrt"" /Tcbuild\temp.win-amd64-cpython-39\Release\_openssl.c /Fobuild\temp.win-amd64-cpython-39\Release\build\temp.win-amd64-cpython-39\Release\_openssl.obj
  _openssl.c
  build\temp.win-amd64-cpython-39\Release\_openssl.c(575): fatal error C1083: Cannot open include file: 'openssl/opensslv.h': No such file or directory
  error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.32.31326\\bin\\HostX86\\x64\\cl.exe' failed with exit code 2
  ----------------------------------------
  ERROR: Failed building wheel for cryptography
Failed to build cryptography
ERROR: Could not build wheels for cryptography which use PEP 517 and cannot be installed directly
...

"
tkrabel/bamboolib,"cryptography super old version: https://github.com/tkrabel/bamboolib/issues/41
Description: ### Environment

* Operating System: Ubuntu + Docker
* Python Version: 3.9.10
* How did you install bamboolib: pip

```
(base) jovyan@66c9eae4b466:~$ jupyter labextension list
JupyterLab v2.3.2
Known labextensions:
   app dir: /opt/conda/share/jupyter/lab
        @jupyter-widgets/jupyterlab-manager v2.0.0  enabled  OK
        bamboolib v1.30.0  enabled  OK
        ipyslickgrid v0.0.3  enabled  OK
        jupyterlab-plotly v4.14.3  enabled  OK
        luxwidget v0.1.4  enabled  OK
        mitosheet v0.1.340  enabled  OK
        plotlywidget v4.14.3  enabled  OK`
```

```
(base) jovyan@66c9eae4b466:~$ pip list
Package                       Version
----------------------------- ---------
alembic                       1.7.6
altair                        4.2.0
analytics-python              1.2.9
ansiwrap                      0.8.4
anyio                         3.5.0
argon2-cffi                   21.3.0
argon2-cffi-bindings          21.2.0
asttokens                     2.0.5
async-generator               1.10
attrs                         21.4.0
autopep8                      1.6.0
Babel                         2.9.1
backcall                      0.2.0
backoff                       1.11.1
backports.functools-lru-cache 1.6.4
bamboolib                     1.30.0
beautifulsoup4                4.10.0
bleach                        4.1.0
blinker                       1.4
bokeh                         2.4.2
boto3                         1.21.24
botocore                      1.24.24
Bottleneck                    1.3.4
brotlipy                      0.7.0
cached-property               1.5.2
certifi                       2021.10.8
certipy                       0.1.3
cffi                          1.15.0
charset-normalizer            2.0.12
click                         8.0.4
cloudpickle                   2.0.0
colorama                      0.4.4
conda                         4.11.0
conda-package-handling        1.7.3
cryptography                  2.9.2
cycler                        0.11.0
Cython                        0.29.28
cytoolz                       0.11.2
dask                          2022.2.1
debugpy                       1.5.1
decorator                     5.1.1
defusedxml                    0.7.1
dill                          0.3.4
distributed                   2022.2.1
distro                        1.7.0
entrypoints                   0.4
executing                     0.8.3
flit_core                     3.7.1
fonttools                     4.30.0
fsspec                        2022.2.0
gmpy2                         2.1.2
greenlet                      1.1.2
h5py                          3.6.0
HeapDict                      1.0.1
humanize                      4.0.0
idna                          3.3
imagecodecs                   2022.2.22
imageio                       2.16.1
importlib-metadata            4.11.3
importlib-resources           5.4.0
ipdb                          0.13.9
ipykernel                     6.9.2
ipympl                        0.8.8
ipyslickgrid                  0.0.3
ipython                       7.32.0
ipython-genutils              0.2.0
ipywidgets                    7.6.5
iso3166                       2.0.2
jedi                          0.18.1
Jinja2                        3.0.3
jmespath                      1.0.0
joblib                        1.1.0
json5                         0.9.5
jsonschema                    4.4.0
jupyter                       1.0.0
jupyter-client                7.1.2
jupyter-console               6.4.3
jupyter-core                  4.9.2
jupyter-server                1.15.1
jupyter-telemetry             0.1.0
jupyterhub                    0.9.2
jupyterlab                    2.3.2
jupyterlab-pygments           0.1.2
jupyterlab-server             1.2.0
jupyterlab-widgets            1.0.2
jupytext                      1.13.7
kiwisolver                    1.3.2
libmambapy                    0.22.1
llvmlite                      0.38.0
locket                        0.2.0
lux-api                       0.5.1
lux-widget                    0.1.4
Mako                          1.2.0
mamba                         0.22.1
markdown-it-py                1.1.0
MarkupSafe                    2.1.0
matplotlib                    3.5.1
matplotlib-inline             0.1.3
mdit-py-plugins               0.3.0
mistune                       0.8.4
mitosheet                     0.1.340
monotonic                     1.6
mpmath                        1.2.1
msgpack                       1.0.3
munkres                       1.1.4
nbclassic                     0.3.6
nbclient                      0.5.13
nbconvert                     6.4.4
nbformat                      5.2.0
nest-asyncio                  1.5.4
networkx                      2.7.1
notebook                      6.4.9
notebook-shim                 0.1.0
numba                         0.55.1
numexpr                       2.8.0
numpy                         1.21.5
oauthlib                      3.2.0
packaging                     21.3
pamela                        1.0.0
pandas                        1.4.1
pandocfilters                 1.5.0
papermill                     2.3.4
parso                         0.8.3
partd                         1.2.0
patsy                         0.5.2
pexpect                       4.8.0
pickleshare                   0.7.5
Pillow                        9.0.1
pip                           22.0.4
ploomber                      0.16
ploomber-scaffold             0.3.1
plotly                        4.14.3
posthog                       1.4.5
ppscore                       1.2.0
prometheus-client             0.13.1
prompt-toolkit                3.0.27
protobuf                      3.19.4
psutil                        5.9.0
psycopg2                      2.9.3
ptyprocess                    0.7.0
pure-eval                     0.2.2
pycodestyle                   2.8.0
pycosat                       0.6.3
pycparser                     2.21
pycurl                        7.45.1
pydantic                      1.9.0
pyflakes                      2.4.0
Pygments                      2.11.2
pygraphviz                    1.9
PyJWT                         2.3.0
pyOpenSSL                     22.0.0
pyparsing                     3.0.7
pyrsistent                    0.18.1
PySocks                       1.7.1
python-dateutil               2.8.2
python-json-logger            2.0.1
python-oauth2                 1.1.1
pytz                          2021.3
PyWavelets                    1.3.0
PyYAML                        6.0
pyzmq                         22.3.0
qtconsole                     5.2.2
QtPy                          2.0.1
requests                      2.27.1
retrying                      1.3.3
ruamel.yaml                   0.17.21
ruamel.yaml.clib              0.2.6
ruamel-yaml-conda             0.15.80
s3transfer                    0.5.2
scikit-image                  0.19.2
scikit-learn                  0.24.2
scipy                         1.8.0
seaborn                       0.10.1
Send2Trash                    1.8.0
setuptools                    59.8.0
sh                            1.14.2
six                           1.16.0
sniffio                       1.2.0
sortedcontainers              2.4.0
soupsieve                     2.3.1
SQLAlchemy                    1.4.32
sqlparse                      0.4.2
stack-data                    0.2.0
statsmodels                   0.13.2
sympy                         1.10
tables                        3.7.0
tabulate                      0.8.9
tblib                         1.7.0
tenacity                      8.0.1
terminado                     0.13.3
testpath                      0.6.0
textwrap3                     0.9.2
threadpoolctl                 3.1.0
tifffile                      2022.2.9
toml                          0.10.2
toolz                         0.11.2
tornado                       6.1
tqdm                          4.63.0
traitlets                     5.1.1
typing_extensions             4.1.1
unicodedata2                  14.0.0
urllib3                       1.26.8
wcwidth                       0.2.5
webencodings                  0.5.1
websocket-client              1.3.1
wheel                         0.37.1
widgetsnbextension            3.5.2
xlrd                          2.0.1
zict                          2.1.0
zipp                          3.7.0
```

### Description of Issue

```
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
pyopenssl 22.0.0 requires cryptography>=35.0, but you have cryptography 2.9.2 which is incompatible.
```

### Reproduction Steps

Triggered during pip install.

### What steps have you taken to resolve this already?

None
### Anything else?

I believe you should upgrade the cryptography 2.9.2 requirement because is far too old: https://pypi.org/project/cryptography/



"
tkrabel/bamboolib,"Permission denied: https://github.com/tkrabel/bamboolib/issues/40
Description: ### Environment

* Operating System: Ubuntu + Docker
* Python Version: 3.9.10
* How did you install bamboolib: pip

```
(base) jovyan@66c9eae4b466:~$ jupyter labextension list
JupyterLab v2.3.2
Known labextensions:
   app dir: /opt/conda/share/jupyter/lab
        @jupyter-widgets/jupyterlab-manager v2.0.0  enabled  OK
        bamboolib v1.30.0  enabled  OK
        ipyslickgrid v0.0.3  enabled  OK
        jupyterlab-plotly v4.14.3  enabled  OK
        luxwidget v0.1.4  enabled  OK
        mitosheet v0.1.340  enabled  OK
        plotlywidget v4.14.3  enabled  OK`
```

```
(base) jovyan@66c9eae4b466:~$ pip list
Package                       Version
----------------------------- ---------
alembic                       1.7.6
altair                        4.2.0
analytics-python              1.2.9
ansiwrap                      0.8.4
anyio                         3.5.0
argon2-cffi                   21.3.0
argon2-cffi-bindings          21.2.0
asttokens                     2.0.5
async-generator               1.10
attrs                         21.4.0
autopep8                      1.6.0
Babel                         2.9.1
backcall                      0.2.0
backoff                       1.11.1
backports.functools-lru-cache 1.6.4
bamboolib                     1.30.0
beautifulsoup4                4.10.0
bleach                        4.1.0
blinker                       1.4
bokeh                         2.4.2
boto3                         1.21.24
botocore                      1.24.24
Bottleneck                    1.3.4
brotlipy                      0.7.0
cached-property               1.5.2
certifi                       2021.10.8
certipy                       0.1.3
cffi                          1.15.0
charset-normalizer            2.0.12
click                         8.0.4
cloudpickle                   2.0.0
colorama                      0.4.4
conda                         4.11.0
conda-package-handling        1.7.3
cryptography                  2.9.2
cycler                        0.11.0
Cython                        0.29.28
cytoolz                       0.11.2
dask                          2022.2.1
debugpy                       1.5.1
decorator                     5.1.1
defusedxml                    0.7.1
dill                          0.3.4
distributed                   2022.2.1
distro                        1.7.0
entrypoints                   0.4
executing                     0.8.3
flit_core                     3.7.1
fonttools                     4.30.0
fsspec                        2022.2.0
gmpy2                         2.1.2
greenlet                      1.1.2
h5py                          3.6.0
HeapDict                      1.0.1
humanize                      4.0.0
idna                          3.3
imagecodecs                   2022.2.22
imageio                       2.16.1
importlib-metadata            4.11.3
importlib-resources           5.4.0
ipdb                          0.13.9
ipykernel                     6.9.2
ipympl                        0.8.8
ipyslickgrid                  0.0.3
ipython                       7.32.0
ipython-genutils              0.2.0
ipywidgets                    7.6.5
iso3166                       2.0.2
jedi                          0.18.1
Jinja2                        3.0.3
jmespath                      1.0.0
joblib                        1.1.0
json5                         0.9.5
jsonschema                    4.4.0
jupyter                       1.0.0
jupyter-client                7.1.2
jupyter-console               6.4.3
jupyter-core                  4.9.2
jupyter-server                1.15.1
jupyter-telemetry             0.1.0
jupyterhub                    0.9.2
jupyterlab                    2.3.2
jupyterlab-pygments           0.1.2
jupyterlab-server             1.2.0
jupyterlab-widgets            1.0.2
jupytext                      1.13.7
kiwisolver                    1.3.2
libmambapy                    0.22.1
llvmlite                      0.38.0
locket                        0.2.0
lux-api                       0.5.1
lux-widget                    0.1.4
Mako                          1.2.0
mamba                         0.22.1
markdown-it-py                1.1.0
MarkupSafe                    2.1.0
matplotlib                    3.5.1
matplotlib-inline             0.1.3
mdit-py-plugins               0.3.0
mistune                       0.8.4
mitosheet                     0.1.340
monotonic                     1.6
mpmath                        1.2.1
msgpack                       1.0.3
munkres                       1.1.4
nbclassic                     0.3.6
nbclient                      0.5.13
nbconvert                     6.4.4
nbformat                      5.2.0
nest-asyncio                  1.5.4
networkx                      2.7.1
notebook                      6.4.9
notebook-shim                 0.1.0
numba                         0.55.1
numexpr                       2.8.0
numpy                         1.21.5
oauthlib                      3.2.0
packaging                     21.3
pamela                        1.0.0
pandas                        1.4.1
pandocfilters                 1.5.0
papermill                     2.3.4
parso                         0.8.3
partd                         1.2.0
patsy                         0.5.2
pexpect                       4.8.0
pickleshare                   0.7.5
Pillow                        9.0.1
pip                           22.0.4
ploomber                      0.16
ploomber-scaffold             0.3.1
plotly                        4.14.3
posthog                       1.4.5
ppscore                       1.2.0
prometheus-client             0.13.1
prompt-toolkit                3.0.27
protobuf                      3.19.4
psutil                        5.9.0
psycopg2                      2.9.3
ptyprocess                    0.7.0
pure-eval                     0.2.2
pycodestyle                   2.8.0
pycosat                       0.6.3
pycparser                     2.21
pycurl                        7.45.1
pydantic                      1.9.0
pyflakes                      2.4.0
Pygments                      2.11.2
pygraphviz                    1.9
PyJWT                         2.3.0
pyOpenSSL                     22.0.0
pyparsing                     3.0.7
pyrsistent                    0.18.1
PySocks                       1.7.1
python-dateutil               2.8.2
python-json-logger            2.0.1
python-oauth2                 1.1.1
pytz                          2021.3
PyWavelets                    1.3.0
PyYAML                        6.0
pyzmq                         22.3.0
qtconsole                     5.2.2
QtPy                          2.0.1
requests                      2.27.1
retrying                      1.3.3
ruamel.yaml                   0.17.21
ruamel.yaml.clib              0.2.6
ruamel-yaml-conda             0.15.80
s3transfer                    0.5.2
scikit-image                  0.19.2
scikit-learn                  0.24.2
scipy                         1.8.0
seaborn                       0.10.1
Send2Trash                    1.8.0
setuptools                    59.8.0
sh                            1.14.2
six                           1.16.0
sniffio                       1.2.0
sortedcontainers              2.4.0
soupsieve                     2.3.1
SQLAlchemy                    1.4.32
sqlparse                      0.4.2
stack-data                    0.2.0
statsmodels                   0.13.2
sympy                         1.10
tables                        3.7.0
tabulate                      0.8.9
tblib                         1.7.0
tenacity                      8.0.1
terminado                     0.13.3
testpath                      0.6.0
textwrap3                     0.9.2
threadpoolctl                 3.1.0
tifffile                      2022.2.9
toml                          0.10.2
toolz                         0.11.2
tornado                       6.1
tqdm                          4.63.0
traitlets                     5.1.1
typing_extensions             4.1.1
unicodedata2                  14.0.0
urllib3                       1.26.8
wcwidth                       0.2.5
webencodings                  0.5.1
websocket-client              1.3.1
wheel                         0.37.1
widgetsnbextension            3.5.2
xlrd                          2.0.1
zict                          2.1.0
zipp                          3.7.0
```

### Description of Issue

`PermissionError: [Errno 13] Permission denied: '/home/jovyan/.bamboolib/config.toml'`

### Reproduction Steps

Run example code:

```
import bamboolib as bam
import pandas as pd
df = pd.read_csv(bam.titanic_csv)
df

```

### What steps have you taken to resolve this already?

None
### Anything else?

None

"
tkrabel/bamboolib,"Not able to install: https://github.com/tkrabel/bamboolib/issues/39
Description: I am using Windows 8.1 pro and latest Visual C++ installed. I've tried installing it in Anaconda as well as Kaggle but this same error keeps
popping up

""Collecting bamboolib
  Downloading bamboolib-1.30.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (8.2 MB)
     |████████████████████████████████| 8.2 MB 13.1 MB/s            
Collecting plotly<5.0.0,>=4.9.0
  Downloading plotly-4.14.3-py2.py3-none-any.whl (13.2 MB)
     |████████████████████████████████| 13.2 MB 63.1 MB/s            
Requirement already satisfied: scikit-learn<2.0.0,>=0.20.2 in /opt/conda/lib/python3.7/site-packages (from bamboolib) (0.23.2)
Collecting xlrd>=1.0.0
  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)
     |████████████████████████████████| 96 kB 2.8 MB/s             
Requirement already satisfied: psutil<6,>=5.4.2 in /opt/conda/lib/python3.7/site-packages (from bamboolib) (5.8.0)
Requirement already satisfied: pandas<2.0.0,>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from bamboolib) (1.3.5)
Requirement already satisfied: packaging>=19.2 in /opt/conda/lib/python3.7/site-packages (from bamboolib) (21.3)
Requirement already satisfied: jedi<1.0.0 in /opt/conda/lib/python3.7/site-packages (from bamboolib) (0.18.1)
Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from bamboolib) (2.10.0)
Requirement already satisfied: toml>=0.10.0 in /opt/conda/lib/python3.7/site-packages (from bamboolib) (0.10.2)
Collecting seaborn<0.11,>=0.10
  Downloading seaborn-0.10.1-py3-none-any.whl (215 kB)
     |████████████████████████████████| 215 kB 25.6 MB/s            
Requirement already satisfied: statsmodels<1.0.0 in /opt/conda/lib/python3.7/site-packages (from bamboolib) (0.13.1)
Collecting ipyslickgrid==0.0.3
  Downloading ipyslickgrid-0.0.3.tar.gz (51.4 MB)
     |████████████████████████████████| 51.4 MB 226 kB/s             
  Preparing metadata (setup.py) ... done
Collecting cryptography<3.0.0,>=2.6.1
  Downloading cryptography-2.9.2-cp35-abi3-manylinux2010_x86_64.whl (2.7 MB)
     |████████████████████████████████| 2.7 MB 26.2 MB/s            
Requirement already satisfied: ipywidgets<8.0.0,>=7.6.0 in /opt/conda/lib/python3.7/site-packages (from bamboolib) (7.6.5)
Requirement already satisfied: attrs>=20.3.0 in /opt/conda/lib/python3.7/site-packages (from bamboolib) (21.2.0)
Collecting ppscore<2.0.0,>=1.2.0
  Downloading ppscore-1.2.0.tar.gz (47 kB)
     |████████████████████████████████| 47 kB 2.8 MB/s             
  Preparing metadata (setup.py) ... done
Collecting analytics-python==1.2.9
  Downloading analytics_python-1.2.9-py2.py3-none-any.whl (13 kB)
Requirement already satisfied: requests<3.0,>=2.7 in /opt/conda/lib/python3.7/site-packages (from analytics-python==1.2.9->bamboolib) (2.26.0)
Requirement already satisfied: python-dateutil>2.1 in /opt/conda/lib/python3.7/site-packages (from analytics-python==1.2.9->bamboolib) (2.8.2)
Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from analytics-python==1.2.9->bamboolib) (1.16.0)
Requirement already satisfied: notebook>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from ipyslickgrid==0.0.3->bamboolib) (6.4.6)
Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography<3.0.0,>=2.6.1->bamboolib) (1.15.0)
Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8.0.0,>=7.6.0->bamboolib) (5.1.3)
Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8.0.0,>=7.6.0->bamboolib) (0.2.0)
Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8.0.0,>=7.6.0->bamboolib) (5.1.1)
Requirement already satisfied: widgetsnbextension~=3.5.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8.0.0,>=7.6.0->bamboolib) (3.5.2)
Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8.0.0,>=7.6.0->bamboolib) (1.0.2)
Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8.0.0,>=7.6.0->bamboolib) (7.30.1)
Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8.0.0,>=7.6.0->bamboolib) (6.6.0)
Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi<1.0.0->bamboolib) (0.8.3)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=19.2->bamboolib) (3.0.6)
Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas<2.0.0,>=1.1.0->bamboolib) (2021.3)
Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas<2.0.0,>=1.1.0->bamboolib) (1.20.3)
Requirement already satisfied: retrying>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from plotly<5.0.0,>=4.9.0->bamboolib) (1.3.3)
Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn<2.0.0,>=0.20.2->bamboolib) (3.0.0)
Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn<2.0.0,>=0.20.2->bamboolib) (1.1.0)
Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn<2.0.0,>=0.20.2->bamboolib) (1.7.3)
Requirement already satisfied: matplotlib>=2.1.2 in /opt/conda/lib/python3.7/site-packages (from seaborn<0.11,>=0.10->bamboolib) (3.5.1)
Requirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from statsmodels<1.0.0->bamboolib) (0.5.2)
Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography<3.0.0,>=2.6.1->bamboolib) (2.21)
Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.6.0->bamboolib) (1.5.1)
Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.6.0->bamboolib) (0.1.3)
Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.6.0->bamboolib) (6.1)
Requirement already satisfied: importlib-metadata<5 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.6.0->bamboolib) (4.10.1)
Requirement already satisfied: argcomplete>=1.12.3 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.6.0->bamboolib) (1.12.3)
Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.6.0->bamboolib) (7.1.0)
Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (5.1.0)
Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (3.0.24)
Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (59.5.0)
Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (0.7.5)
Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (0.2.0)
Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (4.8.0)
Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn<0.11,>=0.10->bamboolib) (8.2.0)
Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn<0.11,>=0.10->bamboolib) (0.11.0)
Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn<0.11,>=0.10->bamboolib) (4.28.4)
Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn<0.11,>=0.10->bamboolib) (1.3.2)
Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (4.3.1)
Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (4.9.1)
Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.7/site-packages (from notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (21.1.0)
Requirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (22.3.0)
Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (1.5.4)
Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (3.0.3)
Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (1.8.0)
Requirement already satisfied: nbconvert in /opt/conda/lib/python3.7/site-packages (from notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (6.3.0)
Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.12.0)
Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.12.1)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.7->analytics-python==1.2.9->bamboolib) (2021.10.8)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.7->analytics-python==1.2.9->bamboolib) (3.1)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.7->analytics-python==1.2.9->bamboolib) (1.26.7)
Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.7->analytics-python==1.2.9->bamboolib) (2.0.9)
Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5->ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.6.0->bamboolib) (4.0.1)
Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5->ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.6.0->bamboolib) (3.6.0)
Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (5.4.0)
Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (0.18.0)
Requirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.6.0->bamboolib) (0.3)
Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (0.7.0)
Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (0.2.5)
Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (2.0.1)
Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.7.1)
Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (4.1.0)
Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.1.2)
Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.5.9)
Requirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.5.0)
Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.8.4)
Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (1.5.0)
Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.5.1)
Building wheels for collected packages: ipyslickgrid, ppscore
  Building wheel for ipyslickgrid (setup.py) ... done
  Created wheel for ipyslickgrid: filename=ipyslickgrid-0.0.3-py2.py3-none-any.whl size=1823285 sha256=ff5626c97e25d645e6119746097866fde3d3aebef00851fac2bfb9f29191c798
  Stored in directory: /root/.cache/pip/wheels/1a/a3/58/9c30cef9afebb4ca9f1e892e9245d676de3c2ae6e214e0781d
  Building wheel for ppscore (setup.py) ... done
  Created wheel for ppscore: filename=ppscore-1.2.0-py2.py3-none-any.whl size=13068 sha256=f1b8ebe3c13f99afcce336007f4df05176ac78b418e506b6adc3c32be8784a2f
  Stored in directory: /root/.cache/pip/wheels/d2/3c/58/2ff786414b21713edc6f4fdb54fdee89ac37bca5edd1f60634
Successfully built ipyslickgrid ppscore
Installing collected packages: xlrd, seaborn, ppscore, plotly, ipyslickgrid, cryptography, analytics-python, bamboolib
  Attempting uninstall: seaborn
    Found existing installation: seaborn 0.11.2
    Uninstalling seaborn-0.11.2:
      Successfully uninstalled seaborn-0.11.2
  Attempting uninstall: plotly
    Found existing installation: plotly 5.5.0
    Uninstalling plotly-5.5.0:
      Successfully uninstalled plotly-5.5.0
  Attempting uninstall: cryptography
    Found existing installation: cryptography 36.0.1
    Uninstalling cryptography-36.0.1:
      Successfully uninstalled cryptography-36.0.1
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
pyopenssl 21.0.0 requires cryptography>=3.3, but you have cryptography 2.9.2 which is incompatible.
Successfully installed analytics-python-1.2.9 bamboolib-1.30.0 cryptography-2.9.2 ipyslickgrid-0.0.3 plotly-4.14.3 ppscore-1.2.0 seaborn-0.10.1 xlrd-2.0.1
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: [https://pip.pypa.io/warnings/venv](https://pip.pypa.io/warnings/venv%3C/span%3E)
Note: you may need to restart the kernel to use updated packages.""

Kindly help

"
tkrabel/bamboolib,"Trying to use the communition edition but bamboolib is prompting for a license: https://github.com/tkrabel/bamboolib/issues/38
Description: ### Environment

* Operating System: Debian Buster
* Python Version: `$ python --version` 3.8.12
* How did you install bamboolib: (`pip`, `conda`, or `other (please explain)`) pip install bamboolib (as root in a docker image)
* Python packages: `$ pip list` or `$ conda list` (please include bamboolib) 
bamboolib            1.29.1
* If bamboolib is used with JupyterLab: JupyterLab extensions: `jupyter labextension list`
JupyterLab v3.2.7
/opt/conda/envs/quant/share/jupyter/labextensions
        @jupyter-widgets/jupyterlab-manager v3.0.1 enabled OK (python, jupyterlab_widgets)

Other labextensions (built into JupyterLab)
   app dir: /opt/conda/envs/quant/share/jupyter/lab
        bamboolib v1.29.0 enabled OK
        ipyslickgrid v0.0.3 enabled OK
        jupyterlab-plotly v4.14.3 enabled OK
        plotlywidget v4.14.3 enabled OK
* If bamboolib is used with Jupyter Notebook: Notebook extension: `jupyter nbextension list`

### Description of Issue

* What did you expect to happen?
Click show bamboolib UI and see the UI
* What happened instead?
Prompt for pro license

### Reproduction Steps

1. 
2.
3.
...

### What steps have you taken to resolve this already?

...

### Anything else?

...

"
tkrabel/bamboolib,"Can't get bamboolib to show up...: https://github.com/tkrabel/bamboolib/issues/34
Description: I installed everything from scratch and according to documentation. When running **bam.test_setup()** all test come back ok, except for the following part:

    bamboolib:
    Python library version is 1.26.0
    Needed version of Jupyter extension is 1.26.0
    Error displaying widget: model not found
    
    JupyterLab and Labextensions:
    JupyterLab v3.1.18
    Other labextensions (built into JupyterLab)
       app dir: C:\ProgramData\Anaconda3\share\jupyter\lab
            @8080labs/qgrid v1.1.1 enabled ok
            @jupyter-widgets/jupyterlab-manager v3.0.1 enabled ok
            @jupyterlab/git v0.30.1 enabled ok
            bamboolib v1.26.0 enabled ok
            ipyslickgrid v0.0.3 enabled ok
            js v0.1.0 enabled ok
            jupyter-leaflet v0.14.0 enabled ok
            jupyter-matplotlib v0.10.0 enabled ok
            jupyterlab-plotly v4.14.3 enabled ok
            jupyterlab_iframe v0.4.0 enabled ok
            nbdime-jupyterlab v2.1.0 enabled ok
            plotlywidget v4.14.3 enabled ok
    
    
    Uninstalled core extensions:
        bamboolib

Any advice is highly appreciated. Also, I'd like to use the community version for now.



"
tkrabel/bamboolib,"Contextual Version Conflict: seaborn: https://github.com/tkrabel/bamboolib/issues/31
Description: ### Environment

* Operating System: Windows 10
* 
* Python Version: `$ python --version`
* 3.7.10
*
* How did you install bamboolib: (`pip`, `conda`, or `other (please explain)`)
* Installed via pip in jupyter notebook
* Python packages: `$ pip list` or `$ conda list` (please include bamboolib)
* pip list
Package                           Version
--------------------------------- -------------------
anaconda-client                   1.8.0
analytics-python                  1.2.9
ansiwrap                          0.8.4
anyjson                           0.3.3
appdirs                           1.4.4
arcgis                            1.8.5
argon2-cffi                       20.1.0
asn1crypto                        1.4.0
async-generator                   1.10
atomicwrites                      1.4.0
attrs                             21.2.0
azure-core                        1.12.0
azure-storage-blob                12.8.0
backcall                          0.2.0
bamboolib                         1.26.0
beautifulsoup4                    4.9.3
black                             19.10b0
bleach                            3.3.0
blinker                           1.4
bokeh                             2.3.3
Bottleneck                        1.3.2
brotlipy                          0.7.0
cached-property                   1.5.2
certifi                           2021.5.30
cffi                              1.14.6
cftime                            1.0.0b1
chardet                           4.0.0
click                             8.0.1
cloudpickle                       1.6.0
clyent                            1.2.2
colorama                          0.4.4
conda-content-trust               0+unknown
conda-package-handling            1.7.3
conda-repo-cli                    1.0.4
cryptography                      2.8
cycler                            0.10.0
cytoolz                           0.11.0
dask                              2021.7.0
decorator                         5.0.9
defusedxml                        0.7.1
despatch                          0.1.0
distributed                       2021.7.0
entrypoints                       0.3
et-xmlfile                        1.0.1
fastcache                         1.1.0
flake8                            3.9.0
fsspec                            2021.6.0
future                            0.18.2
greenlet                          1.1.0
h5py                              2.10.0
HeapDict                          1.0.1
html5lib                          1.1
htmlmin                           0.1.12
idna                              2.10
ijson                             3.1.4
ImageHash                         4.2.0
importlib-metadata                3.10.0
iniconfig                         1.1.1
ipykernel                         5.5.5
ipyslickgrid                      0.0.3
ipython                           7.23.1
ipython-genutils                  0.2.0
ipywidgets                        7.6.3
isodate                           0.6.0
jdcal                             1.4.1
jedi                              0.18.0
Jinja2                            3.0.1
joblib                            1.0.1
json5                             0.9.4
jsonschema                        3.2.0
jupyter                           1.0.0
jupyter-client                    6.2.0
jupyter-console                   6.2.0
jupyter-contrib-core              0.3.3
jupyter-contrib-nbextensions      0.5.1
jupyter-core                      4.7.1
jupyter-highlight-selected-word   0.2.0
jupyter-latex-envs                1.4.4
jupyter-nbextensions-configurator 0.4.1
jupyterlab                        2.2.7
jupyterlab-pygments               0.1.1
jupyterlab-server                 1.2.0
jupyterlab-widgets                1.0.0
keyring                           21.4.0
kiwisolver                        1.3.1
lerc                              2.2
locket                            0.2.1
lxml                              4.6.3
MarkupSafe                        2.0.1
matplotlib                        3.3.1
matplotlib-inline                 0.1.2
mccabe                            0.6.1
menuinst                          1.4.16
missingno                         0.5.0
mistune                           0.8.4
mkl-fft                           1.3.0
mkl-random                        1.2.0
mkl-service                       2.3.0
mpmath                            1.2.1
msgpack                           1.0.2
msrest                            0.6.21
multimethod                       1.4
mypy-extensions                   0.4.3
navigator-updater                 0.2.1
nb-conda                          2.2.1
nb-conda-kernels                  2.3.1
nbclient                          0.5.0
nbconvert                         5.6.1
nbformat                          5.1.3
nest-asyncio                      1.5.1
netCDF4                           1.5.4
networkx                          2.5
nltk                              3.6.2
nose                              1.3.7
notebook                          5.7.10
ntlm-auth                         1.4.0
numexpr                           2.7.3
numpy                             1.20.1
oauthlib                          3.1.0
olefile                           0.46
openpyxl                          3.0.7
packaging                         21.0
pandas                            1.2.4
pandas-profiling                  3.0.0
pandas-ui                         0.1
pandasql                          0.7.3
pandocfilters                     1.4.3
papermill                         2.3.3
parso                             0.8.2
partd                             1.2.0
pathspec                          0.7.0
pdfminer.six                      20200517
pdfplumber                        0.5.28
pefile                            2019.4.18
phik                              0.11.2
pickleshare                       0.7.5
Pillow                            8.3.1
pip                               21.1.3
plotly                            4.14.3
pluggy                            0.13.1
ppscore                           1.2.0
pro-notebook-integration          2.5
prometheus-client                 0.8.0
prompt-toolkit                    3.0.18
psutil                            5.8.0
py                                1.10.0
pycodestyle                       2.6.0
pycosat                           0.6.3
pycparser                         2.20
pycryptodome                      3.10.1
pydantic                          1.8.2
pyflakes                          2.2.0
Pygments                          2.9.0
PyJWT                             2.0.1
pymssql                           2.1.5
pyodbc                            4.0.0-unsupported
pyOpenSSL                         20.0.1
pyparsing                         2.4.7
pypiwin32                         223
pyrsistent                        0.17.3
pyshp                             2.1.3
PySocks                           1.7.1
pytest                            0.0.0
python-certifi-win32              0.0.0
python-dateutil                   2.8.1
pytz                              2020.1
PyWavelets                        1.1.1
pywin32                           301
pywin32-ctypes                    0.2.0
pywinpty                          0.5.7
PyYAML                            5.4.1
pyzmq                             22.0.3
qgrid                             1.3.1
qtconsole                         5.1.0
QtPy                              1.9.0
regex                             2021.7.6
requests                          2.25.1
requests-kerberos                 0.12.0
requests-negotiate-sspi           0.0.0
requests-ntlm                     1.1.0
requests-oauthlib                 1.3.0
requests-toolbelt                 0.9.1
retrying                          1.3.3
ruamel-yaml-conda                 0.15.100
saspy                             3.6.1
scikit-learn                      0.24.2
scipy                             1.6.2
seaborn                           0.10.1
Send2Trash                        1.5.0
setuptools                        52.0.0.post20210125
simplegeneric                     0.8.1
simplejson                        3.17.2
six                               1.16.0
sortedcontainers                  2.4.0
soupsieve                         2.2.1
SQLAlchemy                        1.4.19
swat                              1.8.1
sympy                             1.5.1
tangled-up-in-unicode             0.1.0
tblib                             1.7.0
tenacity                          8.0.1
termcolor                         1.1.0
terminado                         0.9.4
testpath                          0.5.0
textwrap3                         0.9.2
threadpoolctl                     2.2.0
toml                              0.10.2
toolz                             0.11.1
tornado                           6.1
tqdm                              4.61.2
traitlets                         5.0.5
typed-ast                         1.4.2
typing-extensions                 3.10.0.0
ujson                             4.0.2
urllib3                           1.26.6
visions                           0.7.1
Wand                              0.6.6
wcwidth                           0.2.5
webencodings                      0.5.1
wheel                             0.36.2
widgetsnbextension                3.5.1
win-inet-pton                     1.1.0
wincertstore                      0.2
winkerberos                       0.7.0
wrapt                             1.12.1
x86cpu                            0.4
xarray                            0.17.0
xlrd                              1.2.0
xlwt                              1.3.0
xmltodict                         0.12.0
zict                              2.0.0
zipp                              3.5.0


* If bamboolib is used with JupyterLab: JupyterLab extensions: `jupyter labextension list`
* If bamboolib is used with Jupyter Notebook: Notebook extension: `jupyter nbextension list`

### Description of Issue

* What did you expect to happen?
* import bamboolib as bam
* 
* What happened instead?

1

import bamboolib as bam

---------------------------------------------------------------------------
ContextualVersionConflict                 Traceback (most recent call last)
<ipython-input-53-43382a4ff33a> in <module>
----> 1 import bamboolib as bam

~\Miniconda3\envs\eb_arcpro_clone\lib\site-packages\bamboolib\__init__.py in <module>
      8 
      9 try:
---> 10     __version__ = get_distribution(__name__).version
     11 except DistributionNotFound:
     12     __version__ = ""unknown""

~\Miniconda3\envs\eb_arcpro_clone\lib\site-packages\pkg_resources\__init__.py in get_distribution(dist)
    464         dist = Requirement.parse(dist)
    465     if isinstance(dist, Requirement):
--> 466         dist = get_provider(dist)
    467     if not isinstance(dist, Distribution):
    468         raise TypeError(""Expected string, Requirement, or Distribution"", dist)

~\Miniconda3\envs\eb_arcpro_clone\lib\site-packages\pkg_resources\__init__.py in get_provider(moduleOrReq)
    340     """"""Return an IResourceProvider for the named module or requirement""""""
    341     if isinstance(moduleOrReq, Requirement):
--> 342         return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]
    343     try:
    344         module = sys.modules[moduleOrReq]

~\Miniconda3\envs\eb_arcpro_clone\lib\site-packages\pkg_resources\__init__.py in require(self, *requirements)
    884         included, even if they were already activated in this working set.
    885         """"""
--> 886         needed = self.resolve(parse_requirements(requirements))
    887 
    888         for dist in needed:

~\Miniconda3\envs\eb_arcpro_clone\lib\site-packages\pkg_resources\__init__.py in resolve(self, requirements, env, installer, replace_conflicting, extras)
    775                 # Oops, the ""best"" so far conflicts with a dependency
    776                 dependent_req = required_by[req]
--> 777                 raise VersionConflict(dist, req).with_context(dependent_req)
    778 
    779             # push the new requirements onto the stack

ContextualVersionConflict: (seaborn 0.11.1 (c:\users\ebarr\miniconda3\envs\eb_arcpro_clone\lib\site-packages), Requirement.parse('seaborn<0.11,>=0.10'), {'bamboolib'})



"
tkrabel/bamboolib,"Minor CSS Bugs for Bamboolib with Voila: https://github.com/tkrabel/bamboolib/issues/29
Description: ### Background

I was experimenting with `bamboolib` and `voila` and found two very minor aesthetic issues

My use case with `voila` is to potentially provide a way for users who aren't comfortable with Jupyter to take advantage of `bamboolib` for EDA and code generation

### Environment

Fresh environment on Docker or Mac:

```sh
python -m pip install jupyterlab bamboolib voila
python -m bamboolib install_nbextensions

# Create a demo.ipynb file with any dataframe

# import bamboolib as bam
# import pandas as pd
# df = pd.read_csv(bam.titanic_csv)
# df

# Run voila
voila demo.ipynb
```

### Description of Issue

I checked Jupyter lab for the same python venv and neither issue was present within the editor, only in voila

* There is some additional internal spacing for the `.highlight` span
* The data table is excessively wide, but doesn't seem to cause any usability issues

<img width=""171"" alt=""Screen Shot 2021-06-12 at 18 48 56"" src=""https://user-images.githubusercontent.com/3784339/121790756-9d611500-cbb0-11eb-891d-80c5156b8c28.png"">

<img width=""834"" alt=""Screen Shot 2021-06-12 at 17 12 29"" src=""https://user-images.githubusercontent.com/3784339/121790764-b4076c00-cbb0-11eb-9be6-ac96a5d288e2.png"">

"
tkrabel/bamboolib,"After installation of  bamboolib , viewing output data in scrolling windows failed in jupyterlab: https://github.com/tkrabel/bamboolib/issues/28
Description: ### Environment

* Operating System: windows 10 or Ubuntu 2004
* Python Version: 3.6.8
* How did you install bamboolib: pip ,fully fresh installation  in a new venv , just did as the  https://pypi.org/project/bamboolib/.
 the jupyterlab==2.3.1 and notbook==6.3 
* Python packages: 
* If bamboolib is used with JupyterLab: JupyterLab extensions: `jupyter labextension list`
* If bamboolib is used with Jupyter Notebook: Notebook extension: `jupyter nbextension list`

### Description of Issue
After installed bamboolib , viewing data in scrolling windows faild.  In jupyterlab ,menu item of Enable Scrolling for  Outputs does not work.
 
![image](https://user-images.githubusercontent.com/24842100/114638212-af801d80-9cfd-11eb-851f-717d5b4e728f.png)

* What did you expect to happen?
* Just expect to be able to view data in scrolling windws successfully for other cells' output.
* What happened instead?
The scrolling action was failed as mentioned above

### Reproduction Steps
1 After  executed the  code from  https://docs.bamboolib.8080labs.com/documentation/how-tos/installation-and-setup/install-bamboolib/test-bamboolib , the result was as following picture
 
![image](https://user-images.githubusercontent.com/24842100/114532274-422da780-9c7f-11eb-93ca-16dddec9673b.png)
2 Then opened a new notebook and wrote some code as following picture
![image](https://user-images.githubusercontent.com/24842100/114532683-a486a800-9c7f-11eb-9717-55046db46ad5.png)
the code was simple,and the output was a little long
3 I wanted view the output data in a scrolling  windows, so just had a right-clicking on the output, and choose the  menu item of Enable Scrolling for  Outputs , but it failed, just as the following picture

![image](https://user-images.githubusercontent.com/24842100/114533846-ec59ff00-9c80-11eb-9165-fcbd4cbd4014.png)
4 I repeated this on Ubuntu 2004 and conda envs with other conditions keeping the same ,the Scrolling action was failed,too.
I had tried to switch jupyterlab to 2.2.9 and notebook to 6.1.5 and python to 3.8.8, but still could not view output data in scrolling window.
When I uninstalled bamboolib, the scrolling window mode was really enabled as following picture
![image](https://user-images.githubusercontent.com/24842100/114640016-d2143580-9d01-11eb-8dab-cbb890469b64.png)
5 Some more,I tried reinstalled bamboolib as step 1,but failed, In output window,it just kept showing the text of    bamboolib is loading ...

### What steps have you taken to resolve this already?

...Not yet

### Anything else?

... Is this bug or something else I have not got?

"
tkrabel/bamboolib,"Dark theme support in lab?: https://github.com/tkrabel/bamboolib/issues/26
Description: Hi there,

Is there any appetite to adjust lab and I assume notebook to support their dark themes?

While in dark theme screens such as the value_counts drill-down shows white text on a white background.

"
tkrabel/bamboolib,"I was trying to run the test  installation bamboolib   as suggested but it failed: https://github.com/tkrabel/bamboolib/issues/25
Description: ### Environment

* Operating System:
windows10
* Python Version: `$ python --version`
3.7
* How did you install bamboolib: (`pip`, `conda`, or `other (please explain)`)
 using conda, as per the instruction page on your site.
* Python packages: `$ pip list` or `$ conda list` (please include bamboolib)
# packages in environment at C:\Users\kvsse\anaconda3\envs\bamboolibenv:
#
# Name                    Version                   Build  Channel
argon2-cffi               20.1.0           py37he774522_1
async_generator           1.10             py37h28b3542_0
attrs                     20.3.0             pyhd3eb1b0_0
backcall                  0.2.0                      py_0
bleach                    3.2.1                      py_0
brotlipy                  0.7.0           py37he774522_1000
ca-certificates           2020.6.20            hecda079_0    conda-forge
certifi                   2020.6.20        py37hf50a25e_2    conda-forge
cffi                      1.14.3           py37h7a1dbc1_0
chardet                   3.0.4                 py37_1003
colorama                  0.4.4                      py_0
cryptography              3.1.1            py37h7a1dbc1_0
decorator                 4.4.2                      py_0
defusedxml                0.6.0                      py_0
entrypoints               0.3                      py37_0
icu                       58.2                 ha925a31_3
idna                      2.10                       py_0
importlib-metadata        2.0.0                      py_1
importlib_metadata        2.0.0                         1
intel-openmp              2020.2                      254
ipykernel                 5.3.4            py37h5ca1d4c_0
ipython                   7.19.0           py37hd4e2768_0
ipython_genutils          0.2.0                    py37_0
ipywidgets                7.5.1                      py_1
jedi                      0.17.2                   py37_0
jinja2                    2.11.2                     py_0
jpeg                      9b                   hb83a4c4_2
json5                     0.9.5                      py_0
jsonschema                3.2.0                      py_2
jupyter                   1.0.0                    py37_7
jupyter_client            6.1.7                      py_0
jupyter_console           6.2.0                      py_0
jupyter_core              4.6.3                    py37_0
jupyterlab                2.2.6                      py_0
jupyterlab_pygments       0.1.2                      py_0
jupyterlab_server         1.2.0                      py_0
libblas                   3.8.0                    20_mkl    conda-forge
libcblas                  3.8.0                    20_mkl    conda-forge
liblapack                 3.8.0                    20_mkl    conda-forge
libpng                    1.6.37               h2a8f88b_0
libsodium                 1.0.18               h62dcd97_0
m2w64-gcc-libgfortran     5.3.0                         6
m2w64-gcc-libs            5.3.0                         7
m2w64-gcc-libs-core       5.3.0                         7
m2w64-gmp                 6.1.0                         2
m2w64-libwinpthread-git   5.0.0.4634.697f757               2
markupsafe                1.1.1            py37hfa6e2cd_1
mistune                   0.8.4           py37hfa6e2cd_1001
mkl                       2020.2                      256
msys2-conda-epoch         20160418                      1
nbclient                  0.5.1                      py_0
nbconvert                 6.0.7                    py37_0
nbformat                  5.0.8                      py_0
nest-asyncio              1.4.1                      py_0
notebook                  6.1.4                    py37_0
numpy                     1.19.4           py37hd20adf4_1    conda-forge
openssl                   1.1.1h               he774522_0    conda-forge
packaging                 20.4                       py_0
pandoc                    2.11                 h9490d1a_0
pandocfilters             1.4.2                    py37_1
parso                     0.7.0                      py_0
pickleshare               0.7.5                 py37_1001
pip                       20.2.4           py37haa95532_0
prometheus_client         0.8.0                      py_0
prompt-toolkit            3.0.8                      py_0
prompt_toolkit            3.0.8                         0
pycparser                 2.20                       py_2
pygments                  2.7.2              pyhd3eb1b0_0
pyopenssl                 19.1.0                     py_1
pyparsing                 2.4.7                      py_0
pyqt                      5.9.2            py37h6538335_2
pyrsistent                0.17.3           py37he774522_0
pysocks                   1.7.1                    py37_1
python                    3.7.9                h60c2a47_0
python-dateutil           2.8.1                      py_0
python_abi                3.7                     1_cp37m    conda-forge
pywin32                   227              py37he774522_1
pywinpty                  0.5.7                    py37_0
pyzmq                     19.0.2           py37ha925a31_1
qt                        5.9.7            vc14h73c81de_0
qtconsole                 4.7.7                      py_0
qtpy                      1.9.0                      py_0
requests                  2.24.0                     py_0
send2trash                1.5.0                    py37_0
setuptools                50.3.1           py37haa95532_1
sip                       4.19.8           py37h6538335_0
six                       1.15.0                     py_0
sqlite                    3.33.0               h2a8f88b_0
terminado                 0.9.1                    py37_0
testpath                  0.4.4                      py_0
tornado                   6.0.4            py37he774522_1
traitlets                 5.0.5                      py_0
urllib3                   1.25.11                    py_0
vc                        14.1                 h0510ff6_4
vs2015_runtime            14.16.27012          hf0eaf9b_3
wcwidth                   0.2.5                      py_0
webencodings              0.5.1                    py37_1
wheel                     0.35.1                     py_0
widgetsnbextension        3.5.1                    py37_0
win_inet_pton             1.1.0                    py37_0
wincertstore              0.2                      py37_0
winpty                    0.4.3                         4
zeromq                    4.3.2                ha925a31_3
zipp                      3.4.0              pyhd3eb1b0_0
zlib                      1.2.11               h62dcd97_4

(bamboolibenv) C:\Users\kvsse>




* If bamboolib is used with JupyterLab: JupyterLab extensions: `jupyter labextension list`
(bamboolibenv) C:\Users\kvsse>jupyter labextension list
JupyterLab v2.2.6
No installed extensions
* If bamboolib is used with Jupyter Notebook: Notebook extension: `jupyter nbextension list`

### Description of Issue

* What did you expect to happen?
suppose to display pandas dataframe with your app button
* What happened instead/
displayed this message
RuntimeError                              Traceback (most recent call last)
<ipython-input-1-98d62baa4c65> in <module>
----> 1 import bamboolib as bam
      2 import pandas as pd
      3 df = pd.read_csv(bam.titanic_csv)
      4 df

~\AppData\Roaming\Python\Python37\site-packages\bamboolib\__init__.py in <module>
     17 # NOTE: you can think about removing this if we require matplotlib>3.3.1
     18 try:
---> 19     import seaborn
     20 except ImportError as import_error:
     21     if ""DLL load failed"" in str(import_error):

~\AppData\Roaming\Python\Python37\site-packages\seaborn\__init__.py in <module>
      1 # Capture the original matplotlib rcParams
----> 2 import matplotlib as mpl
      3 _orig_rc_params = mpl.rcParams.copy()
      4 
      5 # Import seaborn objects

~\AppData\Roaming\Python\Python37\site-packages\matplotlib\__init__.py in <module>
    105 # cbook must import matplotlib only within function
    106 # definitions, so it is safe to import from it here.
--> 107 from . import cbook, rcsetup
    108 from matplotlib.cbook import MatplotlibDeprecationWarning, sanitize_sequence
    109 from matplotlib.cbook import mplDeprecation  # deprecated

~\AppData\Roaming\Python\Python37\site-packages\matplotlib\cbook\__init__.py in <module>
     26 import weakref
     27 
---> 28 import numpy as np
     29 
     30 import matplotlib

~\AppData\Roaming\Python\Python37\site-packages\numpy\__init__.py in <module>
    303 
    304     if sys.platform == ""win32"" and sys.maxsize > 2**32:
--> 305         _win_os_check()
    306 
    307     del _win_os_check

~\AppData\Roaming\Python\Python37\site-packages\numpy\__init__.py in _win_os_check()
    300                    ""See this issue for more information: ""
    301                    ""https://tinyurl.com/y3dm3h86"")
--> 302             raise RuntimeError(msg.format(__file__)) from None
    303 
    304     if sys.platform == ""win32"" and sys.maxsize > 2**32:

RuntimeError: The current Numpy installation ('C:\\Users\\kvsse\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\__init__.py') fails to pass a sanity check due to a bug in the windows runtime. See this issue for more information: https://tinyurl.com/y3dm3h86

### Reproduction Steps

1.
2.
3.
...

### What steps have you taken to resolve this already?
installed the latest version of numpy and tried the same error

...

### Anything else?

...

"
tkrabel/bamboolib,"[Bug] Cannot import bamboolib on Kaggle anymore: https://github.com/tkrabel/bamboolib/issues/23
Description: ### Environment

* Operating System: 
* Python Version: Python 3.7.6
* How did you install bamboolib: (`pip`, `conda`, or `other (please explain)`)
Just like it was mentioned in the docs/Kaggle kernel
* Python packages: 
_(I can provide this if necessary from the Kaggle environment)_

* If bamboolib is used with JupyterLab: JupyterLab extensions (Kaggle's environment): 
```
JupyterLab v2.2.5
Known labextensions:
   app dir: /opt/conda/share/jupyter/lab
        @jupyter-widgets/jupyterlab-manager v1.1.0 [32m enabled [0m [31m X[0m
        @jupyterlab/celltags v0.2.0 [32m enabled [0m [31m X[0m
        @jupyterlab/git v0.10.0 [32m enabled [0m [31m X[0m
        jupyterlab-plotly v1.5.1 [32m enabled [0m [31m X[0m
        nbdime-jupyterlab v1.0.0 [32m enabled [0m [31m X[0m
        plotlywidget v1.5.1 [32m enabled [0m [32mOK[0m

   The following extension are outdated:
        @jupyter-widgets/jupyterlab-manager
        @jupyterlab/celltags
        @jupyterlab/git
        jupyterlab-plotly
        nbdime-jupyterlab
        
   Consider running ""jupyter labextension update --all"" to check for updates.


Build recommended, please run `jupyter lab build`:
    plotlywidget needs to be included in build
```
* If bamboolib is used with Jupyter Notebook: Notebook extension: `jupyter nbextension list`
_(I can provide this if necessary from the Kaggle environment)_

### Description of Issue

Tried to import Bamboolib but got the below error instead:
```
---------------------------------------------------------------------------
ContextualVersionConflict                 Traceback (most recent call last)
<ipython-input-67-43382a4ff33a> in <module>
----> 1 import bamboolib as bam

/opt/conda/lib/python3.7/site-packages/bamboolib/__init__.py in <module>
      4 
      5 try:
----> 6     __version__ = get_distribution(__name__).version
      7 except DistributionNotFound:
      8     __version__ = ""unknown""

/opt/conda/lib/python3.7/site-packages/pkg_resources/__init__.py in get_distribution(dist)
    480         dist = Requirement.parse(dist)
    481     if isinstance(dist, Requirement):
--> 482         dist = get_provider(dist)
    483     if not isinstance(dist, Distribution):
    484         raise TypeError(""Expected string, Requirement, or Distribution"", dist)

/opt/conda/lib/python3.7/site-packages/pkg_resources/__init__.py in get_provider(moduleOrReq)
    356     """"""Return an IResourceProvider for the named module or requirement""""""
    357     if isinstance(moduleOrReq, Requirement):
--> 358         return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]
    359     try:
    360         module = sys.modules[moduleOrReq]

/opt/conda/lib/python3.7/site-packages/pkg_resources/__init__.py in require(self, *requirements)
    899         included, even if they were already activated in this working set.
    900         """"""
--> 901         needed = self.resolve(parse_requirements(requirements))
    902 
    903         for dist in needed:

/opt/conda/lib/python3.7/site-packages/pkg_resources/__init__.py in resolve(self, requirements, env, installer, replace_conflicting, extras)
    790                 # Oops, the ""best"" so far conflicts with a dependency
    791                 dependent_req = required_by[req]
--> 792                 raise VersionConflict(dist, req).with_context(dependent_req)
    793 
    794             # push the new requirements onto the stack

ContextualVersionConflict: (jupyterlab 1.2.10 (/opt/conda/lib/python3.7/site-packages), Requirement.parse('jupyterlab<3.0,>=2.0'), {'bamboolib'})
```

### Reproduction Steps

Use this kernel and try to run it https://www.kaggle.com/neomatrix369/chaieda-sessions-titanic-using-tools


### What steps have you taken to resolve this already?

Nothing yet

"
tkrabel/bamboolib,"[Troubleshooting] ModuleNotFound [...jupyter/...] when trying to build Jupterlab: https://github.com/tkrabel/bamboolib/issues/18
Description: ### Environment

* Operating System: `macos`
* Python Version: `3.7.2`
* How did you install bamboolib:  `pip`
* Python packages: see issue https://github.com/tkrabel/bamboolib/issues/13
* If bamboolib is used with JupyterLab: JupyterLab extensions: `jupyter labextension list`
see issue https://github.com/tkrabel/bamboolib/issues/13
* If bamboolib is used with Jupyter Notebook: Notebook extension: `jupyter nbextension list`
see issue https://github.com/tkrabel/bamboolib/issues/13

### Description of Issue

The details are mentioned in issue see issue https://github.com/tkrabel/bamboolib/issues/13, although repeating the specific problem see this message on the Jupyter Lab Gitter Channel: https://gitter.im/jupyterlab/jupyterlab?at=5e4711fa46e99d431f78c53d

I was installing `bamboolib` on my macOS and expected the installation to go smooth but it seems some other package might be conflicting after performing the last steps at [installation steps](https://github.com/tkrabel/bamboolib/blob/master/installation/no_virtual_environment/installation.md#22-jupyter-lab-10) - I got this error message:
```
[LabBuildApp] JupyterLab 1.2.7
[LabBuildApp] Building in /Users/swami/.local/share/jupyter/lab
[LabBuildApp] Building jupyterlab assets (build:prod)
[LabBuildApp] WARNING | The extension ""nbdime-jupyterlab"" is outdated.

An error occured.
RuntimeError: JupyterLab failed to build
See the log file for details:  /var/folders/lv/qn495mln6h79dkh4b6j3rclm0000gn/T/jupyterlab-debug-1u0e_k9s.log
```

Looking into the logs shows me:
```
...
[LabBuildApp] > node /Users/swami/.local/lib/python3.7/site-packages/jupyterlab/staging/yarn.js run build:prod
[LabBuildApp] yarn run v1.15.2
$ ensure-max-old-space webpack --config webpack.prod.config.js
ModuleNotFoundError: Module not found: Error: Can't resolve 'es6-promise/auto' in ...
...
```

As per the gitter discussions, it appears a version of plotly lab extension conflicts when trying to build the lab extensions, more details in the section below.

### Reproduction Steps

This one might be hard to reproduce as it's user specific, but hopefully, the attached log file might prove it:

[jupyterlab-debug-1u0e_k9s.log](https://github.com/tkrabel/bamboolib/files/4329059/jupyterlab-debug-1u0e_k9s.log)

Also this message on the `gitter` channel of `jupyterlabs`: https://gitter.im/jupyterlab/jupyterlab?at=5e4711fa46e99d431f78c53d (search for the messages around `ModuleNotFoundError: Module not found: Error: Can't resolve 'es6-promise/auto' in '/opt/conda/share/jupyter/lab/staging/build`

### What steps have you taken to resolve this already?

I did the following and the error went away, and I was able to rebuild the `jupyterlab` extensions:

```
cd /path/to/.local/share/jupyter/lab/staging/
npm install jupyterlab-plotly
npm install jupyterlab-chart-editor
npm install bamboolib
npm install nbdime-jupyterlab
npm install @8080labs/qgrid@1.1.1
npm install qgrid
npm install plotlywidget
npm install @8080labs/qgrid
npm install @jupyter-widgets/jupyterlab-manager
npm install @jupyterlab/coreutils
npm install @jupyterlab/application
npm install es6-promise@^3.0.2 es6-promise@~4.2.6
npm install
jupyter lab build --minimize=False
```
"
Autodesk/notebook-molecular-visualization,"Problems rendering jupyter notebook widget: https://github.com/Autodesk/notebook-molecular-visualization/issues/53
Description: I've just done a clean install using the repo versions of `notebook-molecular-visualization` and `molviz` under conda py3 and I'm getting various issues.

Most notably, the widget doesn't render - browser console shows:

```
manager-base.js:57 Could not instantiate widget
(anonymous) @ manager-base.js:57
step @ manager.js:196
(anonymous) @ manager.js:177
rejected @ manager.js:169
utils.js:8 Error: Could not create a model.
    at promiseRejection (utils.js:8)
promiseRejection @ utils.js:8
2kernel.js:924 Couldn't process kernel message Error: Script error for ""nbmolviz-js""
http://requirejs.org/docs/errors.html#scripterror
    at makeError (require.js?v=6da8be361b9ee26c5e721e76c6d4afce:165)
    at HTMLScriptElement.onScriptError (require.js?v=6da8be361b9ee26c5e721e76c6d4afce:1732)
(anonymous) @ kernel.js:924
OpenLearn.ipynb:1 Uncaught (in promise) Error: Script error for ""nbmolviz-js""
http://requirejs.org/docs/errors.html#scripterror
    at makeError (require.js?v=6da8be361b9ee26c5e721e76c6d4afce:165)
    at HTMLScriptElement.onScriptError (require.js?v=6da8be361b9ee26c5e721e76c6d4afce:1732)
```

After importing the `nbmolviz` (and viewer) python package, this error also appears when I run notebook cells:

```
Error in callback <function _capture_logging_displays at 0x10c5ff158> (for pre_run_cell):
---------------------------------------------------------------------------
TraitError                                Traceback (most recent call last)
~/anaconda3/lib/python3.6/site-packages/nbmolviz/uielements/logwidget.py in _capture_logging_displays(display, **kwargs)
    192     if widgets_enabled:
    193         _current_tabs = LoggingTabs(OrderedDict(x=ipy.Box()), display=display,
--> 194                                     **wu.process_widget_kwargs(kwargs))
    195     else:
    196         _current_tabs = None

~/anaconda3/lib/python3.6/site-packages/nbmolviz/uielements/logwidget.py in __init__(self, objects, display, **kwargs)
    133             super(LoggingTabs, self).__init__(list(objects.values()),
    134                                               **wu.process_widget_kwargs(kwargs))
--> 135             self.selected_index = -1
    136             for ikey, key in enumerate(objects.keys()):
    137                 self.set_title(ikey, key)

~/anaconda3/lib/python3.6/site-packages/traitlets/traitlets.py in __set__(self, obj, value)
    583             raise TraitError('The ""%s"" trait is read-only.' % self.name)
    584         else:
--> 585             self.set(obj, value)
    586 
    587     def _validate(self, obj, value):

~/anaconda3/lib/python3.6/site-packages/traitlets/traitlets.py in set(self, obj, value)
    557 
    558     def set(self, obj, value):
--> 559         new_value = self._validate(obj, value)
    560         try:
    561             old_value = obj._trait_values[self.name]

~/anaconda3/lib/python3.6/site-packages/traitlets/traitlets.py in _validate(self, obj, value)
    591             value = self.validate(obj, value)
    592         if obj._cross_validation_lock is False:
--> 593             value = self._cross_validate(obj, value)
    594         return value
    595 

~/anaconda3/lib/python3.6/site-packages/traitlets/traitlets.py in _cross_validate(self, obj, value)
    597         if self.name in obj._trait_validators:
    598             proposal = Bunch({'trait': self, 'value': value, 'owner': obj})
--> 599             value = obj._trait_validators[self.name](obj, proposal)
    600         elif hasattr(obj, '_%s_validate' % self.name):
    601             meth_name = '_%s_validate' % self.name

~/anaconda3/lib/python3.6/site-packages/traitlets/traitlets.py in __call__(self, *args, **kwargs)
    905         """"""Pass `*args` and `**kwargs` to the handler's function if it exists.""""""
    906         if hasattr(self, 'func'):
--> 907             return self.func(*args, **kwargs)
    908         else:
    909             return self._init_call(*args, **kwargs)

~/anaconda3/lib/python3.6/site-packages/ipywidgets/widgets/widget_selectioncontainer.py in _validated_index(self, proposal)
     28             return proposal.value
     29         else:
---> 30             raise TraitError('Invalid selection: index out of bounds')
     31 
     32     # Public methods

TraitError: Invalid selection: index out of bounds
```

"
Autodesk/notebook-molecular-visualization,"Update README, better notebook extension installation: https://github.com/Autodesk/notebook-molecular-visualization/pull/50
Description: 

"
Autodesk/notebook-molecular-visualization,"Adapts partially to Py3. Adds a known good npm/nodejs download channel.: https://github.com/Autodesk/notebook-molecular-visualization/pull/46
Description: Far from comprehensive, but catches a few Py3 problems I saw in trying to run Tutorial 3. Also specifies conda-forge `nodejs` package as a worthy way to get ""npm"", rather than the broken `npm` conda package on cpcloud channel.

I notice afterward your py3 branch, so let me know if you'd rather have this PR to that.

Current error for Tutorial 3 is:
```
... last 1 frames repeated, from the frame below ...

~/linux/notebook-molecular-visualization/nbmolviz/widgets/selector.py in __getattr__(self, item)
    105 
    106     def __getattr__(self, item):
--> 107         if self.viewer is not None: return getattr(self.viewer, item)
    108         else: raise AttributeError(item)
    109 

RecursionError: maximum recursion depth exceeded while calling a Python object
```

"
Autodesk/notebook-molecular-visualization,"adding user=True to installing_nbext_py: https://github.com/Autodesk/notebook-molecular-visualization/pull/44
Description: I was recently trying to play around with the molecular design toolkit and was having problems related to the nbmolviz extension. I came to a similar conclusion as Issue #63 in autodesk/molecular-design-toolkit, where upon import of moldesign, an attempt to install the extension to /usr/... (somewhere in root) is denied and doesn't allow viewing of molecules. 

In nbmolviz/__init__.py (118), when the enabling of the nbmolviz-js extension is invoked, it uses the notebook.nbextensions.install_nb_extension_python function which optionally takes in a 'user' argument which tells the app to install somewhere user has permissions. I added user=True to the arguments of this function.

I cleared out all previous install attempts of nbmolviz, and spun up a new jupyter notebook. Upon import moldesign, I received no warnings of attempted install to root, and was able to visualize molecules. 

I ran what py.tests I could in both this repo and mdt, with only the same errors that I previously had appearing. This seems a bit too good to be true, but who knows. 

"
Autodesk/notebook-molecular-visualization,"Fix Color Pop-In Bug: https://github.com/Autodesk/notebook-molecular-visualization/pull/42
Description: See issue: https://github.com/Autodesk/notebook-molecular-visualization/issues/39

"
Autodesk/notebook-molecular-visualization,"viewer.color_by is slow for large molecules: https://github.com/Autodesk/notebook-molecular-visualization/issues/39
Description: This actually looks cool, although it's probably not what we want :)

**To reproduce:**
In a notebook, execute this cell:
```python
mol = mdt.from_pdb('3aid')
drug = mol.chains['A'].get_ligand()
viewer = mol.draw(display=True)
viewer.stick()
```

Then this one:
```python
viewer.color_by(lambda atom:atom.distance(drug))
```

In the latest version, the colors draw in 1-at-a-time, which takes a while:
![color_by_slow](https://cloud.githubusercontent.com/assets/9388007/19950966/459b8c28-a118-11e6-96ec-b6df628b3e4f.gif)

In 0.7.3, the color comes in all-at-once after thinking for a second or two:
![color_by_fast](https://cloud.githubusercontent.com/assets/9388007/19951029/ac41e684-a118-11e6-985d-ba5ceed3ca89.gif)

"
Autodesk/notebook-molecular-visualization,"Fix jsdeps Install Error: https://github.com/Autodesk/notebook-molecular-visualization/pull/34
Description: See issue: https://github.com/Autodesk/notebook-molecular-visualization/issues/33

"
Autodesk/notebook-molecular-visualization,"Dev environment fails to build: https://github.com/Autodesk/notebook-molecular-visualization/issues/33
Description: The master branch of this repo - currently on ded6a66dd0e2151fede520fa7fe9d405da9f4f4b - fails to build:

<details>
<summary>

 click for logs </summary>



```
$ python setup.py jsdeps
setup.py entered
$PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
running jsdeps
3.10.8
3.10.8
Installing build dependencies with npm.  This may take a while...

> molecule-2d-for-react@0.2.1 postinstall /root/notebook-molecular-visualization/js/node_modules/molecule-2d-for-react
> node nightwatch.conf.js

module.js:471
    throw err;
    ^

Error: Cannot find module 'selenium-download'
    at Function.Module._resolveFilename (module.js:469:15)
    at Function.Module._load (module.js:417:25)
    at Module.require (module.js:497:17)
    at require (internal/module.js:20:19)
    at Object.<anonymous> (/root/notebook-molecular-visualization/js/node_modules/molecule-2d-for-react/nightwatch.conf.js:2:26)
    at Module._compile (module.js:570:32)
    at Object.Module._extensions..js (module.js:579:10)
    at Module.load (module.js:487:32)
    at tryModuleLoad (module.js:446:12)
    at Function.Module._load (module.js:438:3)
npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.0.0 (node_modules/chokidar/node_modules/fsevents):
npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.0.14: wanted {""os"":""darwin"",""arch"":""any""} (current: {""os"":""linux"",""arch"":""x64""})
npm ERR! Linux 4.4.27-moby
npm ERR! argv ""/usr/local/bin/node"" ""/usr/local/bin/npm"" ""install""
npm ERR! node v6.9.1
npm ERR! npm  v3.10.8
npm ERR! code ELIFECYCLE

npm ERR! molecule-2d-for-react@0.2.1 postinstall: `node nightwatch.conf.js`
npm ERR! Exit status 1
npm ERR! 
npm ERR! Failed at the molecule-2d-for-react@0.2.1 postinstall script 'node nightwatch.conf.js'.
npm ERR! Make sure you have the latest version of node.js and npm installed.
npm ERR! If you do, this is most likely a problem with the molecule-2d-for-react package,
npm ERR! not with npm itself.
npm ERR! Tell the author that this fails on your system:
npm ERR!     node nightwatch.conf.js
npm ERR! You can get information on how to open an issue for this project with:
npm ERR!     npm bugs molecule-2d-for-react
npm ERR! Or if that isn't available, you can get their info via:
npm ERR!     npm owner ls molecule-2d-for-react
npm ERR! There is likely additional logging output above.

npm ERR! Please include the following file with any support request:
npm ERR!     /root/notebook-molecular-visualization/js/npm-debug.log
Traceback (most recent call last):
  File ""setup.py"", line 190, in <module>
    setup(**args)
  File ""/usr/lib/python2.7/distutils/core.py"", line 151, in setup
    dist.run_commands()
  File ""/usr/lib/python2.7/distutils/dist.py"", line 953, in run_commands
    self.run_command(cmd)
  File ""/usr/lib/python2.7/distutils/dist.py"", line 972, in run_command
    cmd_obj.run()
  File ""setup.py"", line 138, in run
    check_call(['npm', 'install'], cwd=node_root, stdout=sys.stdout, stderr=sys.stderr)
  File ""/usr/lib/python2.7/subprocess.py"", line 540, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command '['npm', 'install']' returned non-zero exit status 1
root@6436cf018746:~/notebook-molecular-visualization# 
```

</details>

Pretty sure that this comes from a mismatch of the javascript libraries (being installed straight from NPM) and the github python code. If so, this may be unavoidable, but it would be great if there were a workaround.

"
Autodesk/notebook-molecular-visualization,"CI with Travis: https://github.com/Autodesk/notebook-molecular-visualization/pull/28
Description: https://travis-ci.org/Autodesk/notebook-molecular-visualization


"
Autodesk/notebook-molecular-visualization,"Unable to use on Ubuntu: https://github.com/Autodesk/notebook-molecular-visualization/issues/26
Description: When installing via pip, there is an error saying that it cannot import `utils`, e.g. `from nbmolviz import utils`. When I install from source, I get this error:

```
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-1-fbf31e3efb44> in <module>()
----> 1 import nbmolviz
      2 import pybel
      3 benzene = pybel.read_string('smi','c1cccc1').next()
      4 nbmolviz.visualize(benzene)

/home/herman/repos/universe/notebook-molecular-visualization/nbmolviz/__init__.py in <module>()
     16 
     17 from nbmolviz import utils
---> 18 from nbmolviz import base_widget, widget3d, interfaces3d, drivers3d, widget2d
     19 
     20 # package metadata

/home/herman/repos/universe/notebook-molecular-visualization/nbmolviz/drivers3d.py in <module>()
     16 from traitlets import Bool, Dict, Float, List, Set, Unicode
     17 
---> 18 from moldesign import units as u
     19 
     20 from nbmolviz.utils import JSObject, translate_color

/usr/local/lib/python2.7/dist-packages/moldesign/__init__.pyc in <module>()
     26 from . import utils
     27 from . import units
---> 28 from . import uibase
     29 from . import widgets
     30 

/usr/local/lib/python2.7/dist-packages/moldesign/uibase/__init__.py in <module>()
      4 __all__ = []
      5 
----> 6 from .selector import *
      7 from .components import *
      8 from .plotting import *

/usr/local/lib/python2.7/dist-packages/moldesign/uibase/selector.py in <module>()
     16 
     17 import moldesign as mdt
---> 18 from moldesign import utils, viewer
     19 
     20 

/usr/local/lib/python2.7/dist-packages/moldesign/viewer/__init__.py in <module>()
      6 from .common import *
      7 from .viewer2d import *
----> 8 from .viewer3d import *
      9 from .bondclicker import *

/usr/local/lib/python2.7/dist-packages/moldesign/viewer/viewer3d.py in <module>()
     21 from moldesign import utils
     22 from moldesign.helpers import VolumetricGrid, colormap
---> 23 from nbmolviz.drivers3d import MolViz_3DMol
     24 from . import toplevel, ColorMixin
     25 

ImportError: cannot import name MolViz_3DMol
```

I have a slightly messed up environment with previous attempts using conda to install `chemview` and `nglview`. But AFAIK there is nothing that could affect this. I'm running Ubuntu 16.04

"
hyriver/pygeohydro,"WBD Feature returning keyerror: 'layers': https://github.com/hyriver/pygeohydro/issues/103
Description: ### What happened?

![image](https://user-images.githubusercontent.com/119536203/228628069-54ab0fa5-7289-46be-84d7-5fb109e221b3.png)
When loading a huc6 byid, received KeyError: ""layers""


### What did you expect to happen?

Expected huc6 170900 polygon to be loaded into notebook.

### Minimal Complete Verifiable Example

_No response_

### MVCE confirmation

- [ ] Minimal example — the example is as focused as reasonably possible to demonstrate the underlying issue.
- [ ] Complete example — the example is self-contained, including all data and the text of any traceback.
- [ ] New issue — a search of GitHub Issues suggests this is not a duplicate.

### Relevant log output

_No response_

### Anything else we need to know?

_No response_

### Environment

<details>



</details>

"
vaexio/vaex,"[BUG-REPORT]   example https://vaex.io/docs/tutorial_jupyter.html doesn't work due to dependencies: https://github.com/vaexio/vaex/issues/2355
Description: Hello,
We tried to run the example notebook that was hosted on binder (https://hub.gke2.mybinder.org/user/vaexio-vaex-1co5v7py/notebooks/docs/source/tutorial_jupyter.ipynb ) on local machine.  

It didn't work due to  ""observe"" not being defined. 

After a lot of trial and error, it turned out that the trailtlets package that was automatically installed (5.9.0) wasn't backward compatible. 

Downgrading to 5.4.0, like in the example notebook fixed the problem.  

Please add limiting on trailtlets package


"
vaexio/vaex,"[BUG-REPORT] Left join operation stays broken in vaex version 4.16: https://github.com/vaexio/vaex/issues/2354
Description: **Description**

We are trying to join two filtered data frame in vaex. Previous bug reports indicate vaex join is somewhat broken in this important functionality. Few advices are given in earlier bug reports to use copies of the dataframes or sort columns.

On our case even a combination of both solutions, use copies plus sort data frames on joined columns, didn't work stable.

Assume, we try to compute a left join of two filtered and cleaned (dropna(column_to_join)) data frames using vaex in a Jupyter notebook.

```
joined_df = df1.join(
        df2,
        on=column_to_join, # e.g. column name as string
        how=""left"",
        lsuffix=""_l"",
        rsuffix=""_r"",
        allow_duplication=True
    )
```

Both data frames are expected not to be ordered on the columns to be joined. In addition both datasets have different row count, like df1 having 20778 rows and df2 having 15668457 rows. Not all values of df1 and its column_to_join are found in df2 and vice versa. So there is a smaller union set expected, eventually providing multiple matches from df2, with certain exclusions on both sides where column values do not yield matching rows. This is a common data constellation working with correlations of two different data sources. Other sample provided in #1319.

As suggested in #1319  I tried to to use copies of the two filtered data frames. This also lead to the known bug giving stack trace below showing an index out of bounds error. Hence, using copies of filtered data frames doesn't solve the problem in general.

I tried to sort the dataframe by the values of the column to join. Unfortunately this yielded another error indicating that vaex left join sometimes(!) expected the two data frames having same length to be joined. That one looked pretty weird and indicates vaex sometimes mixes up row indices and/or row counts of the two data frames during join operation. This error and stack trace unfortunately was not always reproducible after rearranging the order to sort or copy both filtered data frames and switching back to any of my previous approaches. Hence I'm not able to provide a stack trace for the second failing case, after running the notebook multiple times with different modifications.

From this quick observation and analysis it seems that vaex version 4.16.0 still has problems running a simple operation of a left join of two data frames with different row count and non-matching values in both data frames. At some point vaex seems to mix up row indices and/or row counts to address individual rows of any of both data frames.

Summary:

- Running a left join on copies of filtered data frames doesn't solve the problem (bug)
- Sorting columns on column values used by the join, doesn't always fix the problem (error stack trace not reproducible)
- The necessity to sort values before joining introduces an annoying penalty, because it is an expensive task plus it doesn't always solve the problem
- To reproduce this bug it is eventually required to create data frames of much different size, with a small union set of matching rows on both sides of the join, eventually also assuming to obtain multiple matches found in df2
- It gives a bit a strange feeling leaving the vaex user in doubts about the proper result of the join
- As joining data frames is one of the most common operations, expectations are high this will be fixed soon

Stacktrace 
```
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
Cell In[49], line 1
----> 1 joined_df = df1.join(
      2         df2,
      3         on=column_to_join,
      4         how=""left"",
      5         lsuffix=""_l"",
      6         rsuffix=""_r"",
      7         allow_duplication=True
      8     )

File ~/git/project_name/venv-python/lib/python3.10/site-packages/vaex/dataframe.py:6714, in DataFrameLocal.join(self, other, on, left_on, right_on, lprefix, rprefix, lsuffix, rsuffix, how, allow_duplication, prime_growth, cardinality_other, inplace)
   6712 kwargs['df'] = kwargs.pop('self')
   6713 del kwargs['vaex']
-> 6714 return vaex.join.join(**kwargs)

File ~/git/project_name/venv-python/lib/python3.10/site-packages/vaex/join.py:214, in join(df, other, on, left_on, right_on, lprefix, rprefix, lsuffix, rsuffix, how, allow_duplication, prime_growth, cardinality_other, inplace)
    212     lookup_left = np.concatenate([k[0] for k in lookup_extra_chunks])
    213     lookup_right = np.concatenate([k[1] for k in lookup_extra_chunks])
--> 214     left = left.concat(left.take(lookup_left))
    215     lookup = np.concatenate([lookup, lookup_right])
    217 if inner:

File ~/git/project_name/venv-python/lib/python3.10/site-packages/vaex/dataframe.py:4461, in DataFrame.take(self, indices, filtered, dropfilter)
   4459     mask = df._selection_masks[FILTER_SELECTION_NAME]
   4460     filtered_indices = mask.first(max_index+1)
-> 4461     indices = filtered_indices[indices]
   4462 df.dataset = df.dataset.take(indices)
   4463 if dropfilter:
   4464     # if the indices refer to the filtered rows, we can discard the
   4465     # filter in the final dataframe

IndexError: index 563015 is out of bounds for axis 0 with size 20778
```

**Software information**
 - {'vaex': '4.16.0',
 'vaex-core': '4.16.1',
 'vaex-viz': '0.5.4',
 'vaex-hdf5': '0.14.1',
 'vaex-server': '0.8.1',
 'vaex-astro': '0.9.3',
 'vaex-jupyter': '0.8.1',
 'vaex-ml': '0.18.1'}

 - Vaex was installed via: pip 
 - OS: Linux

**Additional information**
None.

"
vaexio/vaex,"Interactive widget fix: https://github.com/vaexio/vaex/pull/2353
Description: There are minor typos in widgets.py from `vaex-jupyter` that result in #2321, this PR fixes them.

"
vaexio/vaex,"[BUG-REPORT] AttributeError: module 'vaex.dataset' has no attribute '_parse_f': https://github.com/vaexio/vaex/issues/2348
Description: **Description**

Hi,
I am trying to create a plot2d_contour but it is giving me this error:

```---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[44], line 2
      1 df = vaex.example()
----> 2 df.plot2d_contour(x=df.x,y=df.y)

File ~/opt/anaconda3/envs/agb/lib/python3.10/site-packages/vaex/viz/contour.py:43, in plot2d_contour(self, x, y, what, limits, shape, selection, f, figsize, xlabel, ylabel, aspect, levels, fill, colorbar, colorbar_label, colormap, colors, linewidths, linestyles, vmin, vmax, grid, show, **kwargs)
     14 """"""
     15 Plot conting contours on 2D grid.
     16 
   (...)
     38 :param show:
     39 """"""
     42 # Get the function out of the string
---> 43 f = vaex.dataset._parse_f(f)
     45 # Internals on what to bin
     46 binby = []

AttributeError: module 'vaex.dataset' has no attribute '_parse_f'
```

Code:
```
import vaex
df = vaex.example()
df.plot2d_contour(x=df.x,y=df.y)
```
I also tried to explicitly set f = ""identity"" and f=""log"" and it didn't work. 

**Software information**
 - Vaex version:
 {'vaex': '4.16.0',
 'vaex-core': '4.16.1',
 'vaex-viz': '0.5.4',
 'vaex-hdf5': '0.14.1',
 'vaex-server': '0.8.1',
 'vaex-astro': '0.9.3',
 'vaex-jupyter': '0.8.1',
 'vaex-ml': '0.18.1'}
 - Vaex was installed via: pip
 - OS: macOS 11.6.5


"
vaexio/vaex,"[BUG-REPORT] Strange behavior - vaex vs. numpy - floating point error?: https://github.com/vaexio/vaex/issues/2347
Description: **Description**
Hi,

Thank you for this great software! It's a real game changer for my work!

My question/issue: I observe quantitative differences when I compare the results (of the same computation) of vaex with those of numpy. Those differences are relatively small, but I'd like to understand where they stem from or if I am doing something wrong. 

I want to compute the ECDF of a vector. Since the computation is not compatible with vaex-expressions I resort to the apply-method (Ideas how to vectorize the ECDF computation to make it compatible with vaex would be of course warmly welcome!)

The computation with vaex is lighting fast (almost a bit fishy, considering how long numpy takes), but unfortunately the results differ from those with numpy. With my real world data the differences are more pronounced (~1.0e-03).

Am I doing something wrong? Is vaex using different floating points than numpy? Is it a bug?

Your comments/your help would be very much appreciated! 


```python
import numpy as np
import vaex

# function to compute the ecdf
def ecdf(x, side='right'):
    
    v = x.copy()
    v.sort()
    nobs = len(v)
    y = np.linspace(1./nobs, 1, nobs)

    sx = np.hstack((np.array([-np.inf]), v))
    sy = np.hstack((np.array([0]), y))
    
    tind = np.searchsorted(sx, x, side) - 1
    return sy[tind]

x = np.random.rand(10_000_000,)

df = vaex.from_arrays(x=x)

df['res'] = df.x.apply(ecdf, vectorize = True)
res_vaex = df['res'].evaluate()
res_np = ecdf(x)

np.all(res_vaex == res_np)
# False

np.mean(res_vaex - res_np)
# 3.949999999965477e-06

```


**Software information**
 - Vaex version:
'vaex': '4.16.0',
 'vaex-core': '4.16.1',
 'vaex-viz': '0.5.4',
 'vaex-hdf5': '0.14.1',
 'vaex-server': '0.8.1',
 'vaex-astro': '0.9.3',
 'vaex-jupyter': '0.8.1',
 'vaex-ml': '0.18.1'
 - Vaex was installed via: pip
 - OS: Ubuntu 18.04


"
vaexio/vaex,"[BUG-REPORT] Difference in time taken when slicing an offset differs based on sort order.: https://github.com/vaexio/vaex/issues/2341
Description: Hi Vaex Team,  
We are experiencing an issue with the sort function in Vaex. Specifically, when we sort our dataset (shape: (13160951, 77)) by the estimaterevenue column (dtype: float64) in descending order, we observe a delay in slicing the lower offset, while slicing the higher offset is relatively fast.On the other hand, when we sort in ascending order the same column, we observe that slicing the lower offset is relatively fast, while slicing the higher offset is slow. I would appreciate it if you could look into this issue. It seems to be a bug, can you please confirm ?

Software information
• Vaex version 4.14.0
• Vaex was installed via: pip
• OS: Linux

Additional information  
Jupyter notebook screenshot image attached

![image](https://user-images.githubusercontent.com/99238709/220010021-356659f0-f523-44aa-8361-f4857b5db06f.png)





"
vaexio/vaex,"[BUG-REPORT] Incredibly slow performance with shifted columns: https://github.com/vaexio/vaex/issues/2338
Description: **Description**
Hi! I'm not sure if this is a use case addressed by vaex, but the following example has an incredibly slow performance:

```
import pandas as pd
import vaex
import numpy as np

N=2**26
df = pd.DataFrame()
array = np.linspace(1,N,N).reshape(-1, 2**6)

for i in range(2**6):
    df[f'col_{i}'] = array[:,i]

vaex_df = vaex.from_pandas(df)
for col in vaex_df.get_column_names():
    vaex_df[f""{col}_shift_1""] = vaex_df[col]
    vaex_df.shift(periods=1, column=f""{col}_shift_1"", inplace=True)
```
Fast so far...

But accessing one row:

`%timeit vaex_df[10]`

10.8 s ± 81.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

... not anymore.

Appreciate any help on the issue!

**Software information**
 - Vaex version (`import vaex; vaex.__version__)`:
 ```
{'vaex-core': '4.16.1',
 'vaex-viz': '0.5.4',
 'vaex-hdf5': '0.14.1',
 'vaex-server': '0.8.1',
 'vaex-jupyter': '0.8.1',
 'vaex-ml': '0.18.1'}
```
 - Vaex was installed via: pip
 - OS: macOS Ventura 13.2 / Apple Silicon


"
vaexio/vaex,"[BUG-REPORT] Potential memory leak when exporting large strings to hdf5: https://github.com/vaexio/vaex/issues/2334
Description: Thank you for reaching out and helping us improve Vaex!

Before you submit a new Issue, please read through the [documentation](https://docs.vaex.io/en/latest/). Also, make sure you search through the Open and Closed Issues - your problem may already be discussed or addressed.

**Description**
Please provide a clear and concise description of the problem. This should contain all the steps needed to reproduce the problem. A minimal code example that exposes the problem is very appreciated.

**Software information**
 - Vaex version (`import vaex; vaex.__version__)`: 4.16.1
 - Vaex was installed via: pip / conda-forge / from source pip
 - OS: Linux (colab)

**Additional information**
If you run this on a limited machine like google colab free, you will get a OOM crash when exporting to hdf5, even though it works fine exporting to arrow. We need to convert the string to a large_string because of pyarrow issues https://issues.apache.org/jira/browse/ARROW-17828

```
import vaex
import pyarrow as pa

df = vaex.example()
df[""text""] = vaex.vconstant(""OHYEA""*10000, len(df))

@vaex.register_function()
def to_large(arr):
    return arr.cast(pa.large_string())

df[""text""] = df[""text""].to_large()
#OOM
df.export(""file.hdf5"")
```

"
cmudig/AutoProfiler,"only update when AP is open: https://github.com/cmudig/AutoProfiler/pull/126
Description: ## Functionality
- Only updates the data in AP if the panel is actually visible; does not slow down notebook if the panel is closed

## Issues addressed

Fixes #124 

"
cmudig/AutoProfiler,"Shut-off option: https://github.com/cmudig/AutoProfiler/issues/124
Description: It's possible I've missed some existing way to do this, but I couldn't figure out how using the extension or jupyter lab in general. Basically I'd like to be able to completely turn off autoprofiler and have it clear its memory and not do anything. I'm currently working with very large dataframes and I think autoprofiler is causing a lot of slow down, so I just uninstalled it, but I would have preferred to just shut it down instead if that was possible!


"
cmudig/AutoProfiler,"Export all unique values for columns with long value counts: https://github.com/cmudig/AutoProfiler/issues/121
Description: Likely useful to always have an export unique values button.

Or maybe only when the number of unique values is > 10 so some are getting cut off we can add a line to the topK chart that says ""+ 25 more values"" and an option to export all the unique values to code in the notebook

"
cmudig/AutoProfiler,"Autoprofiler panel  getting stuck and losing color : https://github.com/cmudig/AutoProfiler/issues/116
Description: When there are too many dataframes, Autoprofiler is getting stuck (drop-down menus are not working and are not showing anything) and the charts are distorted.



Steps to reproduce the behavior (to be filled)
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

Example:
<img width=""829"" alt=""df_stuck"" src=""https://user-images.githubusercontent.com/81047350/220182878-bbd58584-91d2-4cf7-a8dd-b58311ad79a4.png"">

 - OS: macOS
 - Browser: safari
 - AutoProfiler version: 0.2.8
 - Jupyter version: 3.4.4


"
cmudig/AutoProfiler,"Stuck when dataframe has no column name: https://github.com/cmudig/AutoProfiler/issues/109
Description: **Bug**
If a dataframe has no specific column name (e.g. 'angle') but just number index (e.g. 0, 1, 2), the AutoProfiler got stuck with loading (pink circle), had to restart the kernel. 

**To Reproduce**
1. Create a list: x = [2, 2, 2]
2. Convert x to dataframe with column name: x_df0 = pd.DataFrame(x, columns = ['angle'])
3. Convert x to dataframe: x_df1 = pd.DataFrame(x)

**Expected behavior**
x_df0 works with the profiler; x_df0 does not work with the profiler but causing it to be stuck and kernel had to be restarted.


**OS, browser, jupyter, and AutoProfiler versions (please complete the following information):**
 - OS: Redhat
 - Browser: Chrome
 - AutoProfiler version: pip installed 2023 Jan 
 - Jupyter version: 3.9.15


"
cmudig/AutoProfiler,"Export selection to notebook: https://github.com/cmudig/AutoProfiler/issues/100
Description: Core use case:
- I see a weird min, max value -- which rows actually have this value? Can export selection to notebook like
`df[df.col_name == MAX_VAL]`

- Another use case that is similar is exporting selections from bar charts or topK charts, this needs a little more design

"
cmudig/AutoProfiler,"initial logger: https://github.com/cmudig/AutoProfiler/pull/97
Description: ## Functionality
- Logs interactions with AutoProfiler
- Saves the interactions to the notebook metadata every 5 seconds

TBD on if we want to merge this in, or only merge it in for a specific version for evaluation

"
dssg/aequitas,"Update visualizations: https://github.com/dssg/aequitas/pull/119
Description: **Colab notebook**: https://colab.research.google.com/drive/1o_l6y7CCghZctzFyrZtY890YHLUlBewo?usp=sharing

"
dssg/aequitas,"issue regarding uploading the data set and run it on compas notebook: https://github.com/dssg/aequitas/issues/114
Description: I want to run a data set on Web Api of Aequitas but am not able to run it, after uploading it is showing a local error, please provide a solution regarding this with proper steps on how can we upload different data sets.

"
dssg/aequitas,"Issue regarding running my data set on compass file as a reference : https://github.com/dssg/aequitas/issues/112
Description: Actually, I am trying to run my data set on Aequitas(Compass notebook ) but after reading my data set whenever I am trying to initialize group class and giving my df as the value in the function get_crosstabs( ) it is generating an error (Exception: get_crosstabs: input df was not preprocessed. There are non-string cols within attr_cols!).
Please help me to resolve this
![stack overflow 1](https://user-images.githubusercontent.com/53338376/137117644-9ca3b7c4-efcb-48fb-b988-ad305eba8f89.JPG)
![stack overflow 2](https://user-images.githubusercontent.com/53338376/137117655-f2c86fbf-1bc6-4740-b96e-bf16f432595e.JPG)
![stack overflow](https://user-images.githubusercontent.com/53338376/137117657-b1384ae1-cc5b-41d6-8233-c22f34741252.JPG)
, how can I run my data set on fraud deduction and resolve my issue
![stack overflow](https://user-images.githubusercontent.com/53338376/137117397-29222d44-ac59-4106-83f0-c560af5d17b9.JPG)

"
interpretml/gam-changer,"inconsistent with package interpret: https://github.com/interpretml/gam-changer/issues/9
Description: ```python
import gamchanger as gc
from json import dump

# Extract model weights
model_data = gc.get_model_data(ebm)

# Generate sample data
sample_data = gc.get_sample_data(ebm, x_test, y_test)

# Save to `model.json` and `sample.json`
dump(model_data, open('./model.json', 'w'))
dump(sample_data, open('./sample.json', 'w'))
```

```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-81-f99310b1230c> in <module>
      3 
      4 # Extract model weights
----> 5 model_data = gc.get_model_data(ebm)
      6 
      7 # Generate sample data

/conda/envs/notebook/lib/python3.6/site-packages/gamchanger/gamchanger.py in get_model_data(ebm, resort_categorical)
     71     score_range = [np.inf, -np.inf]
     72 
---> 73     for i in range(len(ebm.feature_names)):
     74         cur_feature = {}
     75         cur_feature[""name""] = ebm.feature_names[i]

TypeError: object of type 'NoneType' has no len()
```

The object `ebm ` does not have the `feature_names` sub one.

Which version of `gamchanger` I can degrade?

```sh
%watermark -iv

gamchanger 0.1.10
numpy      1.18.0
interpret  0.3.0
pandas     1.0.4
```

"
interpretml/gam-changer,"Version 0.1.4 breaks gam-changer-adult.ipynb notebook: https://github.com/interpretml/gam-changer/issues/5
Description: Hello,

There is a check for nans in 0.1.4 (line 268 of `gamchanger.py`) that breaks on string values. If I use version 0.1.3, the notebook still works. I think columns of string type should be encoded before this check.

Thanks,
Jessica

```
[/usr/local/lib/python3.7/dist-packages/gamchanger/gamchanger.py](https://localhost:8080/#) in get_sample_data(ebm, x_test, y_test, resort_categorical)
    266 
    267     # Drop all rows with any NA values
--> 268     if np.isnan(x_test_copy).any():
    269         na_row_indexes = np.isnan(x_test_copy).any(axis=1)
    270         x_test_copy = x_test_copy[~na_row_indexes]

TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
```
"
JupyterPhysSciLab/JupyterPiDAQ,"Make JupyterPiDAQ specific javascript have its own namespace...: https://github.com/JupyterPhysSciLab/JupyterPiDAQ/issues/18
Description: To be a somewhat better citizen the javascript specific to JupyterPiDAQ should reside in its own namespace (maybe under JupyterPiDAQ). If I can figure out how to get it to work in JLab this will be less of an issue as the namespace protection is embedded in the way the javascript is built.

"
JupyterPhysSciLab/JupyterPiDAQ,"DAQ menu should not show up if the tools are not initialized...: https://github.com/JupyterPhysSciLab/JupyterPiDAQ/issues/16
Description: The DAQ menu setup needs to be wrapped in a call to `JPSLUtils.OTJS()` to avoid the menu being available when the code will not run because jupyterPiDAQ has not been imported.

"
JupyterPhysSciLab/JupyterPiDAQ,"Possible alternative behavior for run setup interface: https://github.com/JupyterPhysSciLab/JupyterPiDAQ/issues/15
Description: Instead of everything in a single cell, which seems to leave traces of the setup table widget in reopened notebooks, the setup could behave more like the jupyter_Pandas_GUI tools. It would create the code for the run in the cell immediately below it and then delete itself when all the settings are approved.

"
JupyterPhysSciLab/JupyterPiDAQ,"Make work in JupyterLab: https://github.com/JupyterPhysSciLab/JupyterPiDAQ/issues/9
Description: This depends on Jupyter notebook specific javascript calls. Need to adjust to work with the alternative JupyterLab calls.
"
predict-idlab/plotly-resampler,":rocket: integrate with tsdownsample: https://github.com/predict-idlab/plotly-resampler/pull/191
Description: - [x] avoid creating tsdownsample objects every time we downsample - should be done in the constructor
- [x] fix serialization -> addressed in https://github.com/predict-idlab/tsdownsample/pull/34
- [x] redefine the size-threshold, given that minmax is now way faster! 🐎 
- [x] verify docs
- [x] remove MinMaxOverlap?
  > I would keep it, but add comments about this aggregator.
- [x] remove lttbc & the build / setup.py commands
- [x] update the `README.md` with the `tsdownsample` integration! 🔥 
- [x] what about the `parallell` parameter?
  > would add the `**downsample_kwargs` argument to the datapoint selector impelemntations.
- [x] Verify that basic-example notebooks are still working.

---

Copy all Python implementation to tsdownsample - where we can properly test whether these yield the same results 

"
predict-idlab/plotly-resampler,"Stop_Server() Is not working to close the server when show_dash() is called in a separate thread: https://github.com/predict-idlab/plotly-resampler/issues/179
Description: Hi, here's a MWE to reproduce this issue:
Here are the module versions:

1. plotly-resampler==0.8.3.1
2. Flask==2.1.3
3. jupyter-dash==0.4.2
4. Werkzeug==2.1.2

```
import plotly.graph_objects as go; import numpy as np
from plotly_resampler import FigureResampler
import time
import threading

x = np.arange(1_000_000)
sin = (3 + np.sin(x / 200) + np.random.randn(len(x)) / 10) * x / 1_000

fig = FigureResampler(go.Figure())
fig.add_trace(go.Scattergl(name='noisy sine', showlegend=True), hf_x=x, hf_y=sin)

threading.Thread(target=fig.show_dash, kwargs={""mode"": ""external"", ""port"":8050}, daemon=True).start()

start_time = time.time()
while True:
    if time.time() - start_time > 10:
        print(""Time to stop!"")
        fig.stop_server()
        print(""Stop server ran!"")
        break

# This loop is meant to keep the script alive
while True:
    pass
```
If the `fig.stop_server()` function is working, refreshing the browser: `127.0.0.1:8050` should yield a ""SITE CANNOT BE REACHED ERROR"" which is expected if the dash server is functional.

However, that is not the case, and the server is still alive. Please kindly advise, how one can go about terminating the server, without having to terminate the entire python script.

"
predict-idlab/plotly-resampler,"About plotly graph exporting to HTML file: https://github.com/predict-idlab/plotly-resampler/issues/175
Description: In my case, I want to export my plotly graph into HTML for later use, and I just run the following code:

`fig.write_html('fig1.html', include_plotlyjs='cdn')`

The good news is I do get a complete figure with basic interactive features (zoom, hover, etc.), but it will lose the advanced features like **auto resampling** when you zoom in/out like in the jupyter. 

I've been looking for similar problems in every place, but I didn't think I find any direct detailed explanation and possible solution to this.

Maybe it's the innate html limitaiton, and hopefully someone can help me understand or even solve this.

"
koaning/whatlies,"AttributeError: 'Word2VecKeyedVectors' object has no attribute 'vocab' on Mac not windows : https://github.com/koaning/whatlies/issues/315
Description: Hi I had a question
I tried similar file with similar code on a Windows and a Mac. I only changed the path names but with the Mac I'm getting an error. I was wondering whether this is due to the fact that it is a Mac or if there is anything else I am overlooking?

In both cases I run the code in Jupyter Notebook in VS code. 

Thanks in advance!

Windows:
![WhatsApp Image 2021-09-19 at 10 07 10](https://user-images.githubusercontent.com/47860858/133920482-b655367c-ccaa-4fea-af7f-311b3165df76.jpeg)

Mac:
<img width=""1126"" alt=""image"" src=""https://user-images.githubusercontent.com/47860858/133920550-b7d9b719-ce04-4a62-9039-428a7ffd9f4d.png"">

full error on Mac:
```
AttributeError: 'Word2VecKeyedVectors' object has no attribute 'vocab'
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/var/folders/yv/1l5fy71n0sb4gr_jh8p2yj3r0000gn/T/ipykernel_58308/781142390.py in <module>
----> 1 embeddings = lang[metaDataWords[:100]]

/opt/miniconda3/envs/wl/lib/python3.9/site-packages/whatlies/language/_gensim_lang.py in __getitem__(self, query)
     94                 vec = np.zeros(self.kv.vector_size)
     95             return Embedding(query, vec)
---> 96         return EmbeddingSet(*[self[tok] for tok in query])
     97 
     98     def _prepare_queries(self, lower):

/opt/miniconda3/envs/wl/lib/python3.9/site-packages/whatlies/language/_gensim_lang.py in <listcomp>(.0)
     94                 vec = np.zeros(self.kv.vector_size)
     95             return Embedding(query, vec)
---> 96         return EmbeddingSet(*[self[tok] for tok in query])
     97 
     98     def _prepare_queries(self, lower):

/opt/miniconda3/envs/wl/lib/python3.9/site-packages/whatlies/language/_gensim_lang.py in __getitem__(self, query)
     90                 )
     91             try:
---> 92                 vec = np.sum([self.kv[q] for q in query.split("" "")], axis=0)
     93             except KeyError:
     94                 vec = np.zeros(self.kv.vector_size)

/opt/miniconda3/envs/wl/lib/python3.9/site-packages/whatlies/language/_gensim_lang.py in <listcomp>(.0)
     90                 )
     91             try:
---> 92                 vec = np.sum([self.kv[q] for q in query.split("" "")], axis=0)
     93             except KeyError:
     94                 vec = np.zeros(self.kv.vector_size)

/opt/miniconda3/envs/wl/lib/python3.9/site-packages/gensim/models/keyedvectors.py in __getitem__(self, entities)
    351         if isinstance(entities, string_types):
    352             # allow calls like trained_model['office'], as a shorthand for trained_model[['office']]
--> 353             return self.get_vector(entities)
    354 
    355         return vstack([self.get_vector(entity) for entity in entities])

/opt/miniconda3/envs/wl/lib/python3.9/site-packages/gensim/models/keyedvectors.py in get_vector(self, word)
    469 
    470     def get_vector(self, word):
--> 471         return self.word_vec(word)
    472 
    473     def words_closer_than(self, w1, w2):

/opt/miniconda3/envs/wl/lib/python3.9/site-packages/gensim/models/keyedvectors.py in word_vec(self, word, use_norm)
    457 
    458         """"""
--> 459         if word in self.vocab:
    460             if use_norm:
    461                 result = self.vectors_norm[self.vocab[word].index]

AttributeError: 'Word2VecKeyedVectors' object has no attribute 'vocab'
```


"
WestHealth/pyvis,"I'cant apply editNode function in pyvis's set_options function: https://github.com/WestHealth/pyvis/issues/226
Description: from pyvis.network import Network
net = Network('500', '50%',notebook=True,cdn_resources='remote')
net.toggle_physics(True)
net.set_options(""""""var options = {
  ""manipulation"": {
    ""enabled"": true,
    ""initiallyActive"": false
    ""editNode"": true
  }
net.add_node(1, label=""Node 1"",color='green')
net.add_node(2, label=""Node 2"",color='red')
net.add_edge(1,2)
net.show('mygraph.html')

when I run it, it throw a error:

net.set_options(""""""var options = {
self.options = self.options.set(options)
options = json.loads(options)
return _default_decoder.decode(s)
obj, end = self.raw_decode(s, idx=_w(s, 0).end())
obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting ',' delimiter: line 1 column 56 (char 55)

Who can help me？ Thx

"
WestHealth/pyvis,"Graphs display with bowing or curving edges in Jupyter Notebook: https://github.com/WestHealth/pyvis/issues/217
Description: Following a recent update, whenever I display a graph in a jupyter notebook now the edges are displayed as bowed or curved lines, rather than straight lines as they used to.  Here is a test example:
```
import pyvis.network
import networkx
g = pyvis.network.Network(notebook = True, bgcolor = 'black', font_color = 'white', height=""600px"", width=""90%"", cdn_resources='remote')
nxg = nx.complete_graph(8)
g.from_nx(nxg)
display(g.show('test_case.html'))
```

which produces this output:
![image](https://user-images.githubusercontent.com/24434775/224180721-a94e958f-76a9-4c73-ad2c-39d9ac3078bd.png)

How can I get it to display with straight edges as it used to?

"
WestHealth/pyvis,"Error in showing network in pyvis_0.3.2 but not in pyvis_0.3.1: https://github.com/WestHealth/pyvis/issues/216
Description: Hello,

I'm running a simple python code for visualising a network. It works fine with pyvis_0.3.1, but with pyvis_0.3.2 I get the following error message when calling 

```python
nt = pyvis.Network('100px', '100px')
nt.from_nx(G)  #networkx graph
nt.show('net.html')
```

```
File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyvis/network.py:546, in Network.show(self, name, local, notebook)
    544 print(name)
    545 if notebook:
--> 546     self.write_html(name, open_browser=False,notebook=True)
    547 else:
    548     self.write_html(name, open_browser=True)

File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyvis/network.py:515, in Network.write_html(self, name, local, notebook, open_browser)
    513 getcwd_name = name
    514 check_html(getcwd_name)
--> 515 self.html = self.generate_html(notebook=notebook)
    517 if self.cdn_resources == ""local"":
    518     if not os.path.exists(""lib""):

File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyvis/network.py:479, in Network.generate_html(self, name, local, notebook)
    476 else:
    477     physics_enabled = self.options.physics.enabled
--> 479 self.html = template.render(height=height,
...
    496                             cdn_resources=self.cdn_resources
    497                             )
    498 return self.html

AttributeError: 'NoneType' object has no attribute 'render'

```

"
WestHealth/pyvis,"TypeError: __init__() got an unexpected keyword argument 'filter_menu': https://github.com/WestHealth/pyvis/issues/214
Description: Code Editor used : Jupyter Notebook (Anaconda)
While using the below command the TypeError appears 

`net = Network(height=""100%"", width=""100%"", bgcolor=""#222222"", font_color=""white"", filter_menu=True)`

However, on removal of `filter_menu` it works and give the desired html file.

P.S : same issue exist for `select_menu` and `cdn_resources` as well. 

I re-run my code after updating the package in my local machine.

"
WestHealth/pyvis,"Bug in the Network.write_html?: https://github.com/WestHealth/pyvis/issues/206
Description: https://github.com/WestHealth/pyvis/blob/8ce37e5027d9821c417e941804be924d27eeeef5/pyvis/network.py#L538

I executed the following code within the Jupyterlab cell (under Windows 11), and I expect it to render the plot on an external browser. However, my browser could not find the HTML file.

```
from pyvis import network as net

g = net.Network()
g.add_node(1)
g.add_node(2)
g.add_node(3)
g.add_edge(1,2)
g.add_edge(2,3)
g.show('example.html')
```

I realized the prefix `./` messes up the `webbrowser.open`. So maybe `webbrowser` is the root cause here?
Anyway, after changing
```
webbrowser.open(f""{tempdir}/{name}"")
```
to
```
webbrowser.open(name)
```
I get the expected behavior. Please review and confirm the bug. 

Cheers!


"
WestHealth/pyvis,"Show function that does not write a file.: https://github.com/WestHealth/pyvis/issues/205
Description: Hi! Thanks for the package, it is very nice!

It seems odd to me that when calling `show` a file is written to disk. I wrote my own show function using your html generator, which doesn't write the file. Here it is in case you want to use it:

```python
def show(net, notebook=False):
    html = net.generate_html(notebook=notebook)

    if notebook:
        # Render HTML directly on the notebook. It needs to be isolated so that the CSS does not mess
        # with the rest of the notebook.
        from IPython.display import display, HTML
        obj = HTML(f'<div style=""height:{net.height}"">{html}</div>', metadata={""isolated"": True}, )
        display(obj)
    else:
        # Write to a temporary file and open it in the webbrowser
        import tempfile
        import webbrowser

        with tempfile.NamedTemporaryFile(""w"", suffix="".html"", delete=False) as f:
            f.write(html)
            name = f.name
        
            webbrowser.open(name)
```

"
WestHealth/pyvis,"html not showing in `vscode` `jupyter` notebook: https://github.com/WestHealth/pyvis/issues/203
Description: I have the same issue that is posted by [rickhg12hs](https://github.com/rickhg12hs) on the vscode-jupyter page. I'm creating a simple graph via NetworkX, then trying to display it using pyvis. A blank space (of the given height) is the result:

![image](https://user-images.githubusercontent.com/87760291/210622411-b285c560-8b7e-43ee-ab15-bb05789e312a.png)

If opened in my browser, the html file works fine:

![image](https://user-images.githubusercontent.com/87760291/210622580-703920e1-2287-47d0-8043-597e7c8f0a0c.png)

"
sfu-db/dataprep,"Latest dataprep versions seem to give incompatible requirements to pip-compile: https://github.com/sfu-db/dataprep/issues/955
Description: Hello, love the dataprep package, thanks for creating it!

I used dataprep in a jupyter notebook, installing via `!pip compile dataprep` recently and it installed smoothly.

Now I'm tidying up some of that work and am using a venv, but `pip-compile` keeps crashing and I seem to have isolated dataprep as the cause. A minimal reproducible example below:

Given a `test_requirements.in` file which contains only a single line of `dataprep`
Running `pip-compile test_requirements.in --verbose --output-file test_requirements.txt` yields the following error:

```
Could not find a version that matches executing<0.9.0,>=0.8.3,>=1.2.0 (from varname==0.8.3->dataprep==0.4.5->-r test_requirements.in (line 1))
Tried: 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.2.0, 0.3.0, 0.3.1, 0.3.2, 0.3.3, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.5.0, 0.5.2, 0.5.3, 0.5.3, 0.5.4, 0.5.4, 0.6.0, 0.6.0, 0.7.0, 0.7.0, 0.8.0, 0.8.0, 0.8.1, 0.8.1, 0.8.2, 0.8.2, 0.8.3, 0.8.3, 0.9.0, 0.9.0, 0.9.1, 0.9.1, 0.10.0, 0.10.0, 1.0.0, 1.0.0, 1.1.0, 1.1.0, 1.1.1, 1.1.1, 1.2.0, 1.2.0
There are incompatible versions in the resolved dependencies:
  executing<0.9.0,>=0.8.3 (from varname==0.8.3->dataprep==0.4.5->-r test_requirements.in (line 1))
  executing>=1.2.0 (from stack-data==0.6.2->ipython==8.8.0->ipywidgets==7.7.2->dataprep==0.4.5->-r test_requirements.in (line 1))
```

I'm not sure if this is a dataprep problem or a pip-compile problem, or a problem with one of dataprep's dependencies (or some other thing I haven't thought of), but wanted to post here in case it's helpful. I also posted a question on stack overflow at: https://stackoverflow.com/q/75090614/1870832


"
sfu-db/dataprep,"EDA plot() not showing properly in VScode: https://github.com/sfu-db/dataprep/issues/952
Description: **Describe the bug**
EDA plot() not showing properly in VScode

**To Reproduce**
Steps to reproduce the behavior:
1. Open new Jupyter notebook
2. Import from dataprep.eda import plot, plot_correlation, create_report, plot_missing
3. Try .head() method on any test dataset so it shows first DF rows as output
4. Next cell call .plot() method from dataprep
5. Near finishing execution, the method  adds padding to the left of all Output cells and it's impossible to scroll to the right to see the full output

Or:

```python

import numpy as np
import pandas as pd
import datetime
from datetime import date
import matplotlib
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from sklearn.preprocessing import StandardScaler, normalize
from sklearn import metrics
from sklearn.mixture import GaussianMixture
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
import warnings
warnings.filterwarnings('ignore')
data=pd.read_csv('marketing_campaign.csv',header=0,sep=';')

from dataprep.eda import plot, plot_correlation, create_report, plot_missing
plot(data)
```

**Expected behavior**
I expected to be shown the output normally in the screen

**Screenshots**

Images: 
![image_2022-12-20_105546368](https://user-images.githubusercontent.com/100173574/208697030-86c02c90-b097-48e8-b3c5-efd262a30807.png)

![image_2022-12-20_105618462](https://user-images.githubusercontent.com/100173574/208697155-4ffa66d5-af83-4fc6-8425-5219b90863d3.png)

![image_2022-12-20_105702601](https://user-images.githubusercontent.com/100173574/208697327-09f50e2e-7903-4189-adfe-c64b297b178b.png)

**Desktop (please complete the following information):**
 - OS: Windows 11
 - Browser: None
 - Platform VSCode 
 - Platform Version 1.74.1
 - Python Version 3.10.9
 - Dataprep Version 0.4.4

**Additional context**





"
sfu-db/dataprep,"Future Warning: Meta is not valid: https://github.com/sfu-db/dataprep/issues/945
Description: **Describe the bug**
When using clean_country on pd Dataframe with python 3.10 the cleaning process does run but shows the following error message: 

> C:\ProgramData\Anaconda3\lib\site-packages\dask\dataframe\core.py:6641: FutureWarning: Meta is not valid, `map_partitions` and `map_overlap` expects output to be a pandas object. Try passing a pandas object as meta or a dict or tuple representing the (name, dtype) of the columns. In the future the meta you passed will not work.
>   warnings.warn(

**To Reproduce**
Steps to reproduce the behavior:
run the function clean_country with pd.dataframe (outputformat ""name"") 

Or:

```python
clean_country(countries, ""country"", output_format=""name"")
```
with countries beeing a pandas Dataframe 

**Expected behavior**
Should not lead to the error

**Desktop (please complete the following information):**
 - OS: Windows 10
 - Browser: Mozilla 
 - Platform: Jupyter Notebook and also testet on Python Script
 - Python Version 3.9.12
 - Dataprep Version 0.4.5

**Additional context**
Problem also exists with clean_email


"
sfu-db/dataprep,"Country name cleaning failed example: https://github.com/sfu-db/dataprep/issues/939
Description: **Describe the bug**
Hi, just found the country name ""Virgin Islands (British)"" would be failed to clean to the correct name.

**To Reproduce**

```python
import pandas as pd
from dataprep.clean import clean_country

df = pd.DataFrame({""country"": [""Virgin Islands (British)"", ""Virgin Islands (U.S.)""]})
clean_country(df, column=""country"", output_format=""name"")
```

Output:
|      | country                  | country_clean                |
| ---- | ------------------------ | ---------------------------- |
| 0    | Virgin Islands (British) | NaN                          |
| 1    | Virgin Islands (U.S.)    | United States Virgin Islands |

**Expected behavior**
The based on project [country_converter](https://github.com/konstantinstadler/country_converter) can work like below.

```python
import country_converter as coco

names = [""Virgin Islands (British)"", ""Virgin Islands (U.S.)""]
cc = coco.CountryConverter()

cc.convert(names=names, to=""name_short"")
# Output: ['British Virgin Islands', 'United States Virgin Islands']
```

**Desktop (please complete the following information):**
 - OS: macOS
 - Browser: Chrome
 - Platform:  Jupyter Notebook
 - Platform Version 6.4.12
 - Python Version: 3.10.5
 - Dataprep Version: 0.4.5

"
sfu-db/dataprep,"build(deps): bump nbconvert from 6.5.0 to 6.5.1: https://github.com/sfu-db/dataprep/pull/938
Description: Bumps [nbconvert](https://github.com/jupyter/nbconvert) from 6.5.0 to 6.5.1.
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/jupyter/nbconvert/commit/7471b75a506b2fec776613e50e4f2234b97f3c8e""><code>7471b75</code></a> Release 6.5.1</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/c1943e0e9fd0ad6abd7d8dae380474cca4b04a31""><code>c1943e0</code></a> Fix pre-commit</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/8685e9378086e8d82a0df92505fe386095f929ad""><code>8685e93</code></a> Fix tests</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/0abf2906bc6c7170c8d70bc0df6995d21c5aeaf1""><code>0abf290</code></a> Run black and prettier</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/418d545ae596d95f5ea82d141c68fd1abc99f1a6""><code>418d545</code></a> Run test on 6.x branch</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/bef65d7ab2a469b01e4aa25f44c0f20326f7c7c5""><code>bef65d7</code></a> Convert input to string prior to escape HTML</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/0818628718c4a5d3ddd671fbd4881bf176e7d6e2""><code>0818628</code></a> Check input type before escaping</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/b206470f9ecd71b006a37dd1298dd3d9e3dd46dd""><code>b206470</code></a> GHSL-2021-1017, GHSL-2021-1020, GHSL-2021-1021</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/a03cbb8a8d04d47aefec51e7b1b816045682aed5""><code>a03cbb8</code></a> GHSL-2021-1026, GHSL-2021-1025</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/48fe71eb3335caf4e03166e56e0d16efcfbeaf44""><code>48fe71e</code></a> GHSL-2021-1024</li>
<li>Additional commits viewable in <a href=""https://github.com/jupyter/nbconvert/compare/6.5...6.5.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=nbconvert&package-manager=pip&previous-version=6.5.0&new-version=6.5.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/sfu-db/dataprep/network/alerts).

</details>
"
cyberlaboratories/cyberhubs,"add multihub 1st draft: https://github.com/cyberlaboratories/cyberhubs/pull/3
Description: * the multihub allows to run a web page with multiple jupyterhub servers
with different configurations
* first commit
"
facebook/Ax,"Service API: How to properly load from json?: https://github.com/facebook/Ax/issues/1565
Description: I want to save ax_client to JSON and later load it from another python file to analyze it. The code below
```
from ax.service.ax_client import AxClient, ObjectiveProperties
from ax.utils.notebook.plotting import render

ax_client = AxClient()
total_trials = 50

ax_client.create_experiment(
    name=""drilling_experiment"",
    parameters=[
        {
            ""name"": ""x"",
            ""type"": ""range"",
            ""bounds"": [-5, 5],
            ""value_type"": ""float"",
        },
        {
            ""name"": ""y"",
            ""type"": ""range"",
            ""bounds"": [-5, 5],
            ""value_type"": ""float"",
        },
    ],
    objectives={""obj"": ObjectiveProperties(minimize=True)},
)


def evaluate(parameters):
    x = parameters[""x""]
    y = parameters[""y""]
    return (x**2 + y - 11) ** 2 + (x + y**2 - 7) ** 2


for i in range(total_trials):
    parameters, trial_index = ax_client.get_next_trial()
    print(f""Trial index: {trial_index}"".center(50, ""-""))
    for k, v in parameters.items():
        print(f""{k} = {v}"")
    ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameters))

ax_client.save_to_json_file()
ax_client = AxClient.load_from_json_file()
render(ax_client.get_contour_plot())  # this gives error!
```

gives the following error:
```
raise UnsupportedPlotError(
ax.exceptions.core.UnsupportedPlotError: Could not obtain contour plot of ""obj"" for parameters ""x"" and ""y"", as a model with predictive ability, such as a Gaussian Process, has not yet been trained in the course of this optimization.
```

It works when I directly render within the same script without saving the ax_client.
"
enthought/mayavi,"add jupyter support version: https://github.com/enthought/mayavi/pull/1214
Description: Update document to include an explanation of Jupyter notebook support for issue #441 #415 

"
enthought/mayavi,"Mayavi 2 python shuts down : https://github.com/enthought/mayavi/issues/1210
Description: Hi! I am new to Mayavi. I would like to use it for huge vtk files over 400Mb. I installed via pip mayavi2: Python 3.10, OS: Win10, VTK 9.2.5, mayavi 4.8.1. Everything working in Mayavi except for internal python. Internal Python crashes to desktop, is there any solution? When I load in jupyter mlab causes an error. Thank you.
![mayavi](https://user-images.githubusercontent.com/124265785/219288381-348389ae-2bac-44ca-adbf-d326243e2b29.jpg)

"
enthought/mayavi,"itk backend RuntimeError: Could not process the viewer data: https://github.com/enthought/mayavi/issues/1202
Description: Follow the https://github.com/enthought/mayavi/blob/master/examples/mayavi/mayavi_jupyter.ipynb

![image](https://user-images.githubusercontent.com/28699575/212167636-2465c8ee-e548-4e22-9fb1-c3cf879e03d4.png)


```shell
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
File ~/.local/lib/python3.8/site-packages/IPython/core/formatters.py:920, in IPythonDisplayFormatter.__call__(self, obj)
    918 method = get_real_method(obj, self.print_method)
    919 if method is not None:
--> 920     method()
    921     return True

File ~/anaconda3/envs/WaveletGeneration/lib/python3.8/site-packages/mayavi/tools/notebook.py:31, in _ipython_display_(self)
     22 def _ipython_display_(self):
     23     '''Method attached to Mayavi objects.
     24 
     25     Note that here `self` is the Mayavi object that is going to be
   (...)
     29 
     30     '''
---> 31     return _backend.display(self)

File ~/anaconda3/envs/WaveletGeneration/lib/python3.8/site-packages/mayavi/tools/notebook.py:145, in ITKBackend.display(self, obj)
    137     # Works around bug in released itkwidgets-0.32.1.
    138     # Can remove when this PR is merged and in a release:
    139     # https://github.com/InsightSoftwareConsortium/itkwidgets/pull/438
    140     kw = dict(
    141         actors=actors, geometries=[], geometry_colors=[],
    142         geometry_opacities=[], point_sets=[], point_set_colors=[],
    143         point_set_opacities=[]
    144     )
--> 145     return idisplay(self._view(**kw))
    146 else:
    147     return obj

File ~/anaconda3/envs/WaveletGeneration/lib/python3.8/site-packages/itkwidgets/viewer.py:436, in view(data, **kwargs)
    322 def view(data=None, **kwargs):
    323     """"""View the image and/or point sets.
    324 
    325     Creates and returns an ImJoy plugin ipywidget to visualize an image, and/or
   (...)
    434         properties on the object to change the visualization.
    435     """"""
--> 436     viewer = Viewer(data=data, **kwargs)
    438     return viewer

File ~/anaconda3/envs/WaveletGeneration/lib/python3.8/site-packages/itkwidgets/viewer.py:136, in Viewer.__init__(self, ui_collapsed, rotate, ui, **add_data_kwargs)
    132 def __init__(
    133     self, ui_collapsed=True, rotate=False, ui=""pydata-sphinx"", **add_data_kwargs
    134 ):
    135     input_data = self.input_data(add_data_kwargs)
--> 136     data = self.init_data(input_data)
    137     """"""Create a viewer.""""""
    138     self.viewer_rpc = ViewerRPC(
    139         ui_collapsed=ui_collapsed, rotate=rotate, ui=ui, data=data
    140     )

File ~/anaconda3/envs/WaveletGeneration/lib/python3.8/site-packages/itkwidgets/viewer.py:179, in Viewer.init_data(self, input_data)
    177         result = _get_viewer_point_sets(data)
    178     if result is None:
--> 179         raise RuntimeError(f""Could not process the viewer {input_type}"")
    180     _init_data[key] = result
    181 return _init_data

RuntimeError: Could not process the viewer data

<mayavi.modules.iso_surface.IsoSurface at 0x7f75beffc450>
````
"
InsightSoftwareConsortium/itkwidgets,"BLD: Bump itk-vtk-viewer 14.35.0 -> 14.36.0: https://github.com/InsightSoftwareConsortium/itkwidgets/pull/644
Description: Adds the following changes from itk-vtk-viewer:
- Handle TAKE_SCREENSHOT event: This should hopefully prevent the occasional ""No object bound to API"" error we were seeing in Colab notebooks
- do not clamp the color range

"
InsightSoftwareConsortium/itkwidgets,"More Compare Methods: cyan-magenta and blend: https://github.com/InsightSoftwareConsortium/itkwidgets/pull/642
Description: Adds docs for `compare_images`, support for `image_mix` param, and another example cell in ThinPlateSpline notebook.

Depends on:
https://github.com/Kitware/itk-vtk-viewer/pull/668

TODO
- [ ] Bump itk-vtk-viewer and bootstrap-ui pointers in viewer_config.py

"
InsightSoftwareConsortium/itkwidgets,"The Python module itkwidgets is not a valid nbextension: https://github.com/InsightSoftwareConsortium/itkwidgets/issues/639
Description: itkwidgets viewer is not working in Azure ML Notebook (jupyter)
I did try a solution from https://github.com/InsightSoftwareConsortium/itkwidgets/issues/425#top

jupyter nbextension install --py itkwidgets
jupyter nbextension enable --py itkwidgets

But itkwidgets is not a valid nbextension...

On Azure ML with itkwidgets 1.0a25:

(azureml_py310_sdkv2) azureuser@nc64ast4v3:~$ jupyter nbextension install --py itkwidgets
Traceback (most recent call last):
  File ""/anaconda/envs/azureml_py310_sdkv2/bin/jupyter-nbextension"", line 8, in <module>
    sys.exit(main())
  File ""/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/jupyter_core/application.py"", line 277, in launch_instance
    return super().launch_instance(argv=argv, **kwargs)
  File ""/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/traitlets/config/application.py"", line 985, in launch_instance
    app.start()
  File ""/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/notebook/nbextensions.py"", line 972, in start
    super().start()
  File ""/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/jupyter_core/application.py"", line 266, in start
    self.subapp.start()
  File ""/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/notebook/nbextensions.py"", line 702, in start
    self.install_extensions()
  File ""/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/notebook/nbextensions.py"", line 675, in install_extensions
    full_dests = install(self.extra_args[0],
  File ""/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/notebook/nbextensions.py"", line 203, in install_nbextension_python
    m, nbexts = _get_nbextension_metadata(module)
  File ""/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/notebook/nbextensions.py"", line 1109, in _get_nbextension_metadata
    raise KeyError(
KeyError: 'The Python module itkwidgets is not a valid nbextension, it is missing the `_jupyter_nbextension_paths()` method.'
(azureml_py310_sdkv2) azureuser@nc64ast4v3:~$ conda install -c conda-forge jupyter_contrib_nbextensions
Collecting package metadata (current_repodata.json): done
Solving environment: done


"
InsightSoftwareConsortium/itkwidgets,"Cannot import `itk` after installing `itkwidgets`: https://github.com/InsightSoftwareConsortium/itkwidgets/issues/636
Description: Hi,

I'm having an issue with `itk 5.3.0` which prevents me from using `itkwidgets` (it fails at import), as there is a check in place which fails because `itk` attribute `__version__` does not exist:

```
$ import itkwidgets

...

File /opt/homebrew/Caskroom/miniconda/base/envs/ai_med/lib/python3.9/site-packages/itkwidgets/integrations/itk.py:8
      6     import itk
      7     if not hasattr(itk, '__version__') or version.parse(itk.__version__) < version.parse('5.3.0'):
----> 8       raise RuntimeError('itk 5.3 or newer is required. `pip install itk>=5.3.0`')
      9     HAVE_ITK = True
     10 except ImportError:

RuntimeError: itk 5.3 or newer is required. `pip install itk>=5.3.0`
```

Upon investigating, this looks like an `itkwidgets` issue that seems to break `itk.__version__` attribute upon installation.

I'm on macOS 13.2.1 (M1), and thus cannot install `itk` or `itkwidgets` through `conda`.

Reproducible environment:

```
$ conda create --name itk python=3.9
$ pip install itk   # installs 5.3.0

Successfully installed itk-5.3.0 itk-core-5.3.0 itk-filtering-5.3.0 itk-io-5.3.0 itk-numerics-5.3.0
itk-registration-5.3.0 itk-segmentation-5.3.0 numpy-1.24.2
```

Let's try at this point in the terminal:

```python
$ python
Python 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:38:11)
[Clang 14.0.6 ] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import itk
>>> itk.__version__
'5.3.0'
```

Now let's install `itkwidgets` (here version `0.32.6`):

```
$ pip install itkwidgets
...
Successfully installed Send2Trash-1.8.0 appnope-0.1.3 argon2-cffi-21.3.0 
argon2-cffi-bindings-21.2.0 asttokens-2.2.1 attrs-22.2.0 backcall-0.2.0 
beautifulsoup4-4.12.0 bleach-6.0.0 cffi-1.15.1 colorcet-3.0.1 comm-0.1.2 
contourpy-1.0.7 cycler-0.11.0 debugpy-1.6.6 decorator-5.1.1 defusedxml-0.7.1 
executing-1.2.0 fastjsonschema-2.16.3 fonttools-4.39.2 importlib-metadata-6.1.0 
importlib-resources-5.12.0 ipydatawidgets-4.3.2 ipykernel-6.22.0 ipympl-0.9.3 
ipython-8.11.0 ipython-genutils-0.2.0 ipywidgets-7.7.3 itk-meshtopolydata-0.10.0 
itkwidgets-0.32.6 jedi-0.18.2 jinja2-3.1.2 jsonschema-4.17.3 jupyter-client-8.1.0 
jupyter-core-5.3.0 jupyterlab-pygments-0.2.2 jupyterlab-widgets-1.1.2 
kiwisolver-1.4.4 markupsafe-2.1.2 matplotlib-3.7.1 matplotlib-inline-0.1.6 
mistune-2.0.5 nbclient-0.7.2 nbconvert-7.2.10 nbformat-5.8.0 nest-asyncio-1.5.6 
notebook-6.4.12 packaging-23.0 pandocfilters-1.5.0 param-1.13.0 parso-0.8.3 
pexpect-4.8.0 pickleshare-0.7.5 pillow-9.4.0 platformdirs-3.1.1 prometheus-client-0.16.0 
prompt-toolkit-3.0.38 psutil-5.9.4 ptyprocess-0.7.0 pure-eval-0.2.2 pycparser-2.21 
pyct-0.5.0 pygments-2.14.0 pyparsing-3.0.9 pyrsistent-0.19.3 python-dateutil-2.8.2 
pyzmq-25.0.2 six-1.16.0 soupsieve-2.4 stack-data-0.6.2 terminado-0.17.1 
tinycss2-1.2.1 tornado-6.2 traitlets-5.6.0 traittypes-0.2.1 wcwidth-0.2.6 
webencodings-0.5.1 widgetsnbextension-3.6.2 zipp-3.15.0 zstandard-0.20.0
```

And check again in the Python interpreter:

```python
$ python
Python 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:38:11)
[Clang 14.0.6 ] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import itk
>>> itk.__version__
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'itk' has no attribute '__version__'
```

Uninstalling `itkwidgets` through `pip uninstall itkwidgets` does not solve this at this point.

I've tried installing `itkwidgets` versions 0.32.6, 0.32.5, 0.32.4 with same results., as well as latest `1.0a24`.

"
InsightSoftwareConsortium/itkwidgets,"DOC: Add version query param to JupyterLite URL: https://github.com/InsightSoftwareConsortium/itkwidgets/pull/634
Description: This attempts to add more cache refreshing so the latest version of the notebook / JupyterLite deployment is retrieved.

However, testing locally, it did provide an updated notebook for me with Chrome, npm's http-server.

"
InsightSoftwareConsortium/itkwidgets,"Jupyterlite docs: https://github.com/InsightSoftwareConsortium/itkwidgets/pull/631
Description: None

"
InsightSoftwareConsortium/itkwidgets,"Notebook sequence of views uses only first view ROI: https://github.com/InsightSoftwareConsortium/itkwidgets/issues/630
Description: ## Summary

When attempting to view multiple images in separate cells in a Jupyter Notebook with `itkwidgets.view`, only the first region of interest for the first cell run is used in subsequent displays. Trying to view a larger image after viewing a smaller image will display the entirety of the expected bounds.

The viewer otherwise behaves as normal without warnings or error messages, which makes it seem visibly as if the entirety of the image contains less information than expected. This can critically impact quantitative pipeline evaluation.

## Minimal Reproducible Example

```py
import numpy as np
from itkwidgets import view
import itk

# Create a smaller and larger image with unit spacing
arr = np.zeros([364, 210], dtype=np.float32)
arr[:,:105] = 1

arr2 = np.zeros([1292,500], dtype=np.float32)
arr2[:646,:] = 1

image = itk.image_from_array(arr) # physical bounds are (0,0), (210,364)
image2 = itk.image_from_array(arr2) # physical bounds are (0,0), (500, 1292)

# View smaller image first in a dedicated cell
view(image)

# View larger image second in a dedicated cell
view(image2)
```

Size values are based on issue occurrence when viewing ITKMontage tile data input vs output:

https://github.com/InsightSoftwareConsortium/ITKMontage/tree/master/examples/SampleData_CMUrun2

https://github.com/InsightSoftwareConsortium/ITKMontage/blob/master/examples/SimpleMontage.py

## Expected Result

The full extent of both images are visible in respective notebook cells.

`image2` (expected):

![expected-image-extent](https://user-images.githubusercontent.com/40648863/223213885-d9520cfb-740e-4b32-840a-aad24d262f3e.png)

## Observed Result

`image` is fully visible but `image2` is visible only on the approximate extent (0,0), (210, 364).

`image2` (observed):

![image-extent](https://user-images.githubusercontent.com/40648863/223212907-1044b4ad-012c-41f0-b689-ad55a00be620.png)

## Platforms / Versions

itkwidgets 0.32.6
itk 5.3.0
ipywidgets 7.7.2
ipydatawidgets 4.3.2
notebook 6.1.6

Windows 10
Python 3.8.5

## Additional Notes

- The behavior does not occur when the largest image is the first viewed when the notebook runs.
- I believe I have observed this for both 2D and 3D cases in the past, but do not have a good 3D reference on hand.
- The issue does not occur for all possible image sizes. I was unable to reproduce an example for image sizes of <100 pixel widths. 

cc @thewtex @PaulHax 

"
InsightSoftwareConsortium/itkwidgets,"BUG: Bump JupyterLite itkwidgets package to 1.0a24: https://github.com/InsightSoftwareConsortium/itkwidgets/pull/628
Description: Update embedded wheels to compatible versions.

"
InsightSoftwareConsortium/itkwidgets,"Itkwidgets import fails due to underlying package update: https://github.com/InsightSoftwareConsortium/itkwidgets/issues/625
Description: ## Overview

Installing itkwidgets==0.32.5 with default dependencies and then importing in Python results in an error. This can be traced back to a recent ipydatawidgets update.

`ipydatawidgets` should be pinned to <4.3.3 pending a fix.

## Observed behavior

```
> import itkwidgets
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\tom.birdsong\Anaconda3\envs\venv-itk\lib\site-packages\itkwidgets\__init__.py"", line 13, in <module>
    from .widget_viewer import Viewer, view
  File ""C:\Users\tom.birdsong\Anaconda3\envs\venv-itk\lib\site-packages\itkwidgets\widget_viewer.py"", line 18, in <module>
    from ipydatawidgets import NDArray, array_serialization, shape_constraints
  File ""C:\Users\tom.birdsong\Anaconda3\envs\venv-itk\lib\site-packages\ipydatawidgets\__init__.py"", line 7, in <module>
    from .ndarray import *
  File ""C:\Users\tom.birdsong\Anaconda3\envs\venv-itk\lib\site-packages\ipydatawidgets\ndarray\__init__.py"", line 7, in <module>
    from .media import DataImage
  File ""C:\Users\tom.birdsong\Anaconda3\envs\venv-itk\lib\site-packages\ipydatawidgets\ndarray\media.py"", line 17, in <module>
    class DataImage(DataWidget, DOMWidget):
  File ""C:\Users\tom.birdsong\Anaconda3\envs\venv-itk\lib\site-packages\traitlets\traitlets.py"", line 958, in __init__
    cls.setup_class(classdict)
  File ""C:\Users\tom.birdsong\Anaconda3\envs\venv-itk\lib\site-packages\traitlets\traitlets.py"", line 981, in setup_class
    super().setup_class(classdict)
  File ""C:\Users\tom.birdsong\Anaconda3\envs\venv-itk\lib\site-packages\traitlets\traitlets.py"", line 973, in setup_class
    v.subclass_init(cls)
  File ""C:\Users\tom.birdsong\Anaconda3\envs\venv-itk\lib\site-packages\ipydatawidgets\ndarray\union.py"", line 59, in subclass_init
    cls._instance_inits.append(self.instance_init)
AttributeError: type object 'DataImage' has no attribute '_instance_inits
```

## Versions

itkwidgets 0.32.5
ipydatawidgets 4.3.3

## Additional Notes

xref https://github.com/vidartf/ipydatawidgets/issues/57

This failure impacts ITK external module notebook tests which `pip install itkwidgets` with default dependencies: https://github.com/KitwareMedical/ITKUltrasound/actions/runs/4303961362/jobs/7504327745

"
InsightSoftwareConsortium/itkwidgets,"JupyterLite Hello3DWorld: https://github.com/InsightSoftwareConsortium/itkwidgets/issues/617
Description: If I open:

https://itkwidgets.readthedocs.io/en/latest/_static/retro/notebooks/?path=Hello3DWorld.ipynb

and ""run"" the first cell, I am getting:

```
ValueError: Can't find a pure Python 3 wheel for 'itkwidgets==1.0a18'.
See: https://pyodide.org/en/stable/usage/faq.html#micropip-can-t-find-a-pure-python-wheel
You can use `micropip.install(..., keep_going=True)`to get a list of all packages with missing wheels.
```

(tried to change `itkwidgets` to some other version, but same error or missing pure python dep e.g. `ngff-zarr[dask-image]>=0.1.6`).
Is there a configuration/simple Jupyter Notebook I can run on:

https://jupyterlite.github.io/demo/lab/index.html

? Thanks

"
InsightSoftwareConsortium/itkwidgets,"Failed to import 'itkwidgets': https://github.com/InsightSoftwareConsortium/itkwidgets/issues/615
Description: Hello
I tried `from itkwidgets import view` , but error occurred as follows.

"" AttributeError: type object 'DataImage' has no attribute '_instance_inits' ""
<img width=""525"" alt=""image"" src=""https://user-images.githubusercontent.com/55653250/220026982-6db51af7-c9b5-41bd-a2da-b6c8f934c156.png"">

Using `pip freeze`, I checked my itkwidgets version is 0.25.2.
<img width=""95"" alt=""image"" src=""https://user-images.githubusercontent.com/55653250/220027157-199c7891-f502-4abd-9cd4-93bb0d47b4a3.png"">

I am currently using VisualStudioCode, and still cannot import the package on the JupyterLab also.
Is there any idea how to fix it?

Thanks



"
adamerose/PandasGUI,"[Bug Report]: Gui Is Larger Than Physical Screen When DataFrames Are Loaded: https://github.com/adamerose/PandasGUI/issues/224
Description: **Describe the bug**  
When at least one dataframe is loaded, the minimum size of PandasGui is larger than my physical screen (width). I'm unable to shrink it even when not putting it into fullscreen

**Environment**
OS: Linux laptop 5.10.0-12-amd64 #1 SMP Debian 5.10.103-1 (2022-03-07) x86_64 GNU/Linux
Python: Python 3.9.2
IDE: vs-code

**Package versions**  
TO GET ALL RELEVANT PACKAGE VERSIONS, RUN THIS COMMAND IN BASH AND PASTE THE OUTPUT
pip freeze | grep -i ""pyqt\|pandasgui\|plotly\|ipython\|jupyter\|notebook""

EXAMPLE OUTPUT
```
ipython==8.12.0
pandasgui @ git+https://github.com/adamerose/pandasgui.git@e71a97c5809b4e31d878ae08f06cc0a1ea6dc88f
plotly==5.14.1
PyQt5==5.15.9
PyQt5-Qt5==5.15.2
PyQt5-sip==12.12.0
PyQtWebEngine==5.15.6
PyQtWebEngine-Qt5==5.15.2
```

Here's a screenshot to show what it looks like.
![Screenshot_20230410_231031](https://user-images.githubusercontent.com/120563837/231046485-2fc95726-4c56-4dce-b4e4-f24257e79d87.png)

The code to load the dataframes looks like:
```Python
import pandasgui
# ...
dataframes: dict = get_dataframes(...)
pandasgui.show(**dataframes)
```

The dictionary is structured as:
```Python
{
  ""dataframe_one"": pd.DataFrame,
  ""dataframe_two"": pd.DataFrame,
  ""dataframe_three"": pd.DataFrame,
  # ...
}
```

"
adamerose/PandasGUI,"No APPDATA environment variable in Linux: expected str, bytes or os.PathLike object, not NoneType: https://github.com/adamerose/PandasGUI/issues/220
Description: Trying to run

```python3
from pandasgui import show
```

on Ubuntu Linux fails with:

```
Traceback (most recent call last):
  File ""/home/hfs/usr/bin/pandasgui-file"", line 7, in <module>
    from pandasgui import show
  File ""/opt/mambaforge/envs/pandasgui/lib/python3.11/site-packages/pandasgui/__init__.py"", line 15, in <module>
    from pandasgui.gui import show
  File ""/opt/mambaforge/envs/pandasgui/lib/python3.11/site-packages/pandasgui/gui.py"", line 13, in <module>
    from pandasgui.constants import PANDASGUI_ICON_PATH
  File ""/opt/mambaforge/envs/pandasgui/lib/python3.11/site-packages/pandasgui/constants.py"", line 15, in <module>
    SHORTCUT_PATH = os.path.join(os.getenv('APPDATA'), 'Microsoft/Windows/Start Menu/Programs/PandasGUI.lnk', )
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""<frozen posixpath>"", line 76, in join
```

`APPDATA` is a Windows-specific environment variable.

As a workaround one can set the variable to a dummy value:

```bash
export APPDATA=
```

In general it would be nice if “adding to the start menu” was wrapped in some operating system detection, because it only makes sense on Windows anyway.


**Environment**
OS: Ubuntu 22.04.2 LTS
Python: Python 3.11.0

**Package versions**  
```
ipython @ file:///home/conda/feedstock_root/build_artifacts/ipython_1677617093347/work
jupyter_core @ file:///home/conda/feedstock_root/build_artifacts/jupyter_core_1675109859355/work
pandasgui==0.2.14
plotly @ file:///home/conda/feedstock_root/build_artifacts/plotly_1677271849976/work
PyQt5==5.15.7
PyQt5-sip==12.11.0
PyQtWebEngine==5.15.6
PyQtWebEngine-Qt5==5.15.2
```


"
adamerose/PandasGUI,"UnicodeEncodeError: 'charmap' codec can't encode character '\u25c4' in position 276395: https://github.com/adamerose/PandasGUI/issues/218
Description: **Describe the bug**  
Trying to execute the website example 

```
import pandas as pd
from pandasgui import show
df = pd.DataFrame({'a':[1,2,3], 'b':[4,5,6], 'c':[7,8,9]})
show(df)
```

I am given the error ""UnicodeEncodeError: 'charmap' codec can't encode character '\u25c4' in position 276395"". Others appear to have the same issue, cf. [StackOverflow](https://stackoverflow.com/questions/75139770/pandasgui-to-read-a-dataframe).

**Environment**
OS: Windows 10
Python: 3.9.15, confirmed in 3.10.8
IDE: PyDev

**Package versions**  
```
ipython @ file:///C:/Windows/TEMP/abs_45b5zb1l7q/croots/recipe/ipython_1659529855872/work
ipython-genutils @ file:///tmp/build/80754af9/ipython_genutils_1606773439826/work
jupyter-server @ file:///C:/b/abs_1cfi3__jl8/croot/jupyter_server_1671707636383/work
jupyter_client @ file:///C:/ci/jupyter_client_1661836943389/work
jupyter_core @ file:///C:/b/abs_84df679bho/croot/jupyter_core_1672332237650/work
jupyterlab-pygments @ file:///tmp/build/80754af9/jupyterlab_pygments_1601490720602/work
jupyterlab-widgets @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_widgets_1655961217661/work
notebook @ file:///C:/b/abs_ca13hqvuzw/croot/notebook_1668179888546/work
notebook_shim @ file:///C:/b/abs_ebfczttg6x/croot/notebook-shim_1668160590914/work
pandasgui==0.2.13
plotly==5.13.0
PyQt5-sip @ file:///C:/Windows/Temp/abs_d7gmd2jg8i/croots/recipe/pyqt-split_1659273064801/work/pyqt_sip
```

"
adamerose/PandasGUI,"Pivot Values only accepting single column: https://github.com/adamerose/PandasGUI/issues/216
Description: PLEASE FILL OUT THE TEMPLATE

**Describe the bug**  
when using Pivot under Reshaper, I am unable to drag and drop two columns into values section. Only way i am able to get the view i am looking for is by going to kwargs and creating a list of the two fields i want to sum by

For example my table includes product line, item number, revenue, and units. I want to summarize this by product line

**Environment**
OS: (eg. Windows 10)
Python: (eg. 3.10.7)
IDE: (eg. VS)

**Package versions**  
TO GET ALL RELEVANT PACKAGE VERSIONS, RUN THIS COMMAND IN BASH AND PASTE THE OUTPUT
pip freeze | grep -i ""pyqt\|pandasgui\|plotly\|ipython\|jupyter\|notebook""

EXAMPLE OUTPUT
```
anyio==3.6.2
appdirs==1.4.4
argon2-cffi==21.3.0
argon2-cffi-bindings==21.2.0
astor==0.8.1
asttokens==2.0.8
attrs==22.1.0
backcall==0.2.0
beautifulsoup4==4.11.1
bleach==5.0.1
certifi==2022.9.24
cffi==1.15.1
charset-normalizer==2.1.1
clr-loader==0.1.7
colorama==0.4.5
contourpy==1.0.5
coverage==6.5.0
cryptography==38.0.1
cycler==0.11.0
debugpy==1.6.3
decorator==5.1.1
defusedxml==0.7.1
easygui==0.98.3
entrypoints==0.4
et-xmlfile==1.1.0
exceptiongroup==1.0.4
executing==1.1.0
fastjsonschema==2.16.2
fonttools==4.37.4
greenlet==1.1.3
htmlmin==0.1.12
idna==3.4
ImageHash==4.3.1
iniconfig==1.1.1
install==1.3.5
ipykernel==6.16.0
ipython==8.5.0
ipython-genutils==0.2.0
ipywidgets==8.0.2
jedi==0.18.1
Jinja2==3.1.2
joblib==1.1.1
jsonschema==4.17.0
jupyter-core==4.11.1
jupyter-server==1.23.3
jupyter_client==7.3.5
jupyterlab-pygments==0.2.2
jupyterlab-widgets==3.0.3
kiwisolver==1.4.4
MarkupSafe==2.1.1
matplotlib==3.5.3
matplotlib-inline==0.1.6
missingno==0.5.1
mistune==2.0.4
msal==1.20.0
multimethod==1.8
nbclassic==0.4.8
nbclient==0.7.0
nbconvert==7.2.5
nbformat==5.7.0
nest-asyncio==1.5.5
networkx==2.8.7
notebook==6.5.2
notebook_shim==0.2.2
numpy==1.23.3
Office365-REST-Python-Client==2.3.14
openpyxl==3.0.10
packaging==21.3
pandas==1.4.4
pandas-profiling==3.3.0
pandasgui==0.2.13
pandocfilters==1.5.0
parso==0.8.3
patsy==0.5.3
phik==0.12.2
pickleshare==0.7.5
Pillow==9.2.0
pivottablejs==0.9.0
plotly==5.11.0
pluggy==1.0.0
prometheus-client==0.15.0
prompt-toolkit==3.0.31
psutil==5.9.2
pure-eval==0.2.2
pyarrow==10.0.0
pycparser==2.21
pydantic==1.9.2
Pygments==2.13.0
PyJWT==2.5.0
pynput==1.7.6
pyodbc==4.0.34
pyparsing==3.0.9
PyQt5==5.15.7
PyQt5-Qt5==5.15.2
PyQt5-sip==12.11.0
PyQtWebEngine==5.15.6
PyQtWebEngine-Qt5==5.15.2
pyrsistent==0.19.2
pytest==7.2.0
python-dateutil==2.8.2
python-magic==0.4.6
python-tabular==0.1.2
pythonnet==3.0.0a2
pytz==2022.2.1
PyWavelets==1.4.1
pywin32==304
pywinpty==2.0.9
PyYAML==6.0
pyzmq==24.0.1
qgrid==1.3.1
qtstylish==0.1.5
requests==2.28.1
scipy==1.9.2
seaborn==0.11.2
Send2Trash==1.8.0
six==1.16.0
sniffio==1.3.0
soupsieve==2.3.2.post1
SQLAlchemy==1.4.41
stack-data==0.5.1
statsmodels==0.13.2
tangled-up-in-unicode==0.2.0
tenacity==8.1.0
terminado==0.17.0
tinycss2==1.2.1
tomli==2.0.1
tornado==6.2
tqdm==4.64.1
traitlets==5.4.0
types-cryptography==3.3.23
typing_extensions==4.4.0
urllib3==1.26.12
visions==0.7.5
wcwidth==0.2.5
webencodings==0.5.1
websocket-client==1.4.2
widgetsnbextension==4.0.3
wordcloud==1.8.2.2
XlsxWriter==3.0.3
xmltodict==0.13.0
```


"
adamerose/PandasGUI,"PandasGUI not responding: https://github.com/adamerose/PandasGUI/issues/205
Description: System: Windows 10. Python 3.9.12

Hi. PandasGUI is not responding after appearing in VS code Jupyter notebook (see figure). It works when I do show(data, settings={'block': True}) but the jupyter cell won't stop running even after I close pandasgui. I was able to show the dataframe without problem in jupyter using data.head().

I have no issue using pandasgui with the examples in https://github.com/adamerose/PandasGUI.

Code modified from https://github.com/ultralytics/yolov5/issues/36, ""Simple Example""
import torch
import pathlib
from pandasgui import show

model = torch.hub.load('ultralytics/yolov5', 'yolov5s')
im = pathlib.Path('sample_image.png') 

#Inference
results = model(im)
data = results.pandas().xyxy[0]
data.head()
gui = show(data)

![image](https://user-images.githubusercontent.com/99534415/173220483-5b0e4e87-facb-4a3b-ad38-906b24a3d87b.png)
![sample_image](https://user-images.githubusercontent.com/99534415/173220636-5b76d2d8-a99d-47f1-873c-eac798c22b68.png)

"
adamerose/PandasGUI,"OpenGL error - hard crash - PandasGUI quits or throws warning about : https://github.com/adamerose/PandasGUI/issues/202
Description: When I run the demo on the github page, I get an OpenGL error : 

```
from pandasgui import show
from pandasgui.datasets import pokemon, titanic, all_datasets
show(pokemon, titanic)
show(**all_datasets)

```

It works fine with 1 or 2 datasets but with all of them it throws driver and performance errors.  

ERROR MESSAGE:   

![image](https://user-images.githubusercontent.com/9345711/165178918-31ef1c3e-8b99-4faf-9949-8d6a11c89824.png)

```
>>> show(pokemon, titanic)
PandasGUI INFO — pandasgui.gui — Opening PandasGUI
<pandasgui.gui.PandasGui object at 0x0000017230D42310>
>>> show(**all_datasets)
PandasGUI INFO — pandasgui.gui — Opening PandasGUI
<pandasgui.gui.PandasGui object at 0x00000172306508B0>
>>> ARB::createContext: wglCreateContextAttribsARB() failed (GL error code: 0x0) for format: QSurfaceFormat(version 2.0, options QFlags<QSurfaceFormat::FormatOption>(), depthBufferSize 24, redBufferSize -1, greenBufferSize -1, blueBufferSize -1, alphaBufferSize -1, stencilBufferSize 8, samples 0, swapBehavior QSurfaceFormat::DefaultSwapBehavior, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile  QSurfaceFormat::NoProfile), shared context: 0x30000 (The operation completed successfully.)
GDI::createContext: wglCreateContext failed. (Unknown error 0xc00705aa.)
Unable to create a GL Context.
Failed to create OpenGL context for format QSurfaceFormat(version 2.0, options QFlags<QSurfaceFormat::FormatOption>(), depthBufferSize 24, redBufferSize -1, greenBufferSize -1, blueBufferSize -1, alphaBufferSize -1, stencilBufferSize 8, samples 0, swapBehavior QSurfaceFormat::DefaultSwapBehavior, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile  QSurfaceFormat::NoProfile) .
This is most likely caused by not having the necessary graphics drivers installed.

Install a driver providing OpenGL 2.0 or higher, or, if this is not possible, make sure the ANGLE Open GL ES 2.0 emulation libraries (libEGL.dll, libGLESv2.dll and d3dcompiler_*.dll) are available in the application executable's directory or in a location listed in PATH.
```

I ran it again in a new Python terminal session, and then I selected a smaller dataset and it throws a warning and an ERROR: 

```
>>> from pandasgui import show
>>> show(**all_datasets)
PandasGUI INFO — pandasgui.gui — Opening PandasGUI
<pandasgui.gui.PandasGui object at 0x0000016EE6582820>
>>> [17940:27544:0425/172740.921:ERROR:gles2_cmd_decoder.cc(10941)] [.WebGL-0000025A68386020]PERFORMANCE WARNING: Attribute 0 is disabled. This has significant performance penalty
js: [.WebGL-0000025A68386020]PERFORMANCE WARNING: Attribute 0 is disabled. This has significant performance penalty
```

**Environment**
OS: (eg. Windows 10)
Python: (eg. 3.8.13)
IDE: (eg. Anaconda Console (terminal window)

**Package versions**  
TO GET ALL RELEVANT PACKAGE VERSIONS, RUN THIS COMMAND IN BASH AND PASTE THE OUTPUT
pip freeze | grep -i ""pyqt\|pandasgui\|plotly\|ipython\|jupyter\|notebook""

EXAMPLE OUTPUT
```
(pandasgui2) c:\>pip freeze > pandasGUI_installed_packages_text.txt

(pandasgui2) c:\>type  pandasGUI_installed_packages_text.txt
appdirs==1.4.4
astor==0.8.1
asttokens==2.0.5
backcall==0.2.0
brotlipy==0.7.0
certifi==2021.10.8
cffi @ file:///C:/ci_310/cffi_1642682485096/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
colorama @ file:///tmp/build/80754af9/colorama_1607707115595/work
conda==4.12.0
conda-package-handling @ file:///C:/ci/conda-package-handling_1649105961774/work
cryptography @ file:///C:/ci/cryptography_1639472366776/work
cycler==0.11.0
decorator==5.1.1
executing==0.8.3
fonttools==4.33.2
idna @ file:///tmp/build/80754af9/idna_1637925883363/work
ipython==8.2.0
jedi==0.18.1
kiwisolver==1.4.2
matplotlib==3.5.1
matplotlib-inline==0.1.3
menuinst @ file:///C:/ci/menuinst_1631733428175/work
numpy==1.22.3
packaging==21.3
pandas==1.4.2
pandasgui==0.2.13
parso==0.8.3
pickleshare==0.7.5
Pillow==9.1.0
plotly==5.7.0
prompt-toolkit==3.0.29
pure-eval==0.2.2
pyarrow==7.0.0
pycosat==0.6.3
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
Pygments==2.12.0
pynput==1.7.6
pyOpenSSL @ file:///opt/conda/conda-bld/pyopenssl_1643788558760/work
pyparsing==3.0.8
PyQt5==5.15.6
PyQt5-Qt5==5.15.2
PyQt5-sip==12.10.1
PyQtWebEngine==5.15.5
PyQtWebEngine-Qt5==5.15.2
PySocks @ file:///C:/ci/pysocks_1605287845585/work
python-dateutil==2.8.2
pytz==2022.1
pywin32==302
qtstylish==0.1.5
requests @ file:///opt/conda/conda-bld/requests_1641824580448/work
ruamel-yaml-conda @ file:///C:/ci/ruamel_yaml_1616016967756/work
six==1.16.0
stack-data==0.2.0
tenacity==8.0.1
tqdm @ file:///C:/ci/tqdm_1650618053064/work
traitlets==5.1.1
typing_extensions==4.2.0
urllib3 @ file:///C:/ci/urllib3_1650640043075/work
wcwidth==0.2.5
win-inet-pton @ file:///C:/ci/win_inet_pton_1605306167264/work
wincertstore==0.2
wordcloud==1.8.1

```


"
adamerose/PandasGUI,"Automatic script when a file is dragged&droped: https://github.com/adamerose/PandasGUI/issues/194
Description: Hi,

I was wondering if it would be possible to run a short python script each time a file is dragged and dropped. Basically now I am loading a json file as a dataframe in a jupyter notebook, applying some filtering and conversion and then I open PandasGUI with this dataframe. This works very well but I wanted to know if I could include this small script somewhere to do it automatically when someone does drag and drop a file in PandasGUI?

Thanks again for this really nice project!
"
cbouy/mols2grid,"Upcoming features 🚀setHoverable for atoms name: https://github.com/cbouy/mols2grid/issues/36
Description: import py3Dmol
v = py3Dmol.view(query=""pdb:1ubq"",style={'cartoon':{'color':'spectrum'},'stick':{}})
v.setHoverable({},True,'''function(atom,viewer,event,container) {
                   if(!atom.label) {
                    atom.label = viewer.addLabel(atom.resn+"":""+atom.atom,{position: atom, backgroundColor: 'mintcream', fontColor:'black'});
                   }}''',
               '''function(atom,viewer) { 
                   if(atom.label) {
                    viewer.removeLabel(atom.label);
                    delete atom.label;
                   }
                }''')



### Discussed in https://github.com/cbouy/mols2grid/discussions/5

<div type='discussions-op-text'>

<sup>Originally posted by **cbouy** March 24, 2021</sup>
This is a list of upcoming features for mols2grid 

**🙏 Contributions are very much welcome 🙏**


## JupyterLab compatibility

Needs to refactor the grid as an [ipywidget](https://ipywidgets.readthedocs.io/en/stable/developer_docs.html) instead of an interactive HTML page

## Better text search

The current text search is quite buggy as it escapes some regular expressions characters in the query without actually performing a regex search (see [list.js issue](https://github.com/javve/list.js/issues/699)) . As a consequence you can't search text containing `-` or `#` for example.
It's also not possible to exclude words from the search.
It would be great to 1) fix the text search 2) allow some more complex search on specific fields to be performed (i.e. a proper query system with AND, OR, NOT...etc)
</div>

"
cbouy/mols2grid,"Inuiry regaring filtering in saved html & images in tooltip.: https://github.com/cbouy/mols2grid/issues/35
Description: Thank you for creating this really nifty tool for generation of html reports of chemical data, it has been really helpful for exporting results to other non-computational scientists to view results.

I wanted to ask two things regarding the current capabilities of `mols2grid` and if these were possible:

1. In the tooltip, is it possible to render the image of a particular feature of the molecule? The image I am trying to render is a rdkit generated pillow image of the bemis murcko scaffold of the molecule so that viewers can easily see the scaffold of a molecule of interest. Currently just adding a column from a `pandas.DataFrame` containing pillow objects of those images just shows up blank in the html report so I was wondering if this feature was supported and if there are any extra steps I need to do for it to work?

2. For the filtering sliders demonstrated in the collab notebooks, are there any ways to include this into the saved html report? I was trying to allow the chemists to do some filtering based on frequency of bemis murcko scaffold to find those molecules which occur the most.

Thank you and I look forward to your reply. 😊

"
cbouy/mols2grid,"Refactor as a widget: https://github.com/cbouy/mols2grid/pull/33
Description: Refactors mols2grid for better compatibility with Jupyter Lab and VSCode. Also gets rid of the slightly different implementation between Colab and Jupyter.

"
cbouy/mols2grid,"v0.2.0 update: https://github.com/cbouy/mols2grid/pull/21
Description: ### Added
* `cache_selection=True` allows to retrieve the checkbox state when re-displaying a grid, as long as they have the same name. Fixes #22
* `prerender=False` moves the rendering of molecule images from Python to the browser and only when the molecule is on the current page, giving a performance boost and allowing to process much larger files. Fixes #17
* `substruct_highlight=True` highlight the atoms that matched the substructure query when using the SMARTS search (only available when `prerender=False`). Fixes #18 
* Added CSV (tab separated) save option. Exports all the data present in `subset` and `tooltip` for the current selection
* Support for `.sdf.gz` files
* Added automated tests of the interface, which should prevent future updates from breaking things.

### Changed
* Bumped the minimum supported version of Python from 3.6 to 3.7
* Molecule images are now generated by the web browser (see `prerender=False` argument)
* The coordinates of the input file are now ignored by default (`use_coords=False`). This change was made to comply with generating images from SMILES string with the browser by default.
* Python callbacks are now automatically registered in Google Colab
* Javascript callbacks can access RDKit as either `RDKit` or `RDKitModule`
* The ""img"" field is now available from the callback data
* The `subset` parameter now throws an error if ""img"" is not present
* Clicking ""Check all""/""Uncheck all"" should now be faster
* Bumped RDKit JS version to `2021.9.4` to better support moldrawoptions
* Installation now requires `jinja2>=2.11.0` to prevent an error when given a `pathlib.Path` object instead of a string

### Fixed
* Callbacks now work when `selection=False`. Fixes #22
* Using both `transform` and `style` should now display the labels as expected in the tooltip
* Fixed a race condition when clicking checkboxes on different grids
* Fixed the `gap` argument not being properly taken into account
* Automatic resizing of the iframe (used in `mols2Grid.display`) should now work even better

TODO:
- [x] Automate GUI testing with selenium
- [x] Update CHANGELOG

"
cbouy/mols2grid,"MolGrid.get_selection() always returns empty dict: https://github.com/cbouy/mols2grid/issues/20
Description: Hi cbouy, 

Thanks for making this tool available - it's great! 

I'm trying out the code (with jupyter notebook) and I've observed that after selecting a molecule entry in the `mol2grid.MolGrid` I am not able to retrieve the selection through `mol2grid.MolGrid.get_selection()` - an empty dict is returned irrespective of the selection. I have looked through your google colab examples too (RDkit UGM and solubility examples) and see the same behaviour so I think there's an issue within mol2grid.

Thanks in advance,
Pete

"
cbouy/mols2grid,"mol2grid.display slow for a large number of compounds: https://github.com/cbouy/mols2grid/issues/17
Description: I applied `mol2grid.display` to a `pd.DataFrame` storing the [VEHICLe](https://chembl.blogspot.com/2010/04/vehicle-virtual-exploratory.html) data set, which contains  ~25k samples, and the call does not seem to terminate after a reasonable time (for a 6x6 grid). I tried to stop the Jupyter cell execution after ~600s but did not work and I had to kill the entire notebook.

---

Offline discussion with @cbouy : the bottleneck is probabluy image generation for the whole data set and it would be nice to have the option to perform image generation on page load.

"
cbouy/mols2grid,"Selection Option for Streamlit: https://github.com/cbouy/mols2grid/issues/16
Description: Hello, thanks a lot for sharing your project!

The `README` says that the checkbox option is relevant only in the context of Jupyter Notebook. Is there no way of extracting the selection through Streamlit? 

Thank you in advance and apologies for the trivial question.


"
tensorflow/model-analysis,"Fix jupiter integration: https://github.com/tensorflow/model-analysis/pull/165
Description: I'm not sure where the configMap comes from, but at least in jupiter-lab this causes the widget to not display (and error out with `ReferenceError: configMap is not defined`).

Also not sure how to update the build since the build resources seems to be included in the repository (tested by using ` jupyter lab build --dev-build=True --minimize=False` and modifying the static generated js file since all of this jupiter extensions are new to me).

"
tensorflow/model-analysis,"TFMA analyze_raw_data function support with MultiClassConfusionMatrixPlot: https://github.com/tensorflow/model-analysis/issues/162
Description: ### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow Model Analysis)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04.2 LTS Linux  5.4.0-65-generic
-   **TensorFlow Model Analysis installed from (source or binary)**: `pip install tensorflow-model-analysis `
-   **TensorFlow Model Analysis version (use command below)**: 0.41.1
-   **Python version**: Python 3.8.5
-   **Jupyter Notebook version**: NA

### Describe the problem

I am currently trying to get `tfma.analyze_raw_data` to work with `MultiClassConfusionMatrixPlot` which has multiple prediction values per record. Is this not supported? I will be happy to provide any further details or run any further tests.

#### Details

Currently `tfma.analyze_raw_data` does not seem to work with metrics for multi classification tasks (e.g. `tfma.metrics.MultiClassConfusionMatrixPlot`). However, I do not see this limitation documented anywhere.

The prediction column for a multi classification column will be a series of whose values are a list or array (e.g.,. `pd.DataFrame({'predictions': [[0.2, .3, .5]], 'label': [1]})`)

The `tfma.analyze_raw_data` funciton uses  `tfx_bsl.arrow.DataFrameToRecordBatch` to convert a Pandas DataFrame to Arrow RecordBatch. The problem, however, is that it encodes columns with the dtype of `object` as a `pyarrow.Binary`. Since a column that has lists or arrays as values has a dtype of `object`, these columns are being encoded as a `pyarrow.Binary` instead of the relevant pyarrow list-like type.

### Source code / logs

```
import tensorflow_model_analysis as tfma
from google.protobuf import text_format
import pandas as pd

eval_config = text_format.Parse(""""""
  ## Model information
  model_specs {
    label_key: ""label"",
    prediction_key: ""predictions""
  }

  ## Post training metric information. These will be merged with any built-in
  ## metrics from training.
  metrics_specs {
    metrics { class_name: ""MultiClassConfusionMatrixPlot"" }
  }
  
  ## Slicing information
  slicing_specs {}  # overall slice
"""""", tfma.EvalConfig())

df = pd.DataFrame({'predictions': [[0.2, .3, .5]], 'label': [1]})
tfma.analyze_raw_data(df, eval_config)
```
#### Error

```
---------------------------------------------------------------------------
ArrowTypeError                            Traceback (most recent call last)
/tmp/ipykernel_206830/3947320198.py in <cell line: 23>()
     21 
     22 df = pd.DataFrame({'predictions': [[0.2, .3, .5]], 'label': [1]})
---> 23 tfma.analyze_raw_data(df, eval_config)

/localdisk/twilbers/src/repos/xai-tools/model_card_gen/.venv/lib/python3.8/site-packages/tensorflow_model_analysis/api/model_eval_lib.py in analyze_raw_data(data, eval_config, output_path, add_metric_callbacks)
   1511 
   1512   arrow_data = table_util.CanonicalizeRecordBatch(
-> 1513       table_util.DataFrameToRecordBatch(data))
   1514   beam_data = beam.Create([arrow_data])
   1515 

/localdisk/twilbers/src/repos/xai-tools/model_card_gen/.venv/lib/python3.8/site-packages/tfx_bsl/arrow/table_util.py in DataFrameToRecordBatch(dataframe)
    122       continue
    123     arrow_fields.append(pa.field(col_name, arrow_type))
--> 124   return pa.RecordBatch.from_pandas(dataframe, schema=pa.schema(arrow_fields))
    125 
    126 

/localdisk/twilbers/src/repos/xai-tools/model_card_gen/.venv/lib/python3.8/site-packages/pyarrow/table.pxi in pyarrow.lib.RecordBatch.from_pandas()

/localdisk/twilbers/src/repos/xai-tools/model_card_gen/.venv/lib/python3.8/site-packages/pyarrow/pandas_compat.py in dataframe_to_arrays(df, schema, preserve_index, nthreads, columns, safe)
    592 
    593     if nthreads == 1:
--> 594         arrays = [convert_column(c, f)
    595                   for c, f in zip(columns_to_convert, convert_fields)]
    596     else:

/localdisk/twilbers/src/repos/xai-tools/model_card_gen/.venv/lib/python3.8/site-packages/pyarrow/pandas_compat.py in <listcomp>(.0)
    592 
    593     if nthreads == 1:
--> 594         arrays = [convert_column(c, f)
    595                   for c, f in zip(columns_to_convert, convert_fields)]
    596     else:

/localdisk/twilbers/src/repos/xai-tools/model_card_gen/.venv/lib/python3.8/site-packages/pyarrow/pandas_compat.py in convert_column(col, field)
    579             e.args += (""Conversion failed for column {!s} with type {!s}""
    580                        .format(col.name, col.dtype),)
--> 581             raise e
    582         if not field_nullable and result.null_count > 0:
    583             raise ValueError(""Field {} was non-nullable but pandas column ""

/localdisk/twilbers/src/repos/xai-tools/model_card_gen/.venv/lib/python3.8/site-packages/pyarrow/pandas_compat.py in convert_column(col, field)
    573 
    574         try:
--> 575             result = pa.array(col, type=type_, from_pandas=True, safe=safe)
    576         except (pa.ArrowInvalid,
    577                 pa.ArrowNotImplementedError,

/localdisk/twilbers/src/repos/xai-tools/model_card_gen/.venv/lib/python3.8/site-packages/pyarrow/array.pxi in pyarrow.lib.array()

/localdisk/twilbers/src/repos/xai-tools/model_card_gen/.venv/lib/python3.8/site-packages/pyarrow/array.pxi in pyarrow.lib._ndarray_to_array()

/localdisk/twilbers/src/repos/xai-tools/model_card_gen/.venv/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowTypeError: (""Expected bytes, got a 'list' object"", 'Conversion failed for column predictions with type object')
```

#### Temporary fix

If I change/patch `tfx_bsl.arrow.DataFrameToRecordBatch` as follows, it seems to work, but I doubt this is a solution.

```
def DataFrameToRecordBatch(dataframe):
    arrays = []
    for col_name, col_type in zip(dataframe.columns, dataframe.dtypes):
        arrow_type = None
        if col_type.kind != 'O':
            arrow_type = NumpyKindToArrowType(col_type.kind)
        arrays.append(pa.array(dataframe[col_name].values.tolist(), type=arrow_type))
    return pa.RecordBatch.from_arrays(arrays,  names=dataframe.columns)
```




"
tensorflow/model-analysis,"Error in merge_accumulators when using keras metrics on dataflow: https://github.com/tensorflow/model-analysis/issues/158
Description: ### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow Model Analysis)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: GCP Dataflow Apache Beam Python 3.7 SDK 2.39.0
-   **TensorFlow Model Analysis installed from (source or binary)**: binary
-   **TensorFlow Model Analysis version (use command below)**: 0.33
-   **Python version**: 3.7
-   **Jupyter Notebook version**: Jupyter lab 3.2.8
-   **Exact command to reproduce**: 

I am using TFX's evaluator
```python
eval_config = tfma.EvalConfig(
  model_specs=model_specs,
  metrics_specs=tfma.metrics.specs_from_metrics([
      tf.keras.metrics.AUC(curve='ROC', name='ROCAUC'),
      tf.keras.metrics.AUC(curve='PR', name='PRAUC'),
      tf.keras.metrics.Precision(),
      tf.keras.metrics.Recall(),
      tf.keras.metrics.BinaryAccuracy(),
    ]),
  slicing_specs=slicing_specs
)

evaluator = Evaluator(
  eval_config=eval_config,
  model=model,
  examples=transform_examples,
)

context.run(evaluator)
```

### Describe the problem

Running the same evaluation using Beam's DirectRunner locally will not cause any error, but whenever I run it on dataflow and when dataflow spawns more than one worker, I get an error like so:

> output.with_value(self.phased_combine_fn.apply(output.value)): File ""/usr/local/lib/python3.7/site-packages/apache_beam/transforms/combiners.py"", line 882, in merge_only return self.combine_fn.merge_accumulators(accumulators) File ""/home/sandbox/.pex/install/apache_beam-2.39.0-cp37-cp37m-linux_x86_64.whl.06f7ceb62380d1c704d774a5096a04f953de60c9/apache_beam-2.39.0-cp37-cp37m-linux_x86_64.whl/apache_beam/transforms/combiners.py"", line 665, in merge_accumulators a in zip(self._combiners, zip(*accumulators_batch)) File ""/home/sandbox/.pex/install/apache_beam-2.39.0-cp37-cp37m-linux_x86_64.whl.06f7ceb62380d1c704d774a5096a04f953de60c9/apache_beam-2.39.0-cp37-cp37m-linux_x86_64.whl/apache_beam/transforms/combiners.py"", line 665, in <listcomp> a in zip(self._combiners, zip(*accumulators_batch)) File ""/usr/local/lib/python3.7/site-packages/tensorflow_model_analysis/metrics/tf_metric_wrapper.py"", line 560, in merge_accumulators for metric_index in range(len(self._metrics[output_name])): TypeError: 'NoneType' object is not subscriptable

Based on the dataflow log, the failing steps were:
- ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PreCombineFn)/Combine
- ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PreCombineFn)/GroupByKey
- ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PostCombineFn)/GroupByKey

I see that you have this [commit](https://github.com/tensorflow/model-analysis/commit/0e578d9659a458d1ae12b77fdf8c7f2869d5d2c6), which appears to be addressing this problem, but it is immediately rolled back. I wonder if you have had similar issues and what would you recommend to fix the error.

"
tensorflow/model-analysis,"Renaming Custom Layer breaks TFMA Evaluator: https://github.com/tensorflow/model-analysis/issues/154
Description: ### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow Model Analysis)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: GCP AI Vertex Workbench Debian 10
-   **TensorFlow Model Analysis installed from (source or binary)**:  PyPI binary
-   **TensorFlow Model Analysis version (use command below)**: 0.26.0
-   **Python version**: 3.7
-   **Jupyter Notebook version**: n/a
-   **Exact command to reproduce**:

You can obtain the TensorFlow Model Analysis version with

`python -c ""import tensorflow_model_analysis as tfma; print(tfma.version.VERSION)""`

### Describe the problem

I have a custom layer named MultiHeadAttention layer and when I ran the tfx pipeline, it shows a warning that it has a conflict with the default MultiHeadAttention layer and that I should rename the layer something else. When I rename it to CustomMultiHeadAttention, it suddenly breaks the tfx pipeline particularly in the evaluator component. When I don't change anything else in the code except reverting it back to the name ""MultiHeadAttention"" the evaluator component runs okay but the problem is that when trying to export the model/saving and loading, I'm also having some problems. What is the cause of this or is it a bug in tfma/tfx?

### Source code / logs
**Error when changing Custom Layer name from MultiHeadAttention -> CustomMultiHeadAttention**
![Screenshot from 2022-04-07 10-48-31](https://user-images.githubusercontent.com/15419597/162599530-8674122b-1393-4bdb-8521-45137a28f704.png)

**eval_config.py**

```
import tensorflow_model_analysis as tfma

def set_eval_config() -> tfma.EvalConfig:

    eval_config = tfma.EvalConfig(
        model_specs=[
            tfma.ModelSpec(
                name=""accent_model"",
                signature_name=""serving_evaluator"",
                label_key=""accent"",
                prediction_key=""accent_prediction"",
            ),
            tfma.ModelSpec(
                name=""phones_model"",
                signature_name=""serving_evaluator"",
                label_key=""target_phones"",
                prediction_key=""phone_predictions"",
            ),
        ],
        metrics_specs=[
            tfma.MetricsSpec(
                output_names=[""accent_prediction""],
                model_names=[""accent_model""],
                metrics=[
                    tfma.MetricConfig(
                        class_name=""AccentAccuracy"",
                        module=""aped.mlops.pipeline.metrics"",
                    ),
                ],
            ),
            tfma.MetricsSpec(
                output_names=[""phone_predictions""],
                model_names=[""phones_model""],
                metrics=[
                    tfma.MetricConfig(
                        class_name=""PhoneASRAccuracy"",
                        module=""aped.mlops.pipeline.metrics"",
                        threshold=tfma.MetricThreshold(
                            value_threshold=tfma.GenericValueThreshold(lower_bound={""value"": 0.01}),
                            change_threshold=tfma.GenericChangeThreshold(
                                direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                                absolute={""value"": -1e-10},
                            ),
                        ),
                    ),
                    tfma.MetricConfig(
                        class_name=""PhoneErrorRate"",
                        module=""aped.mlops.pipeline.metrics"",
                    ),
                    tfma.MetricConfig(
                        class_name=""PhonesPrecision"",
                        module=""aped.mlops.pipeline.metrics"",
                    ),
                    tfma.MetricConfig(
                        class_name=""PhonesRecall"",
                        module=""aped.mlops.pipeline.metrics"",
                    ),
                    tfma.MetricConfig(
                        class_name=""PhonesF1Score"",
                        module=""aped.mlops.pipeline.metrics"",
                    ),
                    tfma.MetricConfig(class_name=""ExampleCount""),
                    tfma.MetricConfig(class_name=""SparseCategoricalCrossentropy""),
                ],
            ),
        ],
        slicing_specs=[
            tfma.SlicingSpec(),
            tfma.SlicingSpec(feature_keys=[""accent""]),
            tfma.SlicingSpec(feature_keys=[""recording_length""]),
            tfma.SlicingSpec(feature_keys=[""age""]),
            tfma.SlicingSpec(feature_keys=[""gender""]),
            tfma.SlicingSpec(feature_keys=[""bg_noise_type""]),
            tfma.SlicingSpec(feature_keys=[""bg_noise_level""]),
            tfma.SlicingSpec(feature_keys=[""english_level""]),
        ],
    )

    return eval_config
```

**code snippet for evaluator component in tfx pipeline**
```  
evaluator = tfx.components.Evaluator(
        examples=transform.outputs[""transformed_examples""],
        model=trainer.outputs[""model""],
        # baseline_model=model_resolver.outputs['model'],
        eval_config=eval_config,
        example_splits=[""eval""],
    )
```

**multihead attention layer declaration snippet**
```
class MultiHeadAttention(tf.keras.layers.Layer):
    """"""MultiHeadAttention Custom Layer""""""

    def __init__(self, d_model: int, num_heads: int, dropout_rate: float, mixed_precision: bool = False) -> None:
        """"""Initialise the MultiHeadAttention Layer

        Args:
            d_model (int): Attention  modelling  dimension
            num_heads (int): Number of attention heads
            mixed_precision (bool, optional): True if the layer needs to handle mixed precision
            with float16. Defaults to False
        """"""
        super().__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        self.dropout_rate = dropout_rate
        self.mixed_precision = mixed_precision

        assert d_model % self.num_heads == 0

        self.depth = d_model // self.num_heads

        init = tf.keras.initializers.RandomNormal(mean=0, stddev=np.sqrt(2.0 / (d_model + self.depth)))

        self.wq = tf.keras.layers.Dense(d_model, kernel_initializer=init)
        self.wk = tf.keras.layers.Dense(d_model, kernel_initializer=init)
        self.wv = tf.keras.layers.Dense(d_model, kernel_initializer=init)

        self.dense = tf.keras.layers.Dense(d_model, kernel_initializer=""glorot_normal"")
```

"
tensorflow/model-analysis,"No longer compatible with Dataflow runner v1 after v0.29: https://github.com/tensorflow/model-analysis/issues/153
Description: Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow-model-analysis

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.

**Here's why we have that policy**: TensorFlow Model Analysis developers respond
to issues. We want to focus on work that benefits the whole community, e.g.,
fixing bugs and adding features. Support only helps individuals. GitHub also
notifies thousands of people when issues are filed. We want them to see you
communicating an interesting problem, rather than being redirected to Stack
Overflow.

--------------------------------------------------------------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow Model Analysis)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **TensorFlow Model Analysis installed from (source or binary)**: from pip
-   **TensorFlow Model Analysis version (use command below)**: 0.29.0
-   **Python version**: 3.7
-   **Jupyter Notebook version**:
-   **Exact command to reproduce**:

You can obtain the TensorFlow Model Analysis version with

`python -c ""import tensorflow_model_analysis as tfma; print(tfma.version.VERSION)""`

### Describe the problem

Recently, I've been working on upgrading our ML model R&D stack to use more up-to-date versions of TF & TFX libraries. Before this, we were using TF==2.3.2 and TFMA==0.26.1. While going through this exercise, I noticed that in the upgraded version of TFMA, we started getting the following errors when we run our TFMA job with DataflowRunner, but not when we run it with DirectRunner:
```
DataflowRuntimeException: Dataflow pipeline failed. State: FAILED, Error:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/dataflow_worker/batchworker.py"", line 644, in do_work
    work_executor.execute()
  File ""/usr/local/lib/python3.7/site-packages/dataflow_worker/executor.py"", line 208, in execute
    op.start()
  File ""dataflow_worker/shuffle_operations.py"", line 63, in dataflow_worker.shuffle_operations.GroupedShuffleReadOperation.start
  File ""dataflow_worker/shuffle_operations.py"", line 64, in dataflow_worker.shuffle_operations.GroupedShuffleReadOperation.start
  File ""dataflow_worker/shuffle_operations.py"", line 79, in dataflow_worker.shuffle_operations.GroupedShuffleReadOperation.start
  File ""dataflow_worker/shuffle_operations.py"", line 80, in dataflow_worker.shuffle_operations.GroupedShuffleReadOperation.start
  File ""dataflow_worker/shuffle_operations.py"", line 84, in dataflow_worker.shuffle_operations.GroupedShuffleReadOperation.start
  File ""apache_beam/runners/worker/operations.py"", line 348, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File ""dataflow_worker/shuffle_operations.py"", line 261, in dataflow_worker.shuffle_operations.BatchGroupAlsoByWindowsOperation.process
  File ""dataflow_worker/shuffle_operations.py"", line 267, in dataflow_worker.shuffle_operations.BatchGroupAlsoByWindowsOperation.process
  File ""/usr/local/lib/python3.7/site-packages/apache_beam/transforms/trigger.py"", line 1324, in process_elements
    yield output.with_value(self.phased_combine_fn.apply(output.value))
  File ""/usr/local/lib/python3.7/site-packages/apache_beam/transforms/combiners.py"", line 876, in merge_only
    return self.combine_fn.merge_accumulators(accumulators)
  File ""/usr/local/lib/python3.7/dist-packages/apache_beam/transforms/combiners.py"", line 659, in merge_accumulators
    a in zip(self._combiners, zip(*accumulators_batch))
  File ""/usr/local/lib/python3.7/dist-packages/apache_beam/transforms/combiners.py"", line 659, in <listcomp>
    a in zip(self._combiners, zip(*accumulators_batch))
  File ""/root/.local/lib/python3.7/site-packages/tensorflow_model_analysis/metrics/tf_metric_wrapper.py"", line 562, in merge_accumulators
    for metric_index in range(len(self._metrics[output_name])):
TypeError: 'NoneType' object is not subscriptable
```

Since we skipped multiple releases and I didn't spot anything notable enough in the release notes to cause this, I decided to try each one out to see when the issue started to occur and it was at v0.29. Upon deeper investigation I found that there was [this patch](https://github.com/tensorflow/model-analysis/commit/63ae2cb774853312e963828e61acd2a7d6ca7d2b#diff-85872485766314a4556f4240be002052ac974e9bd940f144889f21e8e817874bR555) which was not included in the release notes. In this patch, TFMA moved from calling its own `_setup()` function to using Beam's `beam.CombineFn.setup()`. However, looking at [Beam's documentation](https://beam.apache.org/releases/pydoc/2.35.0/_modules/apache_beam/runners/dataflow/dataflow_runner.html), `beam.CombineFn.setup()` is only supported in Dataflow Runner v2, making TFMA no longer compatible with Dataflow Runner v1. Retrying with `use_runner_v2` indeed fixed our issue. 

Is this component general to all TFMA runs? If so, it should perhaps try to detect if people are using Dataflow runner v1 and throw an exception and document this compatibility issue.

### Source code / logs

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached. Try to provide a reproducible test case that is the bare
minimum necessary to generate the problem.

"
mwouts/itables,"Column width miss-match in notebook vs jupyter-book: https://github.com/mwouts/itables/issues/175
Description: Hey @mwouts 

I noticed a miss match in column size between my notebook and the jupyter-page of the notebook. Have you come across this issue before?

Screenshot from notebook
![Screenshot from 2023-03-29 13-16-31](https://user-images.githubusercontent.com/15642823/228395233-8e4a7476-3909-4d6b-b7ee-3c081cd62d74.png)

Screenshot from jupyter-book
![Screenshot from 2023-03-29 13-24-52](https://user-images.githubusercontent.com/15642823/228395656-b20ecad1-0a77-4492-ac5f-49bc7debce0f.png)




"
mwouts/itables,"LaTeX/Mathjax example: https://github.com/mwouts/itables/issues/174
Description: Hey @mwouts , me again 😅 

I was wondering -- do you have any suggestions on rendering latex / mathjax in tables?

I followed the suggestion proposed in this stack-overflow: https://stackoverflow.com/questions/35864810/mathjax-render-only-on-first-page-of-the-data-table 

Here is what I was testing out:

```python

import pandas as pd
from itables import init_notebook_mode, show
import itables.options as opt


opt.css = """"""
$('#example').DataTable( { ""drawCallback"": function( settings ) { MathJax.Hub.Queue([""Typeset"",MathJax.Hub]); } } );
""""""


init_notebook_mode(all_interactive=True)


df = pd.DataFrame({
    ""$N_{\\text{event}}$"": [""$\\alpha$"", ""$\\beta$"", ""$\\gamma$""]*10,
    ""Value"": [""$0.8_{-0.1}^{+0.3}$"",""$3.2_{-0.4}^{+0.2}$"", ""$-0.1_{-0.5}^{+0.8}$""]*10
})

show(df)
```
![Screenshot from 2023-03-28 20-02-34](https://user-images.githubusercontent.com/15642823/228155092-fff0f8d9-4772-4f77-a5ca-11eff0db0909.png)


"
mwouts/itables,"Version 1.5.0: https://github.com/mwouts/itables/pull/165
Description: **Fixed**
- We have addressed the `window.initializeDataTable` is not a function error when a notebook is reloaded
([#160](https://github.com/mwouts/itables/issues/160), [#163](https://github.com/mwouts/itables/issues/163)). Many thanks again to [François Wouts](https://github.com/fwouts) for providing the right fix!

**Added**
- Polars DataFrames are supported ([#159](https://github.com/mwouts/itables/issues/159))
- We have added an example to show how to include images in tables ([#158](https://github.com/mwouts/itables/issues/158))
- We have added links and images (flags from https://flagpedia.net) to the sample countries df ([#158](https://github.com/mwouts/itables/issues/158)).

**Changed**
- We have updated the pre-commit hooks

"
mwouts/itables,"fix error ""initializeDataTables is not a function"" when a notebook is reloaded: https://github.com/mwouts/itables/pull/160
Description: Sometimes when a notebook is reloaded from disk, the tables don't load, and in the console I find a message _window.initializeDataTables is not a function_.

This PR intends to postpone the table initialization when this occur, to let some time for the init script to load (that script is located in another section of the HTML document).

@fwouts , do you see a better approach? Ideally I would declare a dependency of (the rendered version of) `datatables_template.html` on (the rendered version of) `initialize_offline_datatable.html` but I have not idea how to do that...

"
mwouts/itables,"Support for Polars Dataframes: https://github.com/mwouts/itables/issues/159
Description: Could you support Polars DataFrames?



```
import polars as pl

from itables import init_notebook_mode
import itables.options as opt
opt.css = """"""
.itables table td { font-style: italic; }
.itables table th { font-style: oblique; }

init_notebook_mode(all_interactive=True,  connected=False)

df = pl.DataFrame({""a"": [1, 2, 3], ""b"": [4, 5, 6]})
df
```


"
mwouts/itables,"Not displaying large numbers (>16 digits) correctly: https://github.com/mwouts/itables/issues/152
Description: The issue: Large numbers only display first 16 digits, 17th digit is rounded (sometimes incorrectly) and the rest are 0s.

How to replicate:
```py
from itables import show
import pandas as pd

a = pd.DataFrame(
    {
        ""Name"": [
            ""Braund, Mr. Owen Harris"",
            ""Allen, Mr. William Henry"",
            ""Bonnell, Miss. Elizabeth"",
        ],
        ""Age"": [1234567890123456789, 2345678901234567890, 3456789012345678901],
        ""Sex"": [""male"", ""male"", ""female""],
    }
)

show(a)
```

Result:


Name | Age | Sex
-- | -- | --
Braund, Mr. Owen Harris | 1234567890123456800 | male
Allen, Mr. William Henry | 2345678901234567700 | male
Bonnell, Miss. Elizabeth | 3456789012345679000 | female

Note that Allen's 17th digit (the second 7 in 77) should be a 9.

Version: 1.4.4

Notebook: Jupyter Notebook
"
altair-viz/altair,"Unable to set width='container' in jupyterlab: https://github.com/altair-viz/altair/issues/3019
Description: Unable to set property width to container as described here: https://altair-viz.github.io/user_guide/customization.html#adjusting-chart-size 

versions:
```
altair==5.0.0rc1
jupyterlab==4.0.0b1
```

```python
alt.Chart(cars).mark_bar().encode(
    x='Origin',
    y='count()',
).properties(
    width='container',
    height=200
)
```

```
Javascript Error: Unrecognized signal name: ""container""
This usually means there's a typo in your chart specification. See the javascript console for the full traceback.
```


![image](https://user-images.githubusercontent.com/31582215/232137475-d144ca08-119c-4476-b012-56185adf4af8.png)


"
altair-viz/altair,"Simplify package mangement: https://github.com/altair-viz/altair/pull/3007
Description: Fix https://github.com/altair-viz/altair/issues/2939.

This PR intends to simplify the package management by moving to a complete pyproject.toml-based build. Meaning we can remove `setup.py`. I've adopted `hatch` as back-end. Reason: eg. jupyter-ecosystem use this. 
I also want to remove `setup.cfg`, meaning we have to upgrade from flake8 to another linter (flake8 has no pyproject.toml support). I've adopted `ruff` as new linter. Reason: eg. pandas and pylint use this.

I tried to use the same settings as our current flake8, but `ruff` finds still 57 errors. What to do with them? Fix in this PR?

Notes during exploration for this PR:

- Test-building the package can be done using `hatch build`. 
- `hatch shell` creates a pipenv based on the dependencies (no need for conda envs). 
- Optional dependencies can be installed using `pip install -e .[dev,doc]` (configured in pyproject.toml)
- I don't know yet what `hatch run` can do for us.

"
altair-viz/altair,"module 'altair' has no attribute 'param': https://github.com/altair-viz/altair/issues/3006
Description: I used to use alt.param() to make selectors for interactive charts, and it's still in the documentation, but now I am getting this error. 

Altair version 4.2.2
Using Google Colab Jupyter Notebook

![image](https://user-images.githubusercontent.com/79999494/228922055-78a74c8b-66ce-4ab5-b649-33a24053b1a9.png)

"
nglviewer/nglview,"nglview run error: https://github.com/nglviewer/nglview/issues/1054
Description: I am trying to run nglview with the following command ""import nglview as nv"" but it show an error.

error::::
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/tmp/ipykernel_8977/1899191847.py in <cell line: 1>()
----> 1 import nglview as nv

~/anaconda3/lib/python3.9/site-packages/nglview/__init__.py in <module>
      2 
      3 # for doc
----> 4 from . import adaptor, datafiles, show, widget
      5 from ._version import get_versions
      6 from .adaptor import *

~/anaconda3/lib/python3.9/site-packages/nglview/show.py in <module>
     11                       RdkitStructure,
     12                       TextStructure)
---> 13 from .widget import NGLWidget
     14 
     15 __all__ = [

~/anaconda3/lib/python3.9/site-packages/nglview/widget.py in <module>
     17 import traitlets
     18 
---> 19 from . import color, interpolate
     20 from .adaptor import Structure, Trajectory
     21 from .component import ComponentViewer

~/anaconda3/lib/python3.9/site-packages/nglview/color.py in <module>
    112 
    113 
--> 114 ColormakerRegistry = _ColormakerRegistry()

~/anaconda3/lib/python3.9/site-packages/nglview/base.py in getinstance()
      8     def getinstance():
      9         if cls not in instances:
---> 10             instances[cls] = cls()
     11         return instances[cls]
     12     return getinstance

~/anaconda3/lib/python3.9/site-packages/nglview/color.py in __init__(self, *args, **kwargs)
     45         try:
     46             get_ipython() # only display in notebook
---> 47             self._ipython_display_()
     48         except NameError:
     49             pass

~/anaconda3/lib/python3.9/site-packages/nglview/color.py in _ipython_display_(self, **kwargs)
     52         if self._ready:
     53             return
---> 54         super()._ipython_display_(**kwargs)
     55 
     56     def __repr__(self):

AttributeError: 'super' object has no attribute '_ipython_display_'


"
nglviewer/nglview,"Quarto support: https://github.com/nglviewer/nglview/issues/1053
Description: Hey,

I manage to run nglview in a jupyter notebook just fine & can customize a lot of things, which is great. However, I'd like to use quarto & render notebooks to html pages, and there the widgets are not displayed. 

Does somebody have any quarto version of nglview working ? I mean the same level of customization support as in nb, not the alias that allows just to pull a structure & display it. I really need to create representations for specific residues, use selection syntax etc ... 

Thanks in advance!

"
nglviewer/nglview,"Installation issue: https://github.com/nglviewer/nglview/issues/1052
Description: - Question about usage? Please search your issue here first: https://github.com/arose/nglview/issues/589 and https://github.com/arose/nglview/issues/785

- Version report

```bash
python -c 'import nglview; print(nglview.__version__)'
3.0.3

python -c 'import ipywidgets; print(ipywidgets.__version__)'
8.0.6
```

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[2], line 3
      1 import pymol
      2 from pymol import cmd
----> 3 import nglview as nv
      4 import requests

File ~/anaconda3/envs/pymol_jupyter/lib/python3.9/site-packages/nglview/__init__.py:4
      1 import warnings
      3 # for doc
----> 4 from . import adaptor, datafiles, show, widget
      5 from ._version import get_versions
      6 from .adaptor import *

File ~/anaconda3/envs/pymol_jupyter/lib/python3.9/site-packages/nglview/show.py:13
      3 from . import datafiles
      4 from .adaptor import (ASEStructure, ASETrajectory, BiopythonStructure,
      5                       FileStructure, HTMDTrajectory, IODataStructure,
      6                       IOTBXStructure, MDAnalysisTrajectory, MDTrajTrajectory,
   (...)
     11                       RdkitStructure,
     12                       TextStructure)
---> 13 from .widget import NGLWidget
     15 __all__ = [
     16     'demo',
     17     'show_pdbid',
   (...)
     40     'show_biopython',
     41 ]
     44 def show_pdbid(pdbid, **kwargs):

File ~/anaconda3/envs/pymol_jupyter/lib/python3.9/site-packages/nglview/widget.py:19
     15 from traitlets import (Bool, CaselessStrEnum, Dict, Instance, Int, Integer,
     16                        List, Unicode, observe, validate)
     17 import traitlets
---> 19 from . import color, interpolate
     20 from .adaptor import Structure, Trajectory
     21 from .component import ComponentViewer

File ~/anaconda3/envs/pymol_jupyter/lib/python3.9/site-packages/nglview/color.py:114
    110         else:
    111             raise ValueError(f""{obj} must be either list of list or string"")
--> 114 ColormakerRegistry = _ColormakerRegistry()

File ~/anaconda3/envs/pymol_jupyter/lib/python3.9/site-packages/nglview/base.py:10, in _singleton.<locals>.getinstance()
      8 def getinstance():
      9     if cls not in instances:
---> 10         instances[cls] = cls()
     11     return instances[cls]

File ~/anaconda3/envs/pymol_jupyter/lib/python3.9/site-packages/nglview/color.py:47, in _ColormakerRegistry.__init__(self, *args, **kwargs)
     45 try:
     46     get_ipython() # only display in notebook
---> 47     self._ipython_display_()
     48 except NameError:
     49     pass

File ~/anaconda3/envs/pymol_jupyter/lib/python3.9/site-packages/nglview/color.py:54, in _ColormakerRegistry._ipython_display_(self, **kwargs)
     52 if self._ready:
     53     return
---> 54 super()._ipython_display_(**kwargs)

AttributeError: 'super' object has no attribute '_ipython_display_'


"
nglviewer/nglview,"jupyterlab chokes on colorbrewer JS shipped with nglview: https://github.com/nglviewer/nglview/issues/1051
Description: - Version report: nglview 3.0.3, ipywidgets 7.7.3.

When using nglview to create isosurfaces from cube files in a jupyterlab environment, we find the following error in the Javascript console (Edge 111.0.1661.51):
```
ErrorEvent {isTrusted: true, message: 'Uncaught SyntaxError: Function statements require a function name', filename: 'blob:[https://.../017a20b1-530c-4508-b3cb-20824f9601c2'](https://.../017a20b1-530c-4508-b3cb-20824f9601c2%27), lineno: 51, colno: 1, …}
```
This prevents the isosurfaces from being shown.

This error originates from the colorbrewer javascript file https://github.com/nglviewer/nglview/blob/master/nglview/staticlab/static/421.04c3327cf7f16704c3b2.js

which contains the following line:

```javascript
function(){return[[0,4,4,4,2,0,0,0....
``` 

I don't know enough about javascript to know whether this was ever legal - in any case, it does not seem to be legal now.

In our particlar case, this function was actually not used, so we were able to fix the problem by giving it a random name, e.g.

```javascript
function cb(){return[[0,4,4,4,2,0,0,0....
``` 

I suspect the unnamed function is actually not used anywhere in the code and could be deleted (?)

"
nglviewer/nglview,"nv.demo() does not show anything: https://github.com/nglviewer/nglview/issues/1048
Description: Hi!

In Jupyter notebook I tried the following:
     import nglview as nv
     view = nv.demo()
     view

After running ""view"" nothing happens. I get the following error message in Ubuntu terminal: 
[W 11:28:00.611 NotebookApp] 404 GET /notebooks/a4c733ec4baef9ad3896d4e34a8a5448.png (127.0.0.1): file or directory does not exist: 'a4c733ec4baef9ad3896d4e34a8a5448.png'
		404 GET /notebooks/a4c733ec4baef9ad3896d4e34a8a5448.png (127.0.0.1) 32.800000ms referer=http://localhost:8888/notebooks/Untitled.ipynb?kernel_name=python3

In Chrome console I see this:
![image](https://user-images.githubusercontent.com/6008175/224314789-d7e88ee2-7338-465f-b31c-099bdf427e74.png)

I went through all the posts where a similiar issue has been raised. So far I have tried every suggestion described, however I still can't get it to work. 

1.) These are the packages that I have installed in my conda environment:
		-nglview: 3.0.3
		-ipywidgets: 7.6.5
		-ipython: 8.11.0
		-notebook: 6.5.2 
		-notebook-shim: 0.2.2
		-jupyter lab: 3.6.1
		-jupyterlab_widgets: 3.0.5
		-widgetsnbextension: 3.5.2
		-node.js: v18.14.0

2.) I have jupyter notebook extensions installed and enabled:
	Known nbextensions:
		config dir: /opt/anacodna3/envs/mdaenv/etc/jupyter/nbconfig
		  notebook section
		    jupyterlab-plotly/extension  enabled
		    - Validating: OK
		    nglview-js-widgets/extension  enabled
		    - Validating: OK
		    jupyter-js-widgets/extension  enabled
		    - Validating: OK

3.) I don't have the folder: $HOME/.local/share/jupyter/nbextensions/ngl-js-widgets/

4.) I tried both Chrome and Edge.

When I type !pip show nglview-js-widgets I get: WARNING: Package(s) not found: nglview-js-widgets. How is this possible given that nglview-js-widgets/extension is enabled?

I would really appreciate any help to get this issue solved.

Thank you,
Goran

"
nglviewer/nglview,"Facing an issue to view the structure of a model by using 'nv.show_biopython(structure)': https://github.com/nglviewer/nglview/issues/1047
Description: nv.show_biopython(structure) is not working in jupyter notebook. It is continuously showing 'I/O operation on closed file'. Please go through the link for further [references.](url)


[
![Protein-Drug interaction project](https://user-images.githubusercontent.com/118954128/223895259-bbef55fd-c3c2-497d-8501-c5b310a6368d.jpg)
](url)

"
nglviewer/nglview,"Change color when clicked: https://github.com/nglviewer/nglview/issues/1044
Description: - Version report

```bash
nglview.__version__ = 3.0.3
ipywidgets.__version__ = 7.7.2
```
Hello,

I would like to customize my widget. I am displaying a protein structure in cif-format. I use the `nv.show_biopython` function in order to create the widget and display it in a jupyter notebook. 
When I click onto one residue, the camera centers around this selection per default. What I now want to do is to color this selection while the other residues get white. In some sense, I want to highlight the selection. 
Second, I would like to add lines to the component. Lets say, I have a list of atom indices ([2, 4, 10]) and the selection is the position 24. Then I would like to add lines dynamically from 24 to those indices 2, 4, and 10. 
I know that there is the option to click onto a residue, then use `view.picked` to extract which position was clicked and then color, add shapes and then update the view. However, this would require to steps, first click, then execute an additional update cell --- meaning, the update does not happen immediately at the click event.

From what I can tell, nglview does not support such things out of the box, hence I need to use the `_execute_js_code` function in order to run JS code. Unfortunately, I don't know how to access access several components, execute function properly etc. 

Here an example of code with the default display.
```python
import io
import urllib
import nglview as nv 
from Bio.PDB import MMCIFParser
parser = MMCIFParser(QUIET=True)

resource = urllib.request.urlopen('https://files.rcsb.org/download/7YMR.cif')
content = resource.read().decode('utf8')
handle = io.StringIO(content)

structure = parser.get_structure(""7YMR"", handle)

view = nv.show_biopython(structure[0], default_representation=False)

view.add_cartoon(""protein"", color_scheme=""chainindex"")
view.center()
view
```

![image](https://user-images.githubusercontent.com/33768245/218035101-acdf78b7-9b4f-459b-8920-29b12f9405d2.png)

If I then click on one residue and execute this code additionally, I would like to get something like this
```python
view.picked

list_of_indices = [83, 106, 108]

if ""atom1"" in view.picked:
    residue_index = view.picked['atom1']['resno']
    nv.color.ColormakerRegistry.add_selection_scheme(
        ""picked"", [[""white"", f""0-{residue_index-1}""], 
                   ['red', f""{residue_index}-{residue_index}""], 
                   [""white"", f""{residue_index+1}-10000""]]
    )
    print(residue_index)
    view.clear()
    view.add_cartoon(""protein"", color_scheme=""picked"")
    
    for atom_index in list_of_indices:
        view.add_distance(atom_pair=[[f""{residue_index}.CA"", f""{atom_index}.CA""]], label_color=""black"")
view
```
![image](https://user-images.githubusercontent.com/33768245/218035707-42da5193-e4c2-4ada-b6bd-241f24041e47.png)

![image](https://user-images.githubusercontent.com/33768245/218037088-68d2e84b-6d40-470c-a1b3-5d55f483a84f.png)

First, you can see that not only the residue I click onto was colored but also the residue position in the other chains got red as well (which is suboptimal). Second, here I have just added distances between the residue I clicked onto and the residues of my list (I would change it to other shapes). 
However, this displaying does only happen after I picked the residue and then executed that additional cell, otherwise, the widget would not change.

I tried experimenting with this in the `_execute_js_code`, however, since I don't know how to access the components, etc, I am kinda stuck. Additionally the camera then behaves strangely when I clicked, so this automatic centering onto the selection does not work anymore.
```js
this.stage.signals.clicked.add(function (pickingProxy) {
    if (pickingProxy && (pickingProxy.atom || pickingProxy.bond)){
        var atom = pickingProxy.atom || pickingProxy.closestBondAtom;
        var residue_index = atom.index;
```

Anyone having an idea how I can tackle this? I have already opened an issue in the ngl repo to ask for JS specifics, however, not sure if they can help me there since I am using the nglview binders.

Thanks a lot in advance!


"
nglviewer/nglview,"nglview showing nothing after applying fix for  'AttributeError on import': https://github.com/nglviewer/nglview/issues/1035
Description: Dear All, thank you in advance for your help on this issue.

I downgraded the  ipywidgets as recommended to fix the problem ""AttributeError on import"", issuing the following: 

conda install ""ipywidgets <8"" -c conda-forge

Now the import of nglview does not give an error any longer inside a Jupyter notebook, but calling nglview does not produce any output inside the notebook.

I am working remotely on the Wayne State University High Perfomance Computing grid, and unfortunately we are only allowed to invoke a Jupyter notebook rather than a Jupyter lab. The version of the notebook is established by the grid, not us. The following is a screenshot of the notebook. All cells were executed, but there was no output in the cells calling nglview.

![Screen Shot 2022-09-21 at 12 32 22 PM](https://user-images.githubusercontent.com/32550835/191560653-eb67332f-e2e5-4961-839e-190b50ddfec7.png)




"
nglviewer/nglview,"Pin ipywidget version to 7: https://github.com/nglviewer/nglview/pull/1033
Description: Following the discussion in #1032 and [here](https://github.com/jupyter-incubator/sparkmagic/issues/769), it seems that nglview is only compatible with version 7 of ipywidgets.  So I went ahead and pinned this in the environment.yml and setup.py.  Feel free to close this again if I've misjudged the situation.

"
nglviewer/nglview,"`AttributeError` on import while using IPython: https://github.com/nglviewer/nglview/issues/1032
Description: To any future internet explorers, **the temporary workaround is to FORCE THE INSTALLATION of`ipywidgets` VERSION 7** instead of the more recent verison 8 that is likely installed if you have come across the issue.

With `conda` infrastructure this is something like

```
$ mamba install ""ipywidgets=7"" -c conda-forge
```

The `pip` syntax is probably similar.

Below it the original bug report, reproduction, etc.:

---

I'm getting an `AttributeError` on import when using an IPython interpreter. The CPython interpreter imports it just fine. A full reproduction is [here](https://gist.github.com/mattwthompson/b7ba4f2f7680016ad6af5537b013817f), but basically:

```
$ conda create --name nglview-test
$ conda activate nglview-test
$ conda install nglview -c conda-forge -yq
$ python
Python 3.10.5 | packaged by conda-forge | (main, Jun 14 2022, 07:09:13) [Clang 13.0.1 ] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import nglview
>>> # command+D to exit out
$ ipython
Python 3.10.5 | packaged by conda-forge | (main, Jun 14 2022, 07:09:13) [Clang 13.0.1 ]
Type 'copyright', 'credits' or 'license' for more information
IPython 8.4.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import nglview
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Input In [1], in <cell line: 1>()
----> 1 import nglview

File ~/miniconda3/envs/nglview-test/lib/python3.10/site-packages/nglview/__init__.py:4, in <module>
      1 import warnings
      3 # for doc
----> 4 from . import adaptor, datafiles, show, widget
      5 from ._version import get_versions
      6 from .adaptor import *

File ~/miniconda3/envs/nglview-test/lib/python3.10/site-packages/nglview/show.py:13, in <module>
      3 from . import datafiles
      4 from .adaptor import (ASEStructure, ASETrajectory, BiopythonStructure,
      5                       FileStructure, HTMDTrajectory, IODataStructure,
      6                       IOTBXStructure, MDAnalysisTrajectory, MDTrajTrajectory,
   (...)
     11                       RdkitStructure,
     12                       TextStructure)
---> 13 from .widget import NGLWidget
     15 __all__ = [
     16     'demo',
     17     'show_pdbid',
   (...)
     40     'show_biopython',
     41 ]
     44 def show_pdbid(pdbid, **kwargs):

File ~/miniconda3/envs/nglview-test/lib/python3.10/site-packages/nglview/widget.py:19, in <module>
     15 from traitlets import (Bool, CaselessStrEnum, Dict, Instance, Int, Integer,
     16                        List, Unicode, observe, validate)
     17 import traitlets
---> 19 from . import color, interpolate
     20 from .adaptor import Structure, Trajectory
     21 from .component import ComponentViewer

File ~/miniconda3/envs/nglview-test/lib/python3.10/site-packages/nglview/color.py:114, in <module>
    110         else:
    111             raise ValueError(f""{obj} must be either list of list or string"")
--> 114 ColormakerRegistry = _ColormakerRegistry()

File ~/miniconda3/envs/nglview-test/lib/python3.10/site-packages/nglview/base.py:10, in _singleton.<locals>.getinstance()
      8 def getinstance():
      9     if cls not in instances:
---> 10         instances[cls] = cls()
     11     return instances[cls]

File ~/miniconda3/envs/nglview-test/lib/python3.10/site-packages/nglview/color.py:47, in _ColormakerRegistry.__init__(self, *args, **kwargs)
     45 try:
     46     get_ipython() # only display in notebook
---> 47     self._ipython_display_()
     48 except NameError:
     49     pass

File ~/miniconda3/envs/nglview-test/lib/python3.10/site-packages/nglview/color.py:54, in _ColormakerRegistry._ipython_display_(self, **kwargs)
     52 if self._ready:
     53     return
---> 54 super()._ipython_display_(**kwargs)

AttributeError: 'super' object has no attribute '_ipython_display_'

In [2]:
Do you really want to exit ([y]/n)? y 
```

The [lastest IPython (8.4.0)](https://pypi.org/project/ipython/#history) is from several months ago, so I don't think there'd be a breaking change there. I've been using stuff that imports `nglview` on Python 3.10 for a while, so I doubt it's that. I did notice `ipywidgets` [bumped their major version](https://pypi.org/project/ipython/#history) within the past day (which has been [rolled out](https://github.com/conda-forge/ipywidgets-feedstock/pulls?q=is%3Apr+is%3Aclosed) to `conda-forge` as well) but I don't see that in the traceback.

Any suggestions on how to hunt this down?


- Version report

```
$ python -c 'import nglview; print(nglview.__version__)'
python -c 'import ipywidgets; print(ipywidgets.__version__)'

3.0.3
8.0.1


"
nglviewer/nglview,"Remove reference backbone from movie: https://github.com/nglviewer/nglview/issues/1031
Description: I'm sorry for raising a question again.

As I make a movie from a trajectory, the movie also includes the reference backbone (the topology?).

Is there any way to remove the reference backbone?

![ezgif com-gif-maker](https://user-images.githubusercontent.com/51283097/185034071-a5186503-bf31-4196-a2fd-4fe0754ddd04.gif)

My script is like this:

```
from google.colab import output
output.enable_custom_widget_manager()
import pytraj as pt
import nglview as nv
import mdtraj as md
from nglview.contrib.movie import MovieMaker
import MDAnalysis as mda

traj_filename = './example_traj/protein_fit.xtc'
pdb_filename = './example_traj/protein_fit.tpr'

u = mda.Universe(pdb_filename, traj_filename)
u.transfer_to_memory(step=10)
view = nv.show_mdanalysis(u)

view
```

then

```
movie = MovieMaker(view, fps=10, in_memory=True, output='peptide_raw.gif')
movie.make()
```

"
nglviewer/nglview,"How to render movie in a standalone python script through terminal by MovieMaker?: https://github.com/nglviewer/nglview/issues/1030
Description: Currently, I successfully rendered movie in Google Colab notebook like this:

![image](https://user-images.githubusercontent.com/51283097/184829976-6e88a250-bbd9-4408-909e-9f02d29a3fca.png)

However, the whole script when copied to a standalone python script and run through the terminal, it didn't work.

Any idea how to make it work?

Thank you!

"
nglviewer/nglview,"Feature request: Initial snapshots: https://github.com/nglviewer/nglview/issues/1026
Description: Hello,

while using your project I was thinking of a feature that would greatly improve displaying offline notebooks.
Vispy has the feature that it saves an initial snapshot of the current interactive view as a png to the notebook.
Viewing the unexecuted notebook, e.g., on Github in documentations, it will still show an image.
An example is shown here: https://gitlab.com/nextdft/eminus/-/blob/master/examples/08_visualizer_addon/08_visualizer_addon.ipynb
Vispy at the bottom shows a snapshot, while nglview does not.

And some (maybe) helpful information:
Vispy uses their [jupyter_rfb](https://github.com/vispy/jupyter_rfb) backend for displaying in notebooks.
The snapshot functionality has been added in v0.2.0 in [this](https://github.com/vispy/jupyter_rfb/commit/6c625bb2f1a8c5f7e326ff9783f47fadd32ebecd#diff-d2f34dc196e0daeafe53639e993b3e6ebc179b0e372cbd75ad23dd5092236c12) commit.

Maybe this is something others would like to see as well.

Keep up the good work!

"
nglviewer/nglview,"Added shapes don't show in saved widget state: https://github.com/nglviewer/nglview/issues/1024
Description: I have a Python package that uses `nglview` to visualize elements (cylinders linking protein residues or spheres as residues) on protein structures. I am using `nbsphinx` to render a tutorial notebook (after running it locally and saving it with the 'Save widget state automatically' option on) into html to show in [my documentation](https://alloviz.readthedocs.io/en/latest/example.html). However, all the shapes (cylinders and spheres) don't show in the rendered html, nor even after saving, closing and reopening the notebook with Jupyter. I don't think https://github.com/nglviewer/nglview/issues/948#issuecomment-898121063 works for this case and the `write_html` function throws an error: `TypeError: Object of type float32 is not JSON serializable`.

Shapes seem to be in [the saved .ipynb file](https://raw.githubusercontent.com/frannerin/AlloViz/main/example.ipynb) (e.g., searching for ""cylinder"").

And, for example, this is what is supposed to look like in one of the cases:
![image](https://user-images.githubusercontent.com/74412019/174265781-e6ddc4df-e75f-4abc-8ee6-805cb6739d2f.png)

"
lux-org/lux,"[BUG] New pandas versions seem to break Vis aggregation: https://github.com/lux-org/lux/issues/492
Description: **Describe the Bug**
The merge call in the execute_aggregate function (lux -> executor -> PandasExecutor -> line 237) fails on pandas versions >1.4.4, which forces Lux to revert to pandas display and stops the widget from rendering. Guessing this has something to do with pandas 1.5.x changes to the groupby function.

**To Reproduce**
```python
import pandas as pd
import lux
df = pd.read_csv(""https://github.com/lux-org/lux-datasets/blob/master/data/car.csv?raw=true"")
df.intent = ['Brand', 'Origin']
df
```

Line that fails: `vis._vis_data = vis.data.merge(df,on=[columns[0], columns[1]],how=""right"",suffixes=["""", ""_right""],)`

**Screenshots**
<img width=""662"" alt=""image"" src=""https://user-images.githubusercontent.com/38196427/226674411-b8accad5-7986-436a-ae97-290184830b30.png"">


**Debugging information**

Package Versions
----------------
               Version      
        python        3.8.13
           lux         0.5.1
        pandas         1.5.0
     luxwidget        0.1.11
    matplotlib         3.5.3
        altair         4.2.0
       IPython         8.5.0
     ipykernel        6.15.2
    ipywidgets         8.0.2
jupyter_client         7.3.5
  jupyter_core        4.11.1
jupyter_server not installed
    jupyterlab not installed
      nbclient not installed
     nbconvert         5.4.1
      nbformat         5.5.0
      notebook        6.4.12
     qtconsole         5.3.2
     traitlets         5.4.0

Widget Setup
-------------
✅ Jupyter Notebook Running
✅ luxwidget is enabled


"
lux-org/lux,"[BUG]: https://github.com/lux-org/lux/issues/489
Description: I am using Jupyter Labs and have these extensions installed:
![image](https://user-images.githubusercontent.com/107151852/218582831-418386dd-4e75-48fb-b74b-dfa12f82c58b.png)
<img width=""187"" alt=""image"" src=""https://user-images.githubusercontent.com/107151852/218582925-5fb8d307-2bdb-47e8-80bc-5a30cace52e6.png"">
I cannot figure out why I am not able to see the sample dataframe (or my own custom df in Lux)
![image](https://user-images.githubusercontent.com/107151852/218582650-c1ab2e12-783b-4df8-ae3b-31123c7a3ce0.png)

I am not positive if I did something to install it incorrectly, but if you could take a look and let me know what you think...

```python
import lux
lux.debug_info()
```
![image](https://user-images.githubusercontent.com/107151852/218582708-4c9464f8-3f15-411c-a19f-9654ee094201.png)
![image](https://user-images.githubusercontent.com/107151852/218582745-c8d2d6d9-408b-4532-b073-fbf926c35857.png)



"
lux-org/lux,"[BUG] Couldnt see visualizations : https://github.com/lux-org/lux/issues/488
Description: **Describe the bug**
Could not observe visualizations for the dataframe

**To Reproduce**
Please refer to the dataset here [Link](https://www.kaggle.com/datasets/piyushgoyal443/red-wine-dataset?select=wineQualityReds.csv)
1. Using this data: `redwine_data = pd.read_csv('data/winequality-red.csv', sep=';')`
2. Looking at the dataframe: 'redwine_data'
3. Figure 1 shows the dataframe
4. When I click on 'Toggle Pandas/Lux', nothing happens, the response is empty in figure 2

![Figure 1](https://user-images.githubusercontent.com/74876112/209623221-f1d95f91-0653-41bd-afd4-87b2803d5976.png)
![Figure 2](https://user-images.githubusercontent.com/74876112/209623509-d87c7f62-574f-4159-8bb1-bddcdb11a6e4.png)



"
lux-org/lux,"[BUG] IndexError: index 0 is out of bounds for axis 0 with size 0: https://github.com/lux-org/lux/issues/481
Description: %matplotlib inline
from IPython.display import display
import numpy as np

import pandas as pd
import lux

autos = pd.read_csv('http://github.com/mattharrison/datasets/raw/master/data/vehicles.csv.zip')
autos.intent = ['combinedUF','cityUF']
autos

- - - - - - - - - OUTPUT - - - - - - - -

/home/max/anaconda3/lib/python3.9/site-packages/IPython/core/formatters.py:918: UserWarning:
Unexpected error in rendering Lux widget and recommendations. Falling back to Pandas display.
Please report the following issue on Github: https://github.com/lux-org/lux/issues 

/home/max/anaconda3/lib/python3.9/site-packages/lux/core/frame.py:632: UserWarning:Traceback (most recent call last):
  File ""/home/max/anaconda3/lib/python3.9/site-packages/lux/core/frame.py"", line 594, in _ipython_display_
    self.maintain_recs()
  File ""/home/max/anaconda3/lib/python3.9/site-packages/lux/core/frame.py"", line 436, in maintain_recs
    custom_action_collection = custom_actions(rec_df)
  File ""/home/max/anaconda3/lib/python3.9/site-packages/lux/action/custom.py"", line 76, in custom_actions
    recommendation = lux.config.actions[action_name].action(ldf)
  File ""/home/max/anaconda3/lib/python3.9/site-packages/lux/action/enhance.py"", line 65, in enhance
    vlist = lux.vis.VisList.VisList(intent, ldf)
  File ""/home/max/anaconda3/lib/python3.9/site-packages/lux/vis/VisList.py"", line 43, in __init__
    self.refresh_source(self._source)
  File ""/home/max/anaconda3/lib/python3.9/site-packages/lux/vis/VisList.py"", line 336, in refresh_source
    lux.config.executor.execute(self._collection, ldf, approx=approx)
  File ""/home/max/anaconda3/lib/python3.9/site-packages/lux/executor/PandasExecutor.py"", line 147, in execute
    PandasExecutor.execute_2D_binning(vis)
  File ""/home/max/anaconda3/lib/python3.9/site-packages/lux/executor/PandasExecutor.py"", line 422, in execute_2D_binning
    result = groups.agg(
  File ""/home/max/anaconda3/lib/python3.9/site-packages/lux/core/groupby.py"", line 32, in aggregate
    ret_val = super(LuxGroupBy, self).aggregate(*args, **kwargs)
  File ""/home/max/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/generic.py"", line 271, in aggregate
    ret = self._aggregate_multiple_funcs(func)
  File ""/home/max/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/generic.py"", line 326, in _aggregate_multiple_funcs
    results[key] = self.aggregate(func)
  File ""/home/max/anaconda3/lib/python3.9/site-packages/lux/core/groupby.py"", line 32, in aggregate
    ret_val = super(LuxGroupBy, self).aggregate(*args, **kwargs)
  File ""/home/max/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/generic.py"", line 284, in aggregate
    return self._python_agg_general(func, *args, **kwargs)
  File ""/home/max/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/groupby.py"", line 1481, in _python_agg_general
    result = self.grouper.agg_series(obj, f)
  File ""/home/max/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/ops.py"", line 981, in agg_series
    result = self._aggregate_series_pure_python(obj, func)
  File ""/home/max/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/ops.py"", line 1005, in _aggregate_series_pure_python
    res = func(group)
  File ""/home/max/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/groupby.py"", line 1467, in <lambda>
    f = lambda x: func(x, *args, **kwargs)
  File ""/home/max/anaconda3/lib/python3.9/site-packages/lux/executor/PandasExecutor.py"", line 423, in <lambda>
    [(""count"", ""count""),(color_attr.attribute, lambda x: pd.Series.mode(x).iat[0]),]).reset_index()
  File ""/home/max/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py"", line 2221, in __getitem__
    return self.obj._get_value(*key, takeable=self._takeable)
  File ""/home/max/anaconda3/lib/python3.9/site-packages/pandas/core/series.py"", line 1066, in _get_value
    return self._values[label]
IndexError: index 0 is out of bounds for axis 0 with size 0


Package Versions
----------------
               Version
        python  3.9.7 
           lux  0.5.1 
        pandas  1.4.2 
     luxwidget 0.1.11 
    matplotlib  3.4.3 
        altair  4.2.0 
       IPython 7.29.0 
     ipykernel  6.4.1 
    ipywidgets  7.6.5 
jupyter_client 6.1.12 
  jupyter_core  4.8.1 
jupyter_server  1.4.1 
    jupyterlab  3.2.1 
      nbclient  0.5.3 
     nbconvert  6.1.0 
      nbformat  5.1.3 
      notebook  6.4.5 
     qtconsole  5.1.1 
     traitlets  5.1.0 

Widget Setup
-------------
✅ Jupyter Notebook Running
✅ luxwidget is enabled


"
lux-org/lux,"[BUG]: https://github.com/lux-org/lux/issues/479
Description: **Describe the bug**
Using the toggle button in a Jupyter Notebook in Chrome causes error ""Uncaught (in promise) Error: Host is not attached."" This impacts the ability to programmatically manipulate the notebook after the error arises. Perhaps requires [de]serialization of subwidgets, as mentioned in low-level widget tutorial?

**To Reproduce**
1. in a jupyter notebook code cell:
2. import lux
3. import pandas as pd
4. df = pd.read_csv(""https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv"")
5. df
6. Open the inspector
7. Click the toggle Pandas/lux button

**Important**: Make sure to provide a sample of (preferably synthetic) data 

**Expected behavior**
Click button to toggle, no error

**Screenshots**

**Debugging information**
Package Versions
----------------
               Version
        python 3.8.13 
           lux  0.5.1 
        pandas  1.4.2 
     luxwidget 0.1.11 
    matplotlib  3.5.2 
        altair  4.2.0 
       IPython 7.29.0 
     ipykernel  6.4.1 
    ipywidgets  7.6.5 
jupyter_client 6.1.12 
  jupyter_core  4.8.1 
jupyter_server  1.4.1 
    jupyterlab  3.2.1 
      nbclient  0.5.3 
     nbconvert  6.1.0 
      nbformat  5.1.3 
      notebook  6.4.5 
     qtconsole  5.1.1 
     traitlets  5.1.0 

Widget Setup
-------------
✅ Jupyter Notebook Running
✅ luxwidget is enabled


"
lux-org/lux,"[Issue] Using lux on Databricks: https://github.com/lux-org/lux/issues/476
Description: I'm trying to use the lux library in a Databricks notebook but I'm getting this warning and error after the df.intent statment:

Out[10]: /databricks/python/lib/python3.8/site-packages/IPython/core/formatters.py:918: UserWarning:
Unexpected error in rendering Lux widget and recommendations. Falling back to Pandas display.
Please report the following issue on Github: https://github.com/lux-org/lux/issues 

/databricks/python/lib/python3.8/site-packages/lux/core/frame.py:632: UserWarning:Traceback (most recent call last):
  File ""/databricks/python/lib/python3.8/site-packages/lux/core/frame.py"", line 594, in _ipython_display_
    self.maintain_recs()
  File ""/databricks/python/lib/python3.8/site-packages/lux/core/frame.py"", line 451, in maintain_recs
    self._widget = rec_df.render_widget()
  File ""/databricks/python/lib/python3.8/site-packages/lux/core/frame.py"", line 681, in render_widget
    widgetJSON = self.to_JSON(self._rec_info, input_current_vis=input_current_vis)
  File ""/databricks/python/lib/python3.8/site-packages/lux/core/frame.py"", line 713, in to_JSON
    widget_spec[""current_vis""] = LuxDataFrame.current_vis_to_JSON(
  File ""/databricks/python/lib/python3.8/site-packages/lux/core/frame.py"", line 736, in current_vis_to_JSON
    current_vis_spec[""allcols""] = False
TypeError: 'NoneType' object does not support item assignment

"
lux-org/lux,"Implement pip installation of jupyter lab extension: https://github.com/lux-org/lux/issues/475
Description: PROBLEM
When coding in a jupyter hub environement with jupyter 3.0 it's only possible to to install jupyter lab extensions for individual users if the installation it's done via pip 

solution
follow the documentation for building JLab extensions for JLab 3.0. If you do it, then a pip install is sufficient, and there's no need to rebuild the image everytjme an individual user would like to install lux which is not part of the base image 

See here:
https://jupyterlab.readthedocs.io/en/stable/user/extensions.html#id12

As the documentation tell us:
There are two types of JupyterLab extensions: a source extension (which requires a rebuild of JupyterLab when installed), and a prebuilt extension (which does not require a rebuild of JupyterLab). Rebuilding JupyterLab requires Node.js to be installed.

The goal here is NOT HAVING TO REBUILD JUPYTER LAB which is not possible for a jupyter hub Server based user. 
A prebuilt extension could be simply installed with pip.

"
lux-org/lux,"[BUG]: https://github.com/lux-org/lux/issues/473
Description: **Describe the bug**
C:\ProgramData\Anaconda3\lib\site-packages\IPython\core\formatters.py:918: UserWarning: Unexpected error in rendering Lux widget and recommendations. Falling back to Pandas display. Please report the following issue on Github: https://github.com/lux-org/lux/issues   C:\ProgramData\Anaconda3\lib\site-packages\lux\core\frame.py:632: UserWarning:Traceback (most recent call last):   File ""C:\ProgramData\Anaconda3\lib\site-packages\lux\core\frame.py"", line 594, in _ipython_display_     self.maintain_recs()   File ""C:\ProgramData\Anaconda3\lib\site-packages\lux\core\frame.py"", line 436, in maintain_recs     custom_action_collection = custom_actions(rec_df)   File ""C:\ProgramData\Anaconda3\lib\site-packages\lux\action\custom.py"", line 74, in custom_actions     recommendation = lux.config.actions[action_name].action(ldf, args)   File ""C:\ProgramData\Anaconda3\lib\site-packages\lux\action\univariate.py"", line 95, in univariate     vlist = VisList(intent, ldf)   File ""C:\ProgramData\Anaconda3\lib\site-packages\lux\vis\VisList.py"", line 43, in __init__     self.refresh_source(self._source)   File ""C:\ProgramData\Anaconda3\lib\site-packages\lux\vis\VisList.py"", line 336, in refresh_source     lux.config.executor.execute(self._collection, ldf, approx=approx)   File ""C:\ProgramData\Anaconda3\lib\site-packages\lux\executor\PandasExecutor.py"", line 138, in execute     PandasExecutor.execute_aggregate(vis, isFiltered=filter_executed)   File ""C:\ProgramData\Anaconda3\lib\site-packages\lux\executor\PandasExecutor.py"", line 211, in execute_aggregate     vis._vis_data = (vis.data.groupby(groupby_attr.attribute, dropna=False, history=False).count().reset_index().rename(columns={index_name: ""Record""}))   File ""C:\ProgramData\Anaconda3\lib\site-packages\lux\core\frame.py"", line 867, in groupby     groupby_obj = super(LuxDataFrame, self).groupby(*args, **kwargs) TypeError: groupby() got an unexpected keyword argument 'dropna'
**To Reproduce**
Please describe the steps needed to reproduce the behavior. For example:
1. Using this data: `df = ...`
2. Go to '...'
3. Click on '....'
4. Scroll down to '....'
5. See error

**Important**: Make sure to provide a sample of (preferably synthetic) data 
that can trigger the error based on the instructions.

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Debugging information**
Please run the following code in your Jupyter Notebook/Lab:

```python
import lux
lux.debug_info()
```

**Additional context**
Add any other context about the problem here.


"
lux-org/lux,"[BUG]  jupyter labextension install @jupyter-widgets/jupyterlab-manager  ValueError: ""@jupyter-widgets/jupyterlab-manager"" is not a valid npm package: https://github.com/lux-org/lux/issues/470
Description: jupyter labextension install @jupyter-widgets/jupyterlab-manager

opt/oss/conda3/lib/python3.7/site-packages/jupyter_client/channels.py:15: VisibleDeprecationWarning: zmq.eventloop.minitornado is deprecated in pyzmq 14.0 and will be removed.
    Install tornado itself to use zmq with the tornado IOLoop.
    
  from .session import Session
An error occurred.
ValueError: ""@jupyter-widgets/jupyterlab-manager"" is not a valid npm package




"
lux-org/lux,"[BUG] `LuxSeries.unique` returns incorrect values after subsetting: https://github.com/lux-org/lux/issues/467
Description: **Describe the bug**

`LuxSeries` wrapper around pandas Series, does not compute the unique values correctly for series corresponding to subsets of dataframe.  

**To Reproduce**

Invent some data:

```
data = pd.DataFrame([['a', 1, 2], ['b', 2, 3], ['c', -1, 17]], columns=['foo', 'bar', 'baz'])
```

View it, no need to click on the lux button or anything.

```
data
```

Now create a subset of this data from `bar > 0` and select `foo` column only. 

```
data = data[data['bar'] > 0]['foo']
```

`data` is now a Series with two values in it `'a'` and `'b'`.
In the notebook I view it again and it produces the correct output, no need to click anything:

```
data
```

However running the `.unique()` function on the series:

```
data.unique()
```

Returns _all_ values including `['a', 'b', 'c']`, when it should forget about the value `'c'` due to subsetting.

See [gist](https://gist.github.com/lukauskas/65d32167f831b0233c80aeacfc1b197d)  and screenshot below

**Expected behavior**

`data.unique()` should return only `['a', 'b']`.

**Screenshots**

<img width=""853"" alt=""image"" src=""https://user-images.githubusercontent.com/108413/158380713-7968d6e9-b202-455a-a60e-742d19c8d351.png"">

**Debugging information**

```
Package Versions
----------------
               Version
        python 3.10.2 
           lux  0.5.1 
        pandas  1.4.0 
     luxwidget 0.1.11 
    matplotlib  3.5.1 
        altair  4.2.0 
       IPython  8.0.1 
     ipykernel  6.9.0 
    ipywidgets  7.6.5 
jupyter_client  7.1.2 
  jupyter_core  4.9.1 
jupyter_server 1.13.5 
    jupyterlab  3.3.0 
      nbclient 0.5.10 
     nbconvert  6.4.1 
      nbformat  5.1.3 
      notebook  6.4.8 
     qtconsole  5.2.2 
     traitlets  5.1.1 

Widget Setup
-------------
✅ Jupyter Lab Running
✅ luxwidget is enabled
```

**Additional context**

This actually can cause some very nasty and silent errors in the analyses that depend on this `.unique()` operator, as only the import of `lux` is needed to redefine behaviour.
"
bqplot/bqplot,"0.12.36: pep517 build fails: https://github.com/bqplot/bqplot/issues/1592
Description: **Describe the bug**
Look slike pep617 based build fails

**To Reproduce**
Just execute `python3 -sBm build -w --no-isolation` in source build tree.

**Expected behavior**
pep517 based build should work.

**Screenshots**
<details>

```console
+ /usr/bin/python3 -sBm build -w --no-isolation
* Getting build dependencies for wheel...
<string>:50: DeprecatedWarning: install_npm is deprecated as of 0.8 and will be removed in 1.0. Use `npm_builder` and `wrap_installers`
<string>:55: DeprecatedWarning: create_cmdclass is deprecated as of 0.8 and will be removed in 1.0. ""
Use `wrap_installers` to handle prebuild steps in cmdclass.
Use `get_data_files` to handle data files.
Use `include_package_data=True` and `MANIFEST.in` for package data.

/usr/lib/python3.8/site-packages/setuptools/config/setupcfg.py:508: SetuptoolsDeprecationWarning: The license_file parameter is deprecated, use license_files instead.
  warnings.warn(msg, warning_class)
running egg_info
creating bqplot.egg-info
writing bqplot.egg-info/PKG-INFO
writing dependency_links to bqplot.egg-info/dependency_links.txt
writing requirements to bqplot.egg-info/requires.txt
writing top-level names to bqplot.egg-info/top_level.txt
writing manifest file 'bqplot.egg-info/SOURCES.txt'
reading manifest file 'bqplot.egg-info/SOURCES.txt'
reading manifest template 'MANIFEST.in'
warning: no previously-included files found matching 'tests'
adding license file 'LICENSE'
writing manifest file 'bqplot.egg-info/SOURCES.txt'
* Building wheel...
<string>:50: DeprecatedWarning: install_npm is deprecated as of 0.8 and will be removed in 1.0. Use `npm_builder` and `wrap_installers`
<string>:55: DeprecatedWarning: create_cmdclass is deprecated as of 0.8 and will be removed in 1.0. ""
Use `wrap_installers` to handle prebuild steps in cmdclass.
Use `get_data_files` to handle data files.
Use `include_package_data=True` and `MANIFEST.in` for package data.

/usr/lib/python3.8/site-packages/setuptools/config/setupcfg.py:508: SetuptoolsDeprecationWarning: The license_file parameter is deprecated, use license_files instead.
  warnings.warn(msg, warning_class)
running bdist_wheel
running jsdeps
yarn not found, ignoring yarn.lock file
`npm` unavailable.  If you're running this command using sudo, make sure `npm` is available to sudo
Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/pep517/in_process/_in_process.py"", line 351, in <module>
    main()
  File ""/usr/lib/python3.8/site-packages/pep517/in_process/_in_process.py"", line 333, in main
    json_out['return_val'] = hook(**hook_input['kwargs'])
  File ""/usr/lib/python3.8/site-packages/pep517/in_process/_in_process.py"", line 249, in build_wheel
    return _build_backend().build_wheel(wheel_directory, config_settings,
  File ""/usr/lib/python3.8/site-packages/setuptools/build_meta.py"", line 413, in build_wheel
    return self._build_with_temp_dir(['bdist_wheel'], '.whl',
  File ""/usr/lib/python3.8/site-packages/setuptools/build_meta.py"", line 398, in _build_with_temp_dir
    self.run_setup()
  File ""/usr/lib/python3.8/site-packages/setuptools/build_meta.py"", line 335, in run_setup
    exec(code, locals())
  File ""<string>"", line 62, in <module>
  File ""/usr/lib/python3.8/site-packages/setuptools/__init__.py"", line 87, in setup
    return distutils.core.setup(**attrs)
  File ""/usr/lib/python3.8/site-packages/setuptools/_distutils/core.py"", line 185, in setup
    return run_commands(dist)
  File ""/usr/lib/python3.8/site-packages/setuptools/_distutils/core.py"", line 201, in run_commands
    dist.run_commands()
  File ""/usr/lib/python3.8/site-packages/setuptools/_distutils/dist.py"", line 969, in run_commands
    self.run_command(cmd)
  File ""/usr/lib/python3.8/site-packages/setuptools/dist.py"", line 1208, in run_command
    super().run_command(command)
  File ""/usr/lib/python3.8/site-packages/setuptools/_distutils/dist.py"", line 988, in run_command
    cmd_obj.run()
  File ""/usr/lib/python3.8/site-packages/jupyter_packaging/setupbase.py"", line 726, in run
    [self.run_command(cmd) for cmd in cmds]
  File ""/usr/lib/python3.8/site-packages/jupyter_packaging/setupbase.py"", line 726, in <listcomp>
    [self.run_command(cmd) for cmd in cmds]
  File ""/usr/lib/python3.8/site-packages/setuptools/_distutils/cmd.py"", line 318, in run_command
    self.distribution.run_command(command)
  File ""/usr/lib/python3.8/site-packages/setuptools/dist.py"", line 1208, in run_command
    super().run_command(command)
  File ""/usr/lib/python3.8/site-packages/setuptools/_distutils/dist.py"", line 988, in run_command
    cmd_obj.run()
  File ""/usr/lib/python3.8/site-packages/jupyter_packaging/setupbase.py"", line 421, in run
    self.command.run()
  File ""/usr/lib/python3.8/site-packages/jupyter_packaging/setupbase.py"", line 345, in run
    c.run()
  File ""/usr/lib/python3.8/site-packages/jupyter_packaging/setupbase.py"", line 442, in run
    raise ValueError((""missing files: %s"" % missing))
ValueError: missing files: ['share/jupyter/nbextensions/bqplot/index.js', 'share/jupyter/labextensions/bqplot/package.json']

ERROR Backend subprocess exited when trying to invoke build_wheel
```
</details>

**Additional context**
Here is list of installed modules in build env
<details>

```console
Package                       Version
----------------------------- -----------------
alabaster                     0.7.13
anyio                         3.6.2
argon2-cffi                   21.3.0
argon2-cffi-bindings          21.2.0
asttokens                     2.2.1
attrs                         22.2.0
Babel                         2.12.1
backcall                      0.2.0
beautifulsoup4                4.11.2
bleach                        6.0.0
build                         0.9.0
cffi                          1.15.1
charset-normalizer            3.0.1
comm                          0.1.2
coverage                      7.2.1
debugpy                       1.6.6
decorator                     5.1.1
defusedxml                    0.7.1
deprecation                   2.1.0
distro                        1.8.0
docutils                      0.19
entrypoints                   0.4
exceptiongroup                1.0.0
executing                     1.2.0
fastjsonschema                2.16.1
gpg                           1.18.0-unknown
html5lib                      1.1
idna                          3.4
imagesize                     1.4.1
importlib-metadata            6.0.0
importlib-resources           5.12.0
iniconfig                     2.0.0
ipykernel                     6.20.2
ipython                       8.6.0
ipython-genutils              0.2.0
ipywidgets                    8.0.3
jedi                          0.18.2
Jinja2                        3.1.2
json5                         0.9.12
jsonschema                    4.17.3
jupyter_client                7.4.9
jupyter_core                  5.2.0
jupyter_packaging             0.12.3
jupyter-server                1.23.3
jupyter-sphinx                0.4.0
jupyterlab                    3.5.1
jupyterlab-pygments           0.1.2
jupyterlab_server             2.18.0
jupyterlab-widgets            3.0.2
libcomps                      0.1.19
MarkupSafe                    2.1.2
matplotlib-inline             0.1.6
mistune                       2.0.5
nbclassic                     0.4.8
nbclient                      0.7.2
nbconvert                     7.2.9
nbformat                      5.7.3
nbval                         0.9.6
nest-asyncio                  1.5.6
notebook                      6.5.2
notebook_shim                 0.2.2
numpy                         1.24.2
packaging                     23.0
pandas                        1.5.2
pandocfilters                 1.5.0
parso                         0.8.3
pep517                        0.13.0
pexpect                       4.8.0
pickleshare                   0.7.5
pip                           22.3.1
pkgutil_resolve_name          1.3.10
platformdirs                  2.6.0
pluggy                        1.0.0
ply                           3.11
prometheus-client             0.16.0
prompt-toolkit                3.0.36
psutil                        5.9.2
ptyprocess                    0.7.0
pure-eval                     0.2.2
pycparser                     2.21
Pygments                      2.14.0
pyrsistent                    0.19.3
pytest                        7.2.2
python-dateutil               2.8.2
pytz                          2022.4
pyzmq                         24.0.1
requests                      2.28.2
rpm                           4.17.0
SciPy                         1.8.1
Send2Trash                    1.8.0
setuptools                    65.6.3
six                           1.16.0
sniffio                       1.2.0
snowballstemmer               2.2.0
soupsieve                     2.4
Sphinx                        6.1.3
sphinx-rtd-theme              1.1.1
sphinx-thebe                  0.2.0
sphinxcontrib-applehelp       1.0.4
sphinxcontrib-devhelp         1.0.2.dev20230202
sphinxcontrib-htmlhelp        2.0.0
sphinxcontrib.jquery          3.0.0
sphinxcontrib-jsmath          1.0.1.dev20230128
sphinxcontrib-qthelp          1.0.3.dev20230128
sphinxcontrib-serializinghtml 1.1.5
stack-data                    0.6.2
terminado                     0.17.1
tinycss2                      1.2.1
tomli                         2.0.1
tomlkit                       0.11.6
tornado                       6.2
traitlets                     5.8.1
traittypes                    0.2.1
urllib3                       1.26.12
wcwidth                       0.2.6
webencodings                  0.5.1
websocket-client              1.5.1
wheel                         0.38.4
widgetsnbextension            4.0.5
zipp                          3.15.0
```
</details>


"
bqplot/bqplot,"Lazy CI + Remove JupyterLab 2 job: https://github.com/bqplot/bqplot/pull/1573
Description: None
"
catboost/catboost,"NDCG seem to have implicit required argument: https://github.com/catboost/catboost/issues/2355
Description: Problem: Jupyter kernel dies when I try to calculate NDCG without `group_id` argument
catboost version: 1.1.1
Operating System: MacOS, Ubuntu

```python
from catboost.metrics import NDCG

ndcg = NDCG()
ndcg.eval([1, 2, 3, 4, 5], [1, 2, 3, 4, 5])
```
this causes kernel to die without any message.

```python
from catboost.metrics import NDCG

ndcg = NDCG()
ndcg.eval([1, 2, 3, 4, 5], [1, 2, 3, 4, 5], group_id=[1, 1, 1, 1, 1])
```

this (see `group_id` presented) returns 1.0

Expected behaviour:
**Either** treat absence of `group_id` as single group (preffered by me).
**Or** define `group_id` required argument in docmuentation and raise human readable error when it is not provided.

"
catboost/catboost,"MetricVisualizer not showing on Jupyter notebook: https://github.com/catboost/catboost/issues/2351
Description: Problem:  running the python tutorials on Jupyter notebook on chrome and i cannot see the visualizations
[https://github.com/catboost/tutorials/blob/master/python_tutorial.ipynb](url)

![image](https://user-images.githubusercontent.com/130661648/231748457-2423b57f-9ea5-4463-9c75-f8df1cf29adb.png)

python version: 3.10
catboost version: 1.1.1
Operating System: mac ventura (Apple M1 Pro)



"
catboost/catboost,"cv valid metric not identical to manual : https://github.com/catboost/catboost/issues/2338
Description: Problem:cv valid error not identical to manual 
catboost version:1.1.1
Operating System:Linux(google colab)
CPU:
GPU:
Please, help me to understand why cv and manual results give different auc metric. I intentionally dropped catgorical features from dataset (read that there tricks in working with them). 
output from cv - 0.7889263104273184, manual - 0.7826946255875921
My code example is below:

`

    import pandas as pd
    import numpy as np
    from sklearn.model_selection import KFold
    from sklearn.metrics import roc_auc_score
    from catboost import Pool, cv, CatBoostClassifier
    
    SEED=0
    np.random.seed(SEED)
    
    
    df = pd.read_csv(""https://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv"")
    df = df.sample(frac=1).reset_index(drop=True)
    
    id_cols = ['name', 'ticket']
    target_col = 'survived'
    num_cols = df.drop([target_col]+id_cols, axis=1).select_dtypes(include=np.number).columns.tolist()
    df = df[[target_col]+id_cols+num_cols]
    
    
    sp = KFold(n_splits=3, shuffle=False)
    
    tr_pool = Pool(df.drop(columns=[target_col]+id_cols), df[target_col])
    
    params = {'loss_function': 'Logloss', 'eval_metric':'AUC', 
              'verbose': False, 'metric_period':1, 'early_stopping_rounds':50}
    
    res = cv(tr_pool, params = params, iterations=500, shuffle=False, folds=sp,stratified=False )
    it_best = np.argmax(res['test-AUC-mean'])+1
    metr_best = max(res['test-AUC-mean'])
    print(f'it_best - {it_best}, metr_best - {metr_best}')
    
    
    
    auc_l = []
    clf = CatBoostClassifier(iterations=it_best, eval_metric='AUC', silent=True)
    for idx_tr, idx_val in sp.split(df):
        
        X_tr, X_val = df.drop(columns=id_cols+[target_col]).iloc[idx_tr], \
                      df.drop(columns=id_cols+[target_col]).iloc[idx_val]
        y_tr, y_val = df[target_col].iloc[idx_tr], df[target_col].iloc[idx_val]
        
        
        clf.fit(X_tr, y_tr)
        
        auc_l.append(roc_auc_score(df[target_col].iloc[idx_val], clf.predict_proba(X_val)[:,1]))
    
    print(np.mean(auc_l))
`

"
catboost/catboost,"Error ""Process finished with exit code 136"" for grid_search with cv = 1: https://github.com/catboost/catboost/issues/2333
Description: Problem: Error ""Process finished with exit code 136"" for grid_search with cv = 1
catboost version:1.1.1
Operating System:Ubuntu
CPU: yes
GPU: yes

Code for reproducing errors
```python
import numpy as np
import scipy.sparse
from catboost import CatBoostClassifier, Pool


row = np.array([0, 0, 1, 2, 2, 2, 3, 3, 4])
col = np.array([0, 2, 2, 0, 1, 2, 0, 2, 2])
data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])

X = scipy.sparse.csr_matrix((data, (row, col)), shape=(5, 3))

y = np.array([0, 1, 0, 1, 0])

dataset = Pool(X, y)

params = {'nan_mode': 'Min',
          'eval_metric': 'NormalizedGini',
          'loss_function': 'Logloss',
          'iterations': 100000,
          'early_stopping_rounds' : 20,
          'task_type': 'CPU',
          }

model = CatBoostClassifier(**params)

grid = {'learning_rate': [0.03]}
res = model.grid_search(grid, X=dataset, train_size=0.9, cv = 1)
```
error in jupyter notebook 

> Canceled future for execute_request message before replies were done

error in python script

> Process finished with exit code 136 (interrupted by signal 8: SIGFPE)

The error is related to the parameter ""cv = 1"". With a cv greater than 1, everything is fine.
May be necessary to throw an exeption that the cv=1 parameter is not valid, or fix this behavior.



"
catboost/catboost,"Catboost will not install on the new version of poetry: https://github.com/catboost/catboost/issues/2331
Description: Problem: Catboost will not install on the new version of poetry
catboost version: 1.1.1
Operating System: macOS (but also Linux)
CPU: Apple M1 Pro
GPU:

The following does not work with the new version of Poetry (1.4.1)

```python
curl -sSL https://install.python-poetry.org | python3 - --version 1.4.1
cd ~
poetry new testPoetry  
cd testPoetry
poetry env use 3.9
poetry -vvv add catboost
```

This gives

```
...

• Installing catboost (1.1.1): Failed

  Stack trace:

  5  ~/Library/Application Support/pypoetry/venv/lib/python3.9/site-packages/poetry/installation/executor.py:272 in _execute_operation
      270│ 
      271│             try:
    → 272│                 result = self._do_execute_operation(operation)
      273│             except EnvCommandError as e:
      274│                 if e.e.returncode == -2:

  4  ~/Library/Application Support/pypoetry/venv/lib/python3.9/site-packages/poetry/installation/executor.py:374 in _do_execute_operation
      372│             return 0
      373│ 
    → 374│         result: int = getattr(self, f""_execute_{method}"")(operation)
      375│ 
      376│         if result != 0:

  3  ~/Library/Application Support/pypoetry/venv/lib/python3.9/site-packages/poetry/installation/executor.py:494 in _execute_install
      492│ 
      493│     def _execute_install(self, operation: Install | Update) -> int:
    → 494│         status_code = self._install(operation)
      495│ 
      496│         self._save_url_reference(operation)

  2  ~/Library/Application Support/pypoetry/venv/lib/python3.9/site-packages/poetry/installation/executor.py:552 in _install
      550│                 self._remove(operation.initial_package)
      551│ 
    → 552│             self._wheel_installer.install(archive)
      553│         finally:
      554│             if cleanup_archive:

  1  ~/Library/Application Support/pypoetry/venv/lib/python3.9/site-packages/poetry/installation/wheel_installer.py:101 in install
       99│     def install(self, wheel: Path) -> None:
      100│         with WheelFile.open(wheel) as source:
    → 101│             source.validate_record()
      102│             install(
      103│                 source=source,

  _WheelFileValidationError

  [""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost-1.1.1.data/data/etc/jupyter/nbconfig/notebook.d/catboost-widget.json didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost-1.1.1.data/data/share/jupyter/labextensions/catboost-widget/package.json didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost-1.1.1.data/data/share/jupyter/labextensions/catboost-widget/static/remoteEntry.64b1a05fa8a34d8acb22.js didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost-1.1.1.data/data/share/jupyter/labextensions/catboost-widget/static/479.8635cb839b51b24dbe44.js.LICENSE.txt didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost-1.1.1.data/data/share/jupyter/labextensions/catboost-widget/static/908.11316378aafcadcd81b8.js didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost-1.1.1.data/data/share/jupyter/labextensions/catboost-widget/static/486.c2672c1c6aabd4dcb1c5.js.LICENSE.txt didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost-1.1.1.data/data/share/jupyter/labextensions/catboost-widget/static/138.594e2e2f9fae55d2125b.js didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost-1.1.1.data/data/share/jupyter/labextensions/catboost-widget/static/style.js didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost-1.1.1.data/data/share/jupyter/labextensions/catboost-widget/static/755.f7277b38a5f70148dece.js didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost-1.1.1.data/data/share/jupyter/labextensions/catboost-widget/static/479.8635cb839b51b24dbe44.js didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost-1.1.1.data/data/share/jupyter/labextensions/catboost-widget/static/486.c2672c1c6aabd4dcb1c5.js didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost-1.1.1.data/data/share/jupyter/labextensions/catboost-widget/static/755.f7277b38a5f70148dece.js.LICENSE.txt didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost-1.1.1.data/data/share/jupyter/nbextensions/catboost-widget/index.js didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost-1.1.1.data/data/share/jupyter/nbextensions/catboost-widget/extension.js didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost-1.1.1.dist-info/WHEEL didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost-1.1.1.dist-info/top_level.txt didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost-1.1.1.dist-info/METADATA didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/monoforest.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/plot_helpers.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/metrics.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/version.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/text_processing.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/datasets.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/__init__.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/core.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/utils.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/_catboost.so didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/widget/__init__.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/widget/metrics_plotter.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/widget/ipythonwidget.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/widget/callbacks.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/hnsw/__init__.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/hnsw/hnsw.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/hnsw/_hnsw.so didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/eval/catboost_evaluation.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/eval/_fold_model.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/eval/_readers.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/eval/log_config.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/eval/_splitter.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/eval/__init__.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/eval/execution_case.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/eval/_fold_storage.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/eval/factor_utils.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/eval/utils.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/eval/evaluation_result.py didn't match RECORD"", ""In /my/path/Library/Caches/pypoetry/artifacts/f0/c2/01/761d1ea5f1c7219383dc66924b24cf458e092da4c4d173f2e1739c0dc5/catboost-1.1.1-cp39-none-macosx_10_6_universal2.whl, hash / size of catboost/eval/_fold_models_handler.py didn't match RECORD""]

  at ~/Library/Application Support/pypoetry/venv/lib/python3.9/site-packages/installer/sources.py:289 in validate_record
      285│                         f""In {self._zipfile.filename}, hash / size of {item.filename} didn't match RECORD""
      286│                     )
      287│ 
      288│         if issues:
    → 289│             raise _WheelFileValidationError(issues)
      290│ 
      291│     def get_contents(self) -> Iterator[WheelContentElement]:
      292│         """"""Sequential access to all contents of the wheel (including dist-info files).
```

See the issue here https://github.com/python-poetry/poetry/issues/7692

I think it might be traced back to https://github.com/python-poetry/poetry/pull/7671

There might be an easy fix https://github.com/python-poetry/poetry/issues/7692#issuecomment-1476567941


"
PAIR-code/what-if-tool,"Unable to import Witwidget: https://github.com/PAIR-code/what-if-tool/issues/224
Description: I am trying to import witwidget on jupyter notebook but it's giving me an error **ModuleNotFoundError** even though witwidget is installed properly

![image](https://user-images.githubusercontent.com/58506467/206202528-9e0cab9e-ffc1-426e-83eb-b41c01145a6f.png)



"
PAIR-code/what-if-tool,"Error when trying to reproduce example notebook locally: https://github.com/PAIR-code/what-if-tool/issues/221
Description: I am trying to re-run locally the notebook provided in the tutorial section: https://colab.research.google.com/github/pair-code/what-if-tool/blob/master/WIT_Age_Regression.ipynb 

And I have got the following error:


```
[Open Browser Console for more detailed log - Double click to close this message]
Failed to create view for 'WITView' from module 'wit-widget' with model 'DOMWidgetModel' from module '@jupyter-widgets/base'
n@http://localhost:8888/static/lab/3790.af02260a4ebe28e0dd86.js?v=af02260a4ebe28e0dd86:1:49265
14416/create_view/e.state_change<@http://localhost:8888/static/lab/4416.9d6d0a2f3f9ed5d7b141.js?v=9d6d0a2f3f9ed5d7b141:1:4346

```
Any idea on what may be going on? I am using Jupyter-lab, and use the install procedures as described [here](https://pair-code.github.io/what-if-tool/learn/tutorials/notebooks/).

"
PAIR-code/what-if-tool,"UnboundLocalError(""local variable 'values' referenced before assignment"") : https://github.com/PAIR-code/what-if-tool/issues/216
Description: I have the training dataframe which has feature columns, preds (predicted score) and label column (ground truth). There is no nan values, all of them are float64.

I convert the dataframe to tf examples according to the reference notebook provided by what-if,

```
# Converts a dataframe into a list of tf.Example protos.
import tensorflow as tf
def df_to_examples(df, columns=None):
    examples = []
    if columns == None:
        columns = df.columns.values.tolist()
    for index, row in df.iterrows():
        example = tf.train.Example()
        for col in columns:
            if df[col].dtype is np.dtype(np.int64):
                example.features.feature[col].int64_list.value.append(int(row[col]))
            elif df[col].dtype is np.dtype(np.float64):
                example.features.feature[col].float_list.value.append(row[col])
            else:
              example.features.feature[col].float_list.value.append(row[col]) # just float64 instead of numpy.float64
        examples.append(example)
    return examples
```
```
num_datapoints = 2000
test_examples = df_to_examples(test_features[0:num_datapoints])
```

```
config_builder = WitConfigBuilder(
    test_examples,
    feature_names=features + ['label', 'preds']).set_custom_predict_fn(
  model.predict).set_target_feature('label')
```

Once the tool is launched, and I try to click on the predict button, I get the following error:
UnboundLocalError(""local variable 'values' referenced before assignment"") 

Screenshot: https://ibb.co/fG3STcP

"
PAIR-code/what-if-tool,"how to install what if in managed notebook of vertex ai?: https://github.com/PAIR-code/what-if-tool/issues/215
Description: It seems what-if is not available directly in managed notebooks? How can I install it?
I have tried multiple times, but either I get permission error (don't have sudo), or something like this happens:
![Screenshot 2022-10-03 at 11 28 04 PM](https://user-images.githubusercontent.com/10291263/193641632-4f3e2603-4862-48cd-96bc-c5ec35d823b9.png)


"
PAIR-code/what-if-tool,"Multiclass classification: https://github.com/PAIR-code/what-if-tool/issues/205
Description: Hi Team,

I see that there is one demo prepared for the multiclass classification of Iris dataset. I was trying to do the same, and added the option of ` .set_multi_class(True) `in the WitConfigBuilder.
But in my output report I am seeing the page as below:

![image](https://user-images.githubusercontent.com/49306345/164002562-f2d4e244-02f8-4b8c-9757-a80a3f73b02c.png)

This output does not see right. 
Is there something that I am missing. Below is the adjust_prediction function i am using:
```
def adjust_prediction(x):
  x_df = pd.DataFrame(x, columns=data.columns.tolist())
  return bst.predict_proba(x_df)
```

Is there any sample notebook running behind Iris demo?

"
PAIR-code/what-if-tool,"Not getting the desired output with Hugging face models on NLP tasks: https://github.com/PAIR-code/what-if-tool/issues/203
Description: I am using Google Colab to run on simple experiment. 

The idea is to visulaize attention weights and predictions on text data.

Here is the code

```
!pip install witwidget
!pip install transformers
!pip install sentence_transformers
from transformers import BertTokenizer, BertForSequenceClassification,BertForMaskedLM
from sentence_transformers import SentenceTransformer
from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM
from transformers import RobertaTokenizer, RobertaForMaskedLM
import torch
import transformers
import sys
import pandas as pd
import numpy as np


tokenizer1=BertTokenizer.from_pretrained('bert-base-uncased')
tokenizer2=BertTokenizer.from_pretrained('bert-base-cased')
model1=BertForSequenceClassification.from_pretrained('bert-base-uncased',return_dict=True)
model2=BertForSequenceClassification.from_pretrained('bert-base-cased',return_dict=True)
```

```
# list of strings
lst = [[""learn the whatif too"",1], [""trying to experiment with whatiftool"",1],[""Some weights of the model checkpoint at bert-base-uncased were not used"",0],
       [""This IS expected if you are initializing BertForMaskedLM"",0]]

# Calling DataFrame constructor on list
df = pd.DataFrame(lst,columns=['text','labels'])
df
```


```

text | labels
-- | --
learn the whatif too | 1
trying to experiment with whatiftool | 1
Some weights of the model checkpoint at bert-b... | 0
This IS expected if you are initializing BertF... | 0

```



```
#@title Define custom prediction functions so that WIT infers using keras models
import tensorflow as tf

# Set up model helper functions:
# Convert list of tf.Examples to list of comment strings.
def examples_to_strings(examples):
  texts = [ex.features.feature['text'].bytes_list.value[0] for ex in examples]
  labels=[ex.features.feature['labels'].int64_list.value[0] for ex in examples]
  labels=torch.tensor(labels).unsqueeze(0)
  if sys.version_info >= (3, 0):
    texts = [t.decode('utf-8') for t in texts]
  return texts,labels

# Get raw string out of tf.Example and prepare it for keras model input
def examples_to_model_in(examples, tokenizer):
  texts,labels = examples_to_strings(examples)
  print(texts,labels)
  # Tokenize string into fixed length sequence of integer based on tokenizer 
  # and model padding
  model_ins = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=""pt"")
  # model_ins = tf.keras.preprocessing.sequence.pad_sequences(
  #     text_sequences, maxlen=PADDING_LEN)
  return model_ins,labels

# WIT predict functions:
def custom_predict_1(examples_to_infer):
  model_ins,labels = examples_to_model_in(examples_to_infer, tokenizer1)
  preds = model1(**model_ins,labels=labels)
  print(preds)
  return preds

def custom_predict_2(examples_to_infer):
  model_ins = examples_to_model_in(examples_to_infer, tokenizer2)
  preds = model2(**model_ins,labels=labels)
  return preds

```



```

**Example**
train_encodings = tokenizer1(lst, truncation=True, padding=True, max_length=512, return_tensors=""pt"")
print(""encoding done"")
{'input_ids': tensor([[  101,  4553,  1996,  2054, 10128,  2205,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  2667,  2000,  7551,  2007,  2054, 10128,  3406,  4747,   102,
             0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  2070, 15871,  1997,  1996,  2944, 26520,  2012, 14324,  1011,
          2918,  1011,  4895, 28969,  2020,  2025,  2109,   102],
        [  101,  2023,  2003,  3517,  2065,  2017,  2024,  3988,  6026, 14324,
         14192, 19895,  2098, 13728,   102,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}
```

```
def make_label_column_numeric(df, label_column, test):
  df[label_column] = np.where(test(df[label_column]), 1, 0)


import numpy as np
import tensorflow as tf

# Converts a dataframe into a list of tf.Example protos.
def df_to_examples(df, columns=None):
  examples = []
  if columns == None:
    columns = df.columns.values.tolist()
  for index, row in df.iterrows():
    example = tf.train.Example()
    for col in columns:
      if df[col].dtype is np.dtype(np.int64):
        example.features.feature[col].int64_list.value.append(int(row[col]))
      elif df[col].dtype is np.dtype(np.float64):
        example.features.feature[col].float_list.value.append(row[col])
      elif row[col] == row[col]:
        example.features.feature[col].bytes_list.value.append(row[col].encode('utf-8'))
    examples.append(example)
  return examples

label_column = 'labels'
make_label_column_numeric(df, label_column, lambda val: val)


import tensorflow as tf
import tensorflow_hub as hub

embed = hub.load(""https://tfhub.dev/google/universal-sentence-encoder/4"")

# For this use-case, we set distance between datapoints to be cosine distance
# between unit-normalized embeddings of each datapoint from the tf.Hub
# Universal Sentence Encoder.
def universal_sentence_encoder_distance(input_example, examples_to_compare, _):
  # Extract comment strings
  input_sentence = examples_to_strings([input_example])[0]
  sentences = examples_to_strings(examples_to_compare)

  # Normalize all embeddings for cosine distance operation
  input_emb = tf.squeeze(tf.nn.l2_normalize(embed([input_sentence]), axis=1))
  sentences_emb = tf.nn.l2_normalize(embed(sentences), axis=1)

  # Tile the input example for easy comparison to all examples
  multiply = tf.constant([len(examples_to_compare)])
  input_matrix = tf.reshape(tf.tile(input_emb, multiply),
                            [multiply[0], tf.shape(input_emb)[0]])
  
  # Compute cosine distance from input example to all examples.
  cosine_distance = tf.keras.losses.CosineSimilarity(
      axis=1, reduction=tf.losses.Reduction.NONE)
  distances = cosine_distance(sentences_emb, input_matrix)
  results = tf.squeeze(distances)
  return results.numpy().tolist()




custom_predict_1(examples)

['learn the whatif too', 'trying to experiment with whatiftool', 'Some weights of the model checkpoint at bert-base-uncased were not used', 'This IS expected if you are initializing BertForMaskedLM'] tensor([[1, 1, 0, 0]])
SequenceClassifierOutput(loss=tensor(0.7033, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2599, -0.4994],
        [-0.2791, -0.5455],
        [-0.2001, -0.6373],
        [-0.2547, -0.3226]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
SequenceClassifierOutput([('loss', tensor(0.7033, grad_fn=<NllLossBackward0>)),
                          ('logits', tensor([[-0.2599, -0.4994],
                                   [-0.2791, -0.5455],
                                   [-0.2001, -0.6373],
                                   [-0.2547, -0.3226]], grad_fn=<AddmmBackward0>))])

#@title Invoke What-If Tool for the data and two models (Note that this step may take a while due to prediction speed of the toxicity model){display-mode: ""form""}
from witwidget.notebook.visualization import WitWidget, WitConfigBuilder
num_datapoints = 4  #@param {type: ""number""}
tool_height_in_px = 720  #@param {type: ""number""}

# Setup the tool with the test examples and the trained classifier
config_builder = WitConfigBuilder(examples[:num_datapoints]).set_custom_predict_fn(
  custom_predict_1).set_compare_custom_predict_fn(custom_predict_2).set_custom_distance_fn(
      universal_sentence_encoder_distance)

wv = WitWidget(config_builder, height=tool_height_in_px)
```

"
PAIR-code/what-if-tool,"WIT works on my jupyter notebook, but it doesn't on vscode.: https://github.com/PAIR-code/what-if-tool/issues/196
Description: Hi, I tried to use WIT in vscode(Visual Studio Code), but witwidget didn't display on it.

<img src=https://user-images.githubusercontent.com/60982832/151500565-5b1ea2c7-8d28-4ac6-8f9e-094d61932840.PNG width=80%>

So i executed the same code in my jupyter notebook, it worked.

<img src=https://user-images.githubusercontent.com/60982832/151501344-5b4ccc7c-7b1f-4b73-96a9-07978b4c35a3.PNG width=80%>

Do i install any package or extension on vscode?
"
VIDA-NYU/PipelineVis,"Colab breaks : https://github.com/VIDA-NYU/PipelineVis/issues/16
Description: IncorrectPackageVersionError: found 'scikit-learn' version 0.22.2.post1 but requires scikit-learn version >=0.24.0

Even I checked the version of 0.24.0 is installed.

So, I tried to run it locally over the Ubuntu, it always said AttributeError: 'NoneType' object has no attribute 'expanduser'. Also, I tried to save it as HTML, but looks like some texts overflowed.

"
VIDA-NYU/PipelineVis,"The colab example breaks: https://github.com/VIDA-NYU/PipelineVis/issues/9
Description: The auto-sklearn classification colab example breaks because of a versioning issue: sklearn 0.24 is required.

"
VIDA-NYU/PipelineVis,"Functionality outside jupyter notebook: https://github.com/VIDA-NYU/PipelineVis/issues/8
Description: The output of plot_pipeline_matrix is interesting to keep as an explanatory artifact of the final model. Although this artifact would be much more interesting if it were a standalone visualization instead of a notebook dependent widget.

"
VIDA-NYU/PipelineVis,"Colab precision: https://github.com/VIDA-NYU/PipelineVis/issues/6
Description: In Colab, hyperparameter values show only two digits after the decimal which is not enough for things like tol or learning rates. Is there a way to change this?

"
VIDA-NYU/PipelineVis,"Getting error - Cannot find Jupyter/Colab namespace for Python: https://github.com/VIDA-NYU/PipelineVis/issues/3
Description: I am trying to use PipelineProfiler to visualize pipelines in auto-sklearn. I am developing using docker, but I am getting the error ""Cannot find Jupyter/Colab namespace for Python"". Is there something that I am missing?
"
interpretml/interpret,"Internal seed of EBM seems to be not fixed: https://github.com/interpretml/interpret/issues/424
Description: Hello, I am a user of Explainable Boosting Machine, and thanks to your effort I was trying to make an interpretation out of the survey data. However, it turned out that although I fixed the seed the scores of score boards are not reproducible. I get different score boards every time I build an EBM model with the same used seed with other settings fixed the same as well. The performance of the model (ex. f1-score, accuracy and etc.) however, is the same and thus reproducible. I was trying to visualize the scoreboard again in my colab, but I wasn't able to get the same scoreboard as before.

I'll wait for your response.
Thank you :)

"
interpretml/interpret,"returned error ""Pickling an AuthenticationString object is disallowed for security reasons"": https://github.com/interpretml/interpret/issues/416
Description: Hi teams,

currently i'm making framework on re-tune  ebm parameters on top of streamlit.
i get this error `""Pickling an AuthenticationString object is disallowed for security reasons""`

full error code is:
Traceback:
```
File ""C:\Users\aiforesee\.virtualenvs\frameworks\lib\site-packages\streamlit\scriptrunner\script_runner.py"", line 557, in _run_script
    exec(code, module.__dict__)
File ""C:\Users\aiforesee\PycharmProjects\frameworks\remodel.py"", line 192, in <module>
    main()
File ""C:\Users\aiforesee\PycharmProjects\frameworks\remodel.py"", line 131, in main
    model.fit(model_data,model_target)
File ""C:\Users\aiforesee\.virtualenvs\frameworks\lib\site-packages\interpret\glassbox\ebm\ebm.py"", line 1003, in fit
    estimators = provider.parallel(BaseCoreEBM.fit_parallel, train_model_args_iter)
File ""C:\Users\aiforesee\.virtualenvs\frameworks\lib\site-packages\interpret\provider\compute.py"", line 20, in parallel
    results = Parallel(n_jobs=self.n_jobs)(
File ""C:\Users\aiforesee\.virtualenvs\frameworks\lib\site-packages\joblib\parallel.py"", line 1085, in __call__
    if self.dispatch_one_batch(iterator):
File ""C:\Users\aiforesee\.virtualenvs\frameworks\lib\site-packages\joblib\parallel.py"", line 901, in dispatch_one_batch
    self._dispatch(tasks)
File ""C:\Users\aiforesee\.virtualenvs\frameworks\lib\site-packages\joblib\parallel.py"", line 819, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
File ""C:\Users\aiforesee\.virtualenvs\frameworks\lib\site-packages\joblib\_parallel_backends.py"", line 556, in apply_async
    future = self._workers.submit(SafeFunction(func))
File ""C:\Users\aiforesee\.virtualenvs\frameworks\lib\site-packages\joblib\externals\loky\reusable_executor.py"", line 176, in submit
    return super().submit(fn, *args, **kwargs)
File ""C:\Users\aiforesee\.virtualenvs\frameworks\lib\site-packages\joblib\externals\loky\process_executor.py"", line 1149, in submit
    self._ensure_executor_running()
File ""C:\Users\aiforesee\.virtualenvs\frameworks\lib\site-packages\joblib\externals\loky\process_executor.py"", line 1123, in _ensure_executor_running
    self._adjust_process_count()
File ""C:\Users\aiforesee\.virtualenvs\frameworks\lib\site-packages\joblib\externals\loky\process_executor.py"", line 1111, in _adjust_process_count
    p.start()
File ""C:\Users\aiforesee\AppData\Local\Programs\Python\Python39\lib\multiprocessing\process.py"", line 121, in start
    self._popen = self._Popen(self)
File ""C:\Users\aiforesee\.virtualenvs\frameworks\lib\site-packages\joblib\externals\loky\backend\process.py"", line 32, in _Popen
    return Popen(process_obj)
File ""C:\Users\aiforesee\.virtualenvs\frameworks\lib\site-packages\joblib\externals\loky\backend\popen_loky_win32.py"", line 96, in __init__
    reduction.dump(process_obj, to_child)
File ""C:\Users\aiforesee\.virtualenvs\frameworks\lib\site-packages\joblib\externals\loky\backend\reduction.py"", line 204, in dump
    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)
File ""C:\Users\aiforesee\.virtualenvs\frameworks\lib\site-packages\joblib\externals\cloudpickle\cloudpickle_fast.py"", line 632, in dump
    return Pickler.dump(self, obj)
File ""C:\Users\aiforesee\AppData\Local\Programs\Python\Python39\lib\multiprocessing\process.py"", line 347, in __reduce__
    raise TypeError: Pickling an AuthenticationString object is disallowed for security reasons
```

then I traceback the error and seek on  .**fit()** function seems that runs the boosting tree part on each variables parallel (which is reasonable) 
now my question is.. is there any workaround to solve this issue?

additional info: if i just retune the model parameters in notebook, it works just fine.. 
but i'm so curious why does it affect when i do the exact code in streamlit apps

"
interpretml/interpret,"Backlog: https://github.com/interpretml/interpret/issues/400
Description: This issue is to maintain links to all feature requests and bugs in one place. Issues listed here should have either the ""enhancement "" or ""bug"" label applied to them.

Issues not listed here are searchable with: [is:issue is:open -label:bug -label:enhancement](https://github.com/interpretml/interpret/issues?q=is%3Aissue+is%3Aopen+-label%3Abug+-label%3Aenhancement+sort%3Aupdated-desc+-created%3A2023-01-14)

## Enhancements, python
- custom validation sets (https://github.com/interpretml/interpret/issues/268)
- add init_score (similar to LightGBM) to the fit function (https://github.com/interpretml/interpret/issues/364)
- add ""warm_start"" parameter to enable stage customization during fitting (https://github.com/interpretml/interpret/issues/304).  Also see issue # 403
- add a prediction function that also calculates uncertainty in the prediction (https://github.com/interpretml/interpret/issues/235)
- avoid resetting the index on call to show after explain_local (https://github.com/interpretml/interpret/issues/72)
- user defined labels for local explanation samples (https://github.com/interpretml/interpret/issues/300)
- change boosting algorithm to stop it from producing graphs with large positive or negative scores in some regions (https://github.com/interpretml/interpret/issues/122)
- custom loss functions (https://github.com/interpretml/interpret/issues/281)
- alternate link functions (https://github.com/interpretml/interpret/issues/137)
- ranking (https://github.com/interpretml/interpret/issues/181)
- monotonicity constraints in both the training phase and as a post-processing utility (https://github.com/interpretml/interpret/issues/184)
- Dask arrays for large datasets (https://github.com/interpretml/interpret/issues/249)
- progress indication during training (https://github.com/interpretml/interpret/issues/252)
- continuous output of metrics during training (https://github.com/interpretml/interpret/issues/7)
- Spyder support for show function (https://github.com/interpretml/interpret/issues/191)
- UI to show missing value scores (https://github.com/interpretml/interpret/issues/18)
- show error bars for interactions in the UI, perhaps with a hover (https://github.com/interpretml/interpret/issues/127)
- Improve contrast of interaction heatmaps (https://github.com/interpretml/interpret/issues/236)
- dashboard data disappears when changing tabs (https://github.com/interpretml/interpret/issues/106)
- add permutation Importance (https://github.com/interpretml/interpret/issues/26)
- alternative feature importance techniques (https://github.com/interpretml/interpret/issues/374)
- add categorical support to Morris Sensitivity Analysis (https://github.com/interpretml/interpret/issues/275)
- alternate base estimators such as linear models, polynomial regression, or splines (https://github.com/interpretml/interpret/issues/185)

## Enhancements, R
- local explanations in R (https://github.com/interpretml/interpret/issues/222)
- R multiple new features (https://github.com/interpretml/interpret/issues/294)
- add threading to R and in general move the parallelism into C++ (https://github.com/interpretml/interpret/issues/180)

## Bugs
- predictions in R different from python (https://github.com/interpretml/interpret/issues/417)
- DataException: MissingColumnsInData, Expected column(s) 0 not found in fitted data (https://github.com/interpretml/interpret/issues/209)
- RandomizedSearchCV doesn't work with InterpretML LogisticRegression (https://github.com/interpretml/interpret/issues/301)
- categorical visualize bug (https://github.com/interpretml/interpret/issues/119)
- lime results may differ between InterpretML and LIME package (https://github.com/interpretml/interpret/issues/133)
- multiclass LIME (https://github.com/interpretml/interpret/issues/306)
- LimeTabular seems to require Pandas dataframe for categoricals (https://github.com/interpretml/interpret/issues/322)
- make notebooks work in Kaggle (https://github.com/interpretml/interpret/issues/216)
- Categorical features in PartialDependence (https://github.com/interpretml/interpret/issues/217)
- show does not display any output in some settings (https://github.com/interpretml/interpret/issues/229)
- ClassHistogram().explain_data() having index error (https://github.com/interpretml/interpret/issues/274)
- raise an exception when the multi-model dashboard is used inside the InlineProvider (https://github.com/interpretml/interpret/issues/333)
- in some scenarios showing multiple EBM dashboards could fail (https://github.com/interpretml/interpret/issues/350)
- feature values above 999000000000000 cannot be visualized (https://github.com/interpretml/interpret/issues/370)
"
jnowak90/GraVisGUI,"Open up API: https://github.com/jnowak90/GraVisGUI/issues/4
Description: Firstly, I really like how easily you can process files with the application. For my own needs though I would love to be able to access the analysis tools from code. Would it at all be possible to make the analysis functions availible as a python library so users can apply them from notebooks, python scripts, etc.? I think that this kind of shape analysis could be a really nice feature extraction method to complement bioimage analysis workflows!
"
wandb/wandb,"fix(artifacts): default to project ""uncategorized"" instead of ""None"" when fetching artifacts: https://github.com/wandb/wandb/pull/5375
Description: Fixes WB-13393

## Description

When creating an artifact, we default to project ""uncategorized"". When fetching an artifact, we default to project ""None"".

![Untitled (1)](https://user-images.githubusercontent.com/127154459/233065752-af9a803c-bb24-493e-a0b6-0ff2ccb98a38.png)

I changed the latter to also use ""uncategorized"".

## Test plan

- Started a Jupyter notebook:
```
$ tox -e py39 --notest
$ source .tox/py39/bin/activate
$ pip install jupyter
$ jupyter notebook
```
- Fetched an artifact from ""uncategorized"" project:
<img width=""1152"" alt=""Untitled"" src=""https://user-images.githubusercontent.com/127154459/233064991-3ec8deb5-67d0-4755-8a91-259d46c6e3e5.png"">
"
sid-the-coder/QuickDA,"Bump nbconvert from 6.0.7 to 6.5.1: https://github.com/sid-the-coder/QuickDA/pull/27
Description: Bumps [nbconvert](https://github.com/jupyter/nbconvert) from 6.0.7 to 6.5.1.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/jupyter/nbconvert/releases"">nbconvert's releases</a>.</em></p>
<blockquote>
<h2>Release 6.5.1</h2>
<p>No release notes provided.</p>
<h2>6.5.0</h2>
<h2>What's Changed</h2>
<ul>
<li>Drop dependency on testpath. by <a href=""https://github.com/anntzer""><code>@​anntzer</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1723"">jupyter/nbconvert#1723</a></li>
<li>Adopt pre-commit by <a href=""https://github.com/blink1073""><code>@​blink1073</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1744"">jupyter/nbconvert#1744</a></li>
<li>Add pytest settings and handle warnings by <a href=""https://github.com/blink1073""><code>@​blink1073</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1745"">jupyter/nbconvert#1745</a></li>
<li>Apply Autoformatters by <a href=""https://github.com/blink1073""><code>@​blink1073</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1746"">jupyter/nbconvert#1746</a></li>
<li>Add git-blame-ignore-revs by <a href=""https://github.com/blink1073""><code>@​blink1073</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1748"">jupyter/nbconvert#1748</a></li>
<li>Update flake8 config by <a href=""https://github.com/blink1073""><code>@​blink1073</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1749"">jupyter/nbconvert#1749</a></li>
<li>support bleach 5, add packaging and tinycss2 dependencies by <a href=""https://github.com/bollwyvl""><code>@​bollwyvl</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1755"">jupyter/nbconvert#1755</a></li>
<li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1752"">jupyter/nbconvert#1752</a></li>
<li>update cli example by <a href=""https://github.com/leahecole""><code>@​leahecole</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1753"">jupyter/nbconvert#1753</a></li>
<li>Clean up pre-commit by <a href=""https://github.com/blink1073""><code>@​blink1073</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1757"">jupyter/nbconvert#1757</a></li>
<li>Clean up workflows by <a href=""https://github.com/blink1073""><code>@​blink1073</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1750"">jupyter/nbconvert#1750</a></li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> made their first contribution in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1752"">jupyter/nbconvert#1752</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/jupyter/nbconvert/compare/6.4.5...6.5"">https://github.com/jupyter/nbconvert/compare/6.4.5...6.5</a></p>
<h2>6.4.3</h2>
<h2>What's Changed</h2>
<ul>
<li>Add section to <code>customizing</code> showing how to use template inheritance by <a href=""https://github.com/stefanv""><code>@​stefanv</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1719"">jupyter/nbconvert#1719</a></li>
<li>Remove ipython genutils by <a href=""https://github.com/rgs258""><code>@​rgs258</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1727"">jupyter/nbconvert#1727</a></li>
<li>Update changelog for 6.4.3 by <a href=""https://github.com/blink1073""><code>@​blink1073</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1728"">jupyter/nbconvert#1728</a></li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a href=""https://github.com/stefanv""><code>@​stefanv</code></a> made their first contribution in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1719"">jupyter/nbconvert#1719</a></li>
<li><a href=""https://github.com/rgs258""><code>@​rgs258</code></a> made their first contribution in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1727"">jupyter/nbconvert#1727</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/jupyter/nbconvert/compare/6.4.2...6.4.3"">https://github.com/jupyter/nbconvert/compare/6.4.2...6.4.3</a></p>
<h2>6.4.0</h2>
<h2>What's Changed</h2>
<ul>
<li>Optionally speed up validation by <a href=""https://github.com/gwincr11""><code>@​gwincr11</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1672"">jupyter/nbconvert#1672</a></li>
<li>Adding missing div compared to JupyterLab DOM structure by <a href=""https://github.com/SylvainCorlay""><code>@​SylvainCorlay</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1678"">jupyter/nbconvert#1678</a></li>
<li>Allow passing extra args to code highlighter by <a href=""https://github.com/yuvipanda""><code>@​yuvipanda</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1683"">jupyter/nbconvert#1683</a></li>
<li>Prevent page breaks in outputs when printing by <a href=""https://github.com/SylvainCorlay""><code>@​SylvainCorlay</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1679"">jupyter/nbconvert#1679</a></li>
<li>Add collapsers to template by <a href=""https://github.com/SylvainCorlay""><code>@​SylvainCorlay</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1689"">jupyter/nbconvert#1689</a></li>
<li>Fix recent pandoc latex tables by adding calc and array (<a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1536"">#1536</a>, <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1566"">#1566</a>) by <a href=""https://github.com/cgevans""><code>@​cgevans</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1686"">jupyter/nbconvert#1686</a></li>
<li>Add an invalid notebook error by <a href=""https://github.com/gwincr11""><code>@​gwincr11</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1675"">jupyter/nbconvert#1675</a></li>
<li>Fix typos in execute.py by <a href=""https://github.com/TylerAnderson22""><code>@​TylerAnderson22</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1692"">jupyter/nbconvert#1692</a></li>
<li>Modernize latex greek math handling (partially fixes <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1673"">#1673</a>) by <a href=""https://github.com/cgevans""><code>@​cgevans</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1687"">jupyter/nbconvert#1687</a></li>
<li>Fix use of deprecated API and update test matrix by <a href=""https://github.com/blink1073""><code>@​blink1073</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1696"">jupyter/nbconvert#1696</a></li>
<li>Update nbconvert_library.ipynb by <a href=""https://github.com/letterphile""><code>@​letterphile</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1695"">jupyter/nbconvert#1695</a></li>
<li>Changelog for 6.4 by <a href=""https://github.com/blink1073""><code>@​blink1073</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1697"">jupyter/nbconvert#1697</a></li>
</ul>
<h2>New Contributors</h2>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/jupyter/nbconvert/commit/7471b75a506b2fec776613e50e4f2234b97f3c8e""><code>7471b75</code></a> Release 6.5.1</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/c1943e0e9fd0ad6abd7d8dae380474cca4b04a31""><code>c1943e0</code></a> Fix pre-commit</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/8685e9378086e8d82a0df92505fe386095f929ad""><code>8685e93</code></a> Fix tests</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/0abf2906bc6c7170c8d70bc0df6995d21c5aeaf1""><code>0abf290</code></a> Run black and prettier</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/418d545ae596d95f5ea82d141c68fd1abc99f1a6""><code>418d545</code></a> Run test on 6.x branch</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/bef65d7ab2a469b01e4aa25f44c0f20326f7c7c5""><code>bef65d7</code></a> Convert input to string prior to escape HTML</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/0818628718c4a5d3ddd671fbd4881bf176e7d6e2""><code>0818628</code></a> Check input type before escaping</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/b206470f9ecd71b006a37dd1298dd3d9e3dd46dd""><code>b206470</code></a> GHSL-2021-1017, GHSL-2021-1020, GHSL-2021-1021</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/a03cbb8a8d04d47aefec51e7b1b816045682aed5""><code>a03cbb8</code></a> GHSL-2021-1026, GHSL-2021-1025</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/48fe71eb3335caf4e03166e56e0d16efcfbeaf44""><code>48fe71e</code></a> GHSL-2021-1024</li>
<li>Additional commits viewable in <a href=""https://github.com/jupyter/nbconvert/compare/6.0.7...6.5.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=nbconvert&package-manager=pip&previous-version=6.0.7&new-version=6.5.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/sid-the-coder/QuickDA/network/alerts).

</details>

"
sid-the-coder/QuickDA,"Bump nbconvert from 6.0.7 to 6.3.0: https://github.com/sid-the-coder/QuickDA/pull/26
Description: Bumps [nbconvert](https://github.com/jupyter/nbconvert) from 6.0.7 to 6.3.0.
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/jupyter/nbconvert/commit/cefe0bfe303e5e9e194c393cb9280c64a77b8219""><code>cefe0bf</code></a> Release 6.3.0</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/a534fb901ff83e0b0c0c082ff47f3de01dc651b1""><code>a534fb9</code></a> Release 6.3.0b0</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/87920c5a47c8ae99600be6c9b9b909ba440adce9""><code>87920c5</code></a> Add changelog for 6.3.0 (<a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1669"">#1669</a>)</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/dd6d9c7d36d0a09db647a8fc993f7330388a1e48""><code>dd6d9c7</code></a> add slide numbering (<a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1654"">#1654</a>)</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/5d2c5e2b79534c11678b73e707feb74d7827a557""><code>5d2c5e2</code></a> Update state filter (<a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1664"">#1664</a>)</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/11ea5931f71fdaaaad8958f634132f45476bf006""><code>11ea593</code></a> fix: avoid closing the script tag early by escaping a forward slash (<a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1665"">#1665</a>)</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/968c5fbabaf99f83d64720a1a6e90969052e978c""><code>968c5fb</code></a> Fix HTML templates mentioned in help docs (<a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1653"">#1653</a>)</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/35c4d07eb7060b505412c0ad83886176fe8409fe""><code>35c4d07</code></a> Add a new output filter that excludes widgets if there is no state (<a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1643"">#1643</a>)</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/c663c75339709c0e1c051d684dba0cf10fa9083e""><code>c663c75</code></a> 6.2.0</li>
<li><a href=""https://github.com/jupyter/nbconvert/commit/fd1dd15b63bfd898c21c90b78165c4c00c448896""><code>fd1dd15</code></a> 6.2.0rc2</li>
<li>Additional commits viewable in <a href=""https://github.com/jupyter/nbconvert/compare/6.0.7...6.3.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=nbconvert&package-manager=pip&previous-version=6.0.7&new-version=6.3.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/sid-the-coder/QuickDA/network/alerts).

</details>

"
sid-the-coder/QuickDA,"Bump notebook from 6.1.5 to 6.4.12: https://github.com/sid-the-coder/QuickDA/pull/21
Description: Bumps [notebook](http://jupyter.org) from 6.1.5 to 6.4.12.


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=notebook&package-manager=pip&previous-version=6.1.5&new-version=6.4.12)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/sid-the-coder/QuickDA/network/alerts).

</details>

"
sid-the-coder/QuickDA,"Bump notebook from 6.1.5 to 6.4.10: https://github.com/sid-the-coder/QuickDA/pull/20
Description: Bumps [notebook](http://jupyter.org) from 6.1.5 to 6.4.10.


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=notebook&package-manager=pip&previous-version=6.1.5&new-version=6.4.10)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/sid-the-coder/QuickDA/network/alerts).

</details>

"
sid-the-coder/QuickDA,"Bump notebook from 6.1.5 to 6.4.1: https://github.com/sid-the-coder/QuickDA/pull/15
Description: Bumps [notebook](http://jupyter.org) from 6.1.5 to 6.4.1.


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=notebook&package-manager=pip&previous-version=6.1.5&new-version=6.4.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/sid-the-coder/QuickDA/network/alerts).

</details>

"
sid-the-coder/QuickDA,"Bump notebook from 6.0.3 to 6.1.5: https://github.com/sid-the-coder/QuickDA/pull/2
Description: Bumps [notebook](https://github.com/jupyter/jupyterhub) from 6.0.3 to 6.1.5.
<details>
<summary>Commits</summary>
<ul>
<li>See full diff in <a href=""https://github.com/jupyter/jupyterhub/commits"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=notebook&package-manager=pip&previous-version=6.0.3&new-version=6.1.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/sid-the-coder/QuickDA/network/alerts).

</details>
"
Mr-Milk/SpatialTis,"errors: https://github.com/Mr-Milk/SpatialTis/issues/27
Description: Hi, this looks like a really useful package, I hope you are able to continue developing it!
I noted there were some bugs between the ```pip install spatialtis``` and ```pip install 'spatialtis[all]'``` with the capitilisation of Config/CONFIG between the two when calling this function throughout.
This issue is resolved in the latest github repository and I have been exploring the spatial ```.st.xxx``` function in the imc_data.h5ad dataset in your tutorial notebooks, however I see get some errors running the ```.sp.xxx``` code from your notebooks, which I have edited in line with the API readthedocs.
shown here for 
sp.cell_components and sp.cell_co_occurrence.

these throw the same errors with my dataset - have there been changes to the sp. functions?


```st.Config```
![image](https://user-images.githubusercontent.com/66101861/220518563-b80f27ba-5b20-4236-817d-06bcea2b423b.png)

![image](https://user-images.githubusercontent.com/66101861/220518755-64148411-8e89-4d34-824b-b84243f10392.png)

```islets_cells = ['gamma', 'delta', 'alpha', 'beta']
sp.cell_components(data, groupby=['stage', 'part'], key='cell_components')

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[55], line 2
      1 islets_cells = ['gamma', 'delta', 'alpha', 'beta']
----> 2 sp.cell_components(data, groupby=['stage', 'part'])

File ~/miniconda3/envs/spatialtis/lib/python3.9/site-packages/spatialtis/plotting/basic.py:42, in cell_components(data, groupby, key, orient, type_order, **plot_options)
     39 data = data.groupby(groupby).sum().melt(
     40     ignore_index=False, value_name=""Count"").reset_index()
     41 data = data.rename(columns={'cell type': 'Cell Type'})
---> 42 return stacked_bar(data,
     43                    group=groupby,
     44                    value=""Count"",
     45                    stacked=""Cell Type"",
     46                    orient=orient,
     47                    **plot_options)

File ~/miniconda3/envs/spatialtis/lib/python3.9/site-packages/milkviz/_stacked_bar.py:88, in stacked_bar(data, group, value, stacked, orient, group_order, stacked_order, percentage, barwidth, cmap, colors, show_values, props, legend_kw, ax)
     86     stacked_order = natsorted(data[stacked].unique())
     87 if group_order is None:
---> 88     group_order = natsorted(data[group].unique())
     90 _, legend_labels, legend_colors = \
     91     cat_colors(stacked_order, stacked_order, cmap, colors)
     93 start_x = 1

File ~/miniconda3/envs/spatialtis/lib/python3.9/site-packages/pandas/core/generic.py:5902, in NDFrame.__getattr__(self, name)
   5895 if (
   5896     name not in self._internal_names_set
   5897     and name not in self._metadata
   5898     and name not in self._accessors
   5899     and self._info_axis._can_hold_identifiers_and_holds_name(name)
   5900 ):
   5901     return self[name]
-> 5902 return object.__getattribute__(self, name)

AttributeError: 'DataFrame' object has no attribute 'unique



sp.cell_co_occurrence(data, ['stage'], use=""heatmap"")
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[48], line 1
----> 1 sp.cell_co_occurrence(data, ['stage'], use=""heatmap"")

TypeError: cell_co_occurrence() got multiple values for argument 'use'```


"
yifanwu/b2,"B2 not loading because of a missing ""midas"" -> ""b2"" rename: https://github.com/yifanwu/b2/issues/13
Description: After I ran...
```
pip3 install b2-ext
```

Somehow the nb extension config still produced a file with the name ""midas"" in it.
```
$ cat /Library/Frameworks/Python.framework/Versions/3.10/etc/jupyter/nbconfig/notebook.d/b2.json
{
  ""load_extensions"": {
    ""midas/index"": true
  }
}
```

B2 wouldn't load. But when I manually changed ""midas"" to ""b2"" in the file above, it worked.

I'd make a PR to fix this, but, despite searching for ""midas"" in the B2 source, I don't know where it went wrong!

"
yifanwu/b2,"Issues about dashboard pane : https://github.com/yifanwu/b2/issues/12
Description: I tried it on py3.7 in anaconda.
I also had installed b2-ext and datascience.
I can run it on jupyter notebook but I can't see the resizable dashboard pane... 
Can you please tell me what should I do?
![image](https://user-images.githubusercontent.com/44886701/116178340-f6333600-a747-11eb-9b3d-fb85ac73ef97.png)


"
yifanwu/b2,"Issues with launching B2: https://github.com/yifanwu/b2/issues/11
Description: Please keep in mind I'm a complete beginner so this problem might be caused by something on my end.

I tried running B2 on Linux (Ubuntu 20.04 LTS), Python 3.8.

I was told to open and test out the notebooks, so I launched b2-ext on jupyter and opened the B2 folder. Then when I tried to open and run ""Untitled.ipynb"" I kept getting errors when importing B2 and midas. It kept saying ""Import Error: cannot import name ""Midas"" from ""midas"".""Import Error: cannot import name ""B2"" from b2. I also tried typing in ""Import b2"" and running it. For this is got ""ModuleNotFoundError: No module named ""Data Science""."" 

Also, can you please give me something like a short beginner's guide on a proper way to actually run b2? I read all the instructions on github but I can't understand most of it since I'm so new. I would really appreciate it.

Sorry for the inconvenience.




"
networkit/networkit,"How to calculate JaccardDistance in Python?: https://github.com/networkit/networkit/issues/1025
Description: Is it possible to calculate JaccardDistance in Python version of Networkit? It requires list of edge triangle counts, but I could not find how to compute this in the Python version. It is also not covered by examples in Jupyter Notebooks.

"
networkit/networkit,"Documentation: Fixes rendering and typos of ""degree distribution"" paragraph from User-Guide notebook: https://github.com/networkit/networkit/pull/1022
Description: Current errors in the web based display of the User Guide are shown here: https://networkit.github.io/dev-docs/notebooks/User-Guide.html#Degree-Distribution

"
networkit/networkit,"Added a missing piece of information to the documentation: https://github.com/networkit/networkit/pull/1018
Description: The ability to expand a given graph with the Babarasi-Albert generator is already implemented in C++ and in Python as well. But this ability is not reflected in the documentation which mistakes the user into believing that such a thing is not possible. This merge request updates the documentation to reflect the underlying capabilities of the framework.

Contribution Guidelines: 
1. Sorry I could not find a development branch, therefore this pull request is on the master branch.
2. No code has been changed only the documentation, therefore no unit tests have been run on this change.
3. No additional Code in the form of notebooks is provided due to this updating **only** the documentation

Thanks for the great library!
"
cuemacro/chartpy,"Headless Offline Plotting fails: https://github.com/cuemacro/chartpy/issues/3
Description: Hi! Plotting fails when ran headlessly i.e. without iPython notebook, just the python interpreter due to the `iplot` function. 

E.g. `python canvas_demo.py` gives

```Python
/chartpy/engine.py"", line 1449, in plot_chart
    fig = data_frame.iplot(kind=chart_type_ord,
AttributeError: 'DataFrame' object has no attribute 'iplot'
```
"
ucsd-ccbb/visJS2jupyter,"Colab compatibility: https://github.com/ucsd-ccbb/visJS2jupyter/issues/30
Description: Thank you for building this great tool! Is there a way to make visJS2jupyter compatible with Google Colab? I receive an ""unable to connect"" error when I try to visualize using `visualizations.draw_graph_overlap()`.

"
ucsd-ccbb/visJS2jupyter,"Error when exporting notebook to HTML: https://github.com/ucsd-ccbb/visJS2jupyter/issues/26
Description: I am plotting multiple frames in one notebook using different  gaph_id for each frame. The graphs work in the notebook but not if I export the notebook to HTML. Only the last graph will show.  

Also any idea how I can export to html without having to save the style file in the same folder. I would like to show some of the notebooks the same way you show your examples. Thank you

"
ucsd-ccbb/visJS2jupyter,"No setup.py: https://github.com/ucsd-ccbb/visJS2jupyter/issues/24
Description: First of all, thank you for making this package available. It looks really promising (and I'm eager to try it out).

In commit:
https://github.com/ucsd-ccbb/visJS2jupyter/commit/7e1bd04a480e69126561ea761e22f3d46f5eafbd

setup.py was removed. Was that intentional? (I cannot pip install https://github.com/ucsd-ccbb/visJS2jupyter/archive/master.tar.gz) because of missing setup.py

(the reason I'm trying this is because I experience gh-17 when using the version on PyPI).

"
ucsd-ccbb/visJS2jupyter,"Uncaught DOMException: Blocked a frame with origin ""null"" from accessing a cross-origin frame.: https://github.com/ucsd-ccbb/visJS2jupyter/issues/22
Description: I encountered this with a newer version of Jupyter notebook. I think it has to do with a change of the Tornado version.

This is how to replicate:

(pull the most recent image from Docker, 3 days old at the time of writing)
 docker run -p 8888:8888 jupyter/scipy-notebook:6c85e4b43a26

do a !pip install visJS2jupyter

Next, follow the basic example (https://github.com/ucsd-ccbb/visJS2jupyter/blob/master/notebooks/default_params_example/visJS2Jupyter_basic_example.ipynb)

Use f12 to look at the console error.

The line is crashes on is: function init() { window.parent.setUpFrame(); return true; }

I believe that using <iframe sandbox=""allow-same-origin"">  may solve it, but it may need to be called from the Jupyter notebook rather than the iframe itself. In any case, the current implementation looks broken. My way of solving it was to go back to an older version of the notebook stack (I still had an  ~4 months old image cached at my system, so I'm unable to give you the tag number that works).



"
ucsd-ccbb/visJS2jupyter,"Blank space in Jupyter Notebook: https://github.com/ucsd-ccbb/visJS2jupyter/issues/21
Description: Hi! I have a problem with displaying graphs on the cells (I am using Python 3.6 and Anaconda). When I run examples from ""notebooks"" folder I get an empty cell (
![default](https://user-images.githubusercontent.com/32366948/44918305-c0eea400-ad64-11e8-8c34-ea84d5d9eb9e.png)
). But if I export data to a separate HTML file and open it in a browser it's OK (
![default](https://user-images.githubusercontent.com/32366948/44918404-0612d600-ad65-11e8-9156-d8690e34eeef.png)
). What can I do to see the content right on the Jupyter Notebook cells?

"
ucsd-ccbb/visJS2jupyter,"ImportError: No module named 'scipy_heatKernel': https://github.com/ucsd-ccbb/visJS2jupyter/issues/20
Description: When importing `from visJS2jupyter import visualizations` in Python3, I get the above error. 

Culprit is this line - which should work for all I know:

https://github.com/ucsd-ccbb/visJS2jupyter/blob/497dbc34a15eb74c7ef4909e9398b6b53885fd9f/visJS2jupyter/visualizations.py#L22

Nevertheless, changing it to
```python
from visJS2jupyter import scipy_heatKernel
```
fixed it for me.

"
ucsd-ccbb/visJS2jupyter,"TypeError on import: https://github.com/ucsd-ccbb/visJS2jupyter/issues/17
Description: I am using python 3.6.4 and Jupyter 5.5.0... I cannot even import the package, which throws
`TypeError: Javascript expects text, not b'/**\r\n * vis.js\r\n * https://github.com/almende/vis\r\n *`[....] (there is a *lot* of text in the [....])

"
ucsd-ccbb/visJS2jupyter,"Error importing visJS2jupyter.visJS_module: https://github.com/ucsd-ccbb/visJS2jupyter/issues/16
Description: Hi! I've gone through and installed everything according to the install instructions. But when importing visJS2jupyter.visJS_module, I get the following error:

```
TypeError                                 Traceback (most recent call last)
<ipython-input-31-342177aa17ad> in <module>()
      1 import matplotlib as mpl
----> 2 import visJS2jupyter.visJS_module

~/anaconda/lib/python3.6/site-packages/visJS2jupyter/visJS_module.py in <module>()
     21 import networkx as nx
     22 
---> 23 Javascript(""https://cdnjs.cloudflare.com/ajax/libs/vis/4.21.0/vis.js"")
     24 
     25 def visjs_network(nodes_dict, edges_dict,

~/anaconda/lib/python3.6/site-packages/IPython/core/display.py in __init__(self, data, url, filename, lib, css)
    973         self.lib = lib
    974         self.css = css
--> 975         super(Javascript, self).__init__(data=data, url=url, filename=filename)
    976 
    977     def _repr_javascript_(self):

~/anaconda/lib/python3.6/site-packages/IPython/core/display.py in __init__(self, data, url, filename, metadata)
    608 
    609         self.reload()
--> 610         self._check_data()
    611 
    612     def __repr__(self):

~/anaconda/lib/python3.6/site-packages/IPython/core/display.py in _check_data(self)
    657     def _check_data(self):
    658         if self.data is not None and not isinstance(self.data, str):
--> 659             raise TypeError(""%s expects text, not %r"" % (self.__class__.__name__, self.data))
    660 
    661 class Pretty(TextDisplayObject):

TypeError: Javascript expects text, not b'/**\r\n * vis.js\r\n * https://github.com/almende/vis\r\n *\r\n * A dynamic, browser-based visualization library.\r\n *\r\n * @version 4.20.1-SNAPSHOT\r\n 
```

I've cut off the error, as it is just a print out of the entire page from https://cdnjs.cloudflare.com/ajax/libs/vis/4.21.0/vis.js. So it appears that I am unable to load the vis.js file -- any ideas what I may be missing or doing wrong? 

Thank you!
Sylvana

"
ucsd-ccbb/visJS2jupyter,"Mulitple plot in the same notebook are not rendering: https://github.com/ucsd-ccbb/visJS2jupyter/issues/13
Description: Hello,
I want to thank you for this beautiful library, very helpful! 
I was playing with it and I tried to plot, in the same jupyter notebook, multiple Graph example. When I try to plot the second one is not rendering anything but an empty figure.

To replicate the problem is sufficient to execute the same simple plot in two different cells!

Does this sound familiar to anyone?
Hope to be clear,

Thank you

"
ucsd-ccbb/visJS2jupyter,"Edge Titles / Labels Not Visible: https://github.com/ucsd-ccbb/visJS2jupyter/issues/10
Description: Hello,

In visJS2jupyter 0.1.8 when hovering over an edge I can not see the title or label. The line does act as it is being highlighted, changing thickness and darkening. I have tried setting `title`, `edge_title_field`, and `edge_label_field` as seen below to no effect.

Is this a known issue? What should I do to display the nodes an edge connects while hovering over it?

Thank you for creating such a useful tool to visualize networks interactively!
```
edges_dict = [{""source"":node_map[edges_list[i][0]],
               ""target"":node_map[edges_list[i][1]],
               ""title"":""A"",
               ""edge_title_field"":""B"",
               ""edge_label_field"":""C"",
               ""color"":edge_to_color[edges_list[i]] 
              }for i in range(len(edges_list))]

visJS_module.visjs_network(nodes_dict, edges_dict, edge_width=9, tooltip_delay = 0)
```

"
ucsd-ccbb/visJS2jupyter,"more output options: https://github.com/ucsd-ccbb/visJS2jupyter/pull/9
Description: Thank you for the useful library (: It's a shame to restrict it to Jupyter though, so I added options to make it easy for users to use the library for Zeppelin and HTML output as well, and to expand graph size to the full available window if desired.

"
ucsd-ccbb/visJS2jupyter,"Using version 0.18 and having issues.: https://github.com/ucsd-ccbb/visJS2jupyter/issues/7
Description: Hello

visJS2jupyter version 0.18
python 2.7.14 
networkx 2.0

when running examples notebooks cell like:

# define the initial positions of the nodes using networkx's spring_layout function, and add to the nodes_dict.
pos = nx.spring_layout(G)
nodes_dict = [{""id"":n,
              ""x"":pos[n][0]*1000,
              ""y"":pos[n][1]*1000} for n in nodes]
node_map = dict(zip(nodes,range(len(nodes))))  # map to indices for source/target in edges

edges_dict = [{""source"":node_map[edges[i][0]], ""target"":node_map[edges[i][1]], 
              ""title"":'test'} for i in range(len(edges))]

visJS2jupyter.visJS_module.visjs_network(nodes_dict,edges_dict)

I get this error: TypeError: 'int' object is not iterable


or when running this cell:

# add a node attributes to color-code by
cc = nx.clustering(G)
degree = G.degree()
bc = nx.betweenness_centrality(G)
nx.set_node_attributes(G,'clustering_coefficient',cc)
nx.set_node_attributes(G,'degree',degree)
nx.set_node_attributes(G,'betweenness_centrality',bc)

I get this error: TypeError: unhashable type: 'dict'

let me know if you need more information.

Thanks in advance for your help.

JM




"
ucsd-ccbb/visJS2jupyter,"Enabling saving HTML in  standalone file: https://github.com/ucsd-ccbb/visJS2jupyter/pull/6
Description: Hi Everyone,

Thanks for the great tool. It's really nice and extremely helpful tool for visualization. I really liked it. 
I wanted to enable the saving the output to standalone HTML rather than keeping it in Jupyter Notebook. I have made the changes necessary for that. I wish if you could accept my changes.

I have removed as well the pycharm project files "".idea"" and added it to .gitignore. 

Thanks for your support on this.

Regards,
Amro

"
ucsd-ccbb/visJS2jupyter,"co-localization breaks when some nodes are strings: https://github.com/ucsd-ccbb/visJS2jupyter/issues/3
Description: G = nx.planted_partition_graph(3,20,.5,.01)
G = nx.relabel_nodes(G,dict(zip(G.nodes(),[chr(n+97) if n < 25 else n for n in G.nodes()])))
visJS2jupyter.visualizations.draw_colocalization(G,G.nodes()[0:3],G.nodes()[35:37],export_network=False,k=1)

the error i get is:

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-3-134e2da5ccdb> in <module>()
----> 1 visJS2jupyter.visualizations.draw_colocalization(G,G.nodes()[0:3],G.nodes()[35:37],export_network=False,k=1,graph_id=3)
      2 #visJS2jupyter.visualizations.draw_heat_prop(G,G.nodes()[0:3],k=1,graph_id=3)

/Users/brin/anaconda/lib/python2.7/site-packages/visJS2jupyter/visualizations.pyc in draw_colocalization(G, seed_nodes_1, seed_nodes_2, edge_cmap, export_file, export_network, highlight_nodes, k, largest_connected_component, node_cmap, node_size, num_nodes, physics_enabled, Wprime, **kwargs)
    566                 node_labels[node] = ''
    567     else:
--> 568         node_labels = {n:np.int64(n).item() for n in nodes}
    569 
    570     nx.set_node_attributes(G,'nodeLabel',node_labels)

/Users/brin/anaconda/lib/python2.7/site-packages/visJS2jupyter/visualizations.pyc in <dictcomp>((n,))
    566                 node_labels[node] = ''
    567     else:
--> 568         node_labels = {n:np.int64(n).item() for n in nodes}
    569 
    570     nx.set_node_attributes(G,'nodeLabel',node_labels)

ValueError: invalid literal for long() with base 10: 'b'

"
ucsd-ccbb/visJS2jupyter,"Issues reproducing demo notebooks: https://github.com/ucsd-ccbb/visJS2jupyter/issues/2
Description: I'm a bit confused: I've installed the latest version of `visJS2jupyter` and tried to reproduce some of the notebooks linked to the README, but I basically failed with all of them. 

(Please note that I use python 3)

As an example, I tried to reproduce the [heat propagation notebook](https://bl.ocks.org/julialen/raw/82c316048ade650effbff3fd9eaddccd/):

```
#first cell in the notebook:
import matplotlib.pyplot as plt
import matplotlib as mpl
import networkx as nx
import visJS2jupyter.visualizations as visualizations
```

And I get:
```
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-2-2fc2f7659132> in <module>()
      2 import matplotlib as mpl
      3 import networkx as nx
----> 4 import visJS2jupyter.visualizations as visualizations

/Users/e/anaconda3/lib/python3.5/site-packages/visJS2jupyter/visualizations.py in <module>()
     17 import pandas as pd
     18 from py2cytoscape import util
---> 19 import visJS_module as visJS_module
     20 
     21 def draw_graph_overlap(G1, G2,

ImportError: No module named 'visJS_module'
```

I could report also the others, but I get similar sort of errors in all of them.  What do I do wrong?



"
datapane/datapane,"[Bug]: Dynamic Block does not call the on_timer function: https://github.com/datapane/datapane/issues/362
Description: ### Is there an existing issue for this?

- [X] I have searched for similar issues and discussions

### Bug Description

```markdown
1. I downloaded datapane and all the relevant dependencies
2. I copy and pasted the code from this example: https://github.com/datapane/examples/blob/main/apps/stock-dashboard/app.py
3. I ran the app
4. The screen was blank
5. Nothing happened even after data was refreshed

I fought with this issue for about an hour. It comes down to the on_timer function in the Dynamic block not being called, or at least I suppose that's the problem.
```


### System Information

```markdown
- Datapane version:0.16.2
- Python version: 3.10.10
- Operating System:Linux Manjaro
- Using Jupyter: No
- Pip or Conda: Pip
- Dependencies:
```


### Anything else?

Example tested: https://github.com/datapane/examples/blob/main/apps/stock-dashboard/app.py

"
datapane/datapane,"ValueError: too many values to unpack (expected 2) when showing a DataTable in Jupyter: https://github.com/datapane/datapane/issues/353
Description: I'm getting the error shown below when running the following cell in Jupyter:
```
dp.DataTable(pandas_df)
```

<details><summary>Trace</summary>

<p>

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
File /opt/conda/lib/python3.9/site-packages/IPython/core/formatters.py:920, in IPythonDisplayFormatter.__call__(self, obj)
    918 method = get_real_method(obj, self.print_method)
    919 if method is not None:
--> 920     method()
    921     return True

File /opt/conda/lib/python3.9/site-packages/datapane/blocks/base.py:94, in BaseBlock._ipython_display_(self)
     91 from datapane.view import Blocks
     93 if get_environment().support_rich_display:
---> 94     html_str = stringify_report(Blocks(self))
     95     display(HTML(html_str))
     96 else:

File /opt/conda/lib/python3.9/site-packages/datapane/processors/api.py:126, in stringify_report(blocks, name, formatting)
    116 """"""Stringify the app document to a HTML string
    117 
    118 Args:
   (...)
    121     formatting: Sets the basic app styling
    122 """"""
    124 s = ViewState(blocks=Blocks.wrap_blocks(blocks), file_entry_klass=B64FileEntry)
    125 report_html: str = (
--> 126     Pipeline(s)
    127     .pipe(PreProcessView())
    128     .pipe(ConvertXML())
    129     .pipe(ExportHTMLStringInlineAssets(name=name, formatting=formatting))
    130     .result
    131 )
    133 return report_html

File /opt/conda/lib/python3.9/site-packages/datapane/processors/types.py:59, in Pipeline.pipe(self, p)
     57 def pipe(self, p: BaseProcessor[P_IN, P_OUT]) -> Pipeline[P_OUT]:
     58     p.s = self._state
---> 59     y = p.__call__(self._x)  # need to call as positional args
     60     self._state = p.s
     61     return Pipeline(self._state, y)

File /opt/conda/lib/python3.9/site-packages/datapane/processors/processors.py:90, in ConvertXML.__call__(self, _)
     89 def __call__(self, _: t.Any) -> ElementT:
---> 90     initial_doc = self.convert_xml()
     91     transformed_doc = self.post_transforms(initial_doc)
     93     # convert to string

File /opt/conda/lib/python3.9/site-packages/datapane/processors/processors.py:107, in ConvertXML.convert_xml(self)
    104 def convert_xml(self) -> ElementT:
    105     # create initial state
    106     builder_state = XMLBuilder(store=self.s.store)
--> 107     self.s.blocks.accept(builder_state)
    108     return builder_state.get_root(self.fragment)

File /opt/conda/lib/python3.9/site-packages/datapane/blocks/base.py:100, in BaseBlock.accept(self, visitor)
     99 def accept(self, visitor: VV) -> VV:
--> 100     visitor.visit(self)
    101     return visitor

File /opt/conda/lib/python3.9/site-packages/multimethod/__init__.py:315, in multimethod.__call__(self, *args, **kwargs)
    313 func = self[tuple(func(arg) for func, arg in zip(self.type_checkers, args))]
    314 try:
--> 315     return func(*args, **kwargs)
    316 except TypeError as ex:
    317     raise DispatchError(f""Function {func.__code__}"") from ex

File /opt/conda/lib/python3.9/site-packages/datapane/view/xml_visitor.py:88, in XMLBuilder.visit(self, b)
     86 @multimethod
     87 def visit(self, b: Blocks) -> XMLBuilder:
---> 88     sub_elements = self._visit_subnodes(b)
     90     # Blocks are converted to Group internally
     91     element = E.Group(*sub_elements, columns=""1"", valign=""top"")

File /opt/conda/lib/python3.9/site-packages/datapane/view/xml_visitor.py:73, in XMLBuilder._visit_subnodes(self, b)
     71 cur_elements = self.elements
     72 self.elements = []
---> 73 b.traverse(self)  # visit subnodes
     74 res = self.elements
     75 self.elements = cur_elements

File /opt/conda/lib/python3.9/site-packages/datapane/blocks/layout.py:70, in ContainerBlock.traverse(self, visitor)
     68 def traverse(self, visitor: VV) -> VV:
     69     # perform a depth-first traversal of the contained blocks
---> 70     return reduce(lambda _visitor, block: block.accept(_visitor), self.blocks, visitor)

File /opt/conda/lib/python3.9/site-packages/datapane/blocks/layout.py:70, in ContainerBlock.traverse.<locals>.<lambda>(_visitor, block)
     68 def traverse(self, visitor: VV) -> VV:
     69     # perform a depth-first traversal of the contained blocks
---> 70     return reduce(lambda _visitor, block: block.accept(_visitor), self.blocks, visitor)

File /opt/conda/lib/python3.9/site-packages/datapane/blocks/base.py:100, in BaseBlock.accept(self, visitor)
     99 def accept(self, visitor: VV) -> VV:
--> 100     visitor.visit(self)
    101     return visitor

File /opt/conda/lib/python3.9/site-packages/multimethod/__init__.py:315, in multimethod.__call__(self, *args, **kwargs)
    313 func = self[tuple(func(arg) for func, arg in zip(self.type_checkers, args))]
    314 try:
--> 315     return func(*args, **kwargs)
    316 except TypeError as ex:
    317     raise DispatchError(f""Function {func.__code__}"") from ex

File /opt/conda/lib/python3.9/site-packages/datapane/view/xml_visitor.py:127, in XMLBuilder.visit(self, b)
    124 @multimethod
    125 def visit(self, b: AssetBlock):
    126     """"""Main XMl creation method - visitor method""""""
--> 127     fe = self._add_asset_to_store(b)
    129     _E = getattr(E, b._tag)
    131     e: etree._Element = _E(
    132         type=fe.mime,
    133         # size=conv_attrib(fe.size),
   (...)
    137         src=f""ref://{fe.hash}"",
    138     )

File /opt/conda/lib/python3.9/site-packages/datapane/view/xml_visitor.py:164, in XMLBuilder._add_asset_to_store(self, b)
    162     meta: AssetMeta = writer.get_meta(b.data)
    163     fe = self.store.get_file(meta.ext, meta.mime)
--> 164     writer.write_file(b.data, fe.file)
    165     self.store.add_file(fe)
    166 except DispatchError:

File /opt/conda/lib/python3.9/site-packages/multimethod/__init__.py:315, in multimethod.__call__(self, *args, **kwargs)
    313 func = self[tuple(func(arg) for func, arg in zip(self.type_checkers, args))]
    314 try:
--> 315     return func(*args, **kwargs)
    316 except TypeError as ex:
    317     raise DispatchError(f""Function {func.__code__}"") from ex

File /opt/conda/lib/python3.9/site-packages/datapane/view/asset_writers.py:87, in DataTableWriter.write_file(self, x, f)
     85     raise DPClientError(""Empty DataFrame provided"")
     86 # process_df called in Arrow.save_file
---> 87 ArrowFormat.save_file(f, df)

File /opt/conda/lib/python3.9/site-packages/datapane/common/datafiles.py:63, in ArrowFormat.save_file(fn, df)
     61 df = process_df(df)
     62 # NOTE - can pass expected schema and columns for output df here
---> 63 table: pa.Table = pa.Table.from_pandas(df, preserve_index=False)
     64 write_table(table, fn)

File /opt/conda/lib/python3.9/site-packages/pyarrow/table.pxi:1553, in pyarrow.lib.Table.from_pandas()

ValueError: too many values to unpack (expected 2)
```

</p>

</details>

```
!pip install datapane
```
<details><summary>pip output</summary>
<p>

```
Requirement already satisfied: pyngrok<6.0.0,>=5.2.1 in /opt/conda/lib/python3.9/site-packages (from datapane) (5.2.1)
Requirement already satisfied: boltons<22.0.0,>=20.0.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (21.0.0)
Requirement already satisfied: cheroot<10.0.0,>=9.0.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (9.0.0)
Requirement already satisfied: pyarrow<11.0.0,>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (10.0.1)
Requirement already satisfied: altair<5.0.0,>=4.0.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (4.2.1)
Requirement already satisfied: dulwich<0.22.0,>=0.20.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (0.21.3)
Requirement already satisfied: furl<3.0.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (2.1.3)
Requirement already satisfied: datacommons<2.0.0,>=1.4.3 in /opt/conda/lib/python3.9/site-packages (from datapane) (1.4.3)
Requirement already satisfied: requests-toolbelt<0.11.0,>=0.9.1 in /opt/conda/lib/python3.9/site-packages (from datapane) (0.10.1)
Requirement already satisfied: dacite<2.0.0,>=1.0.2 in /opt/conda/lib/python3.9/site-packages (from datapane) (1.8.0)
Requirement already satisfied: vega-datasets<1.0.0,>=0.9.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (0.9.0)
Requirement already satisfied: click<9.0.0,>=7.1.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (8.1.3)
Requirement already satisfied: click-spinner<0.2.0,>=0.1.8 in /opt/conda/lib/python3.9/site-packages (from datapane) (0.1.10)
Requirement already satisfied: dominate<3.0.0,>=2.4.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (2.7.0)
Requirement already satisfied: toolz<0.13.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (0.12.0)
Requirement already satisfied: pandas<2.0.0,>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (1.5.3)
Requirement already satisfied: lxml<5.0.0,>=4.0.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (4.9.2)
Requirement already satisfied: micawber>=0.5.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (0.5.4)
Requirement already satisfied: colorlog<7.0.0,>=4.1.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (6.7.0)
Requirement already satisfied: datacommons-pandas<0.0.4,>=0.0.3 in /opt/conda/lib/python3.9/site-packages (from datapane) (0.0.3)
Requirement already satisfied: glom<24.0.0,>=22.1.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (23.1.1)
Requirement already satisfied: requests<3.0.0,>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (2.28.2)
Requirement already satisfied: jsonschema<5.0.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (4.17.3)
Requirement already satisfied: multimethod<2.0,>=1.9 in /opt/conda/lib/python3.9/site-packages (from datapane) (1.9.1)
Requirement already satisfied: nbconvert>=5.6.1 in /opt/conda/lib/python3.9/site-packages (from datapane) (7.2.9)
Requirement already satisfied: chardet<6.0.0,>=4.0.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (5.1.0)
Requirement already satisfied: tabulate<0.10.0,>=0.8.9 in /opt/conda/lib/python3.9/site-packages (from datapane) (0.9.0)
Requirement already satisfied: importlib_resources<6.0.0,>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (5.12.0)
Requirement already satisfied: ipynbname<2022.0.0,>=2021.3.2 in /opt/conda/lib/python3.9/site-packages (from datapane) (2021.3.2)
Requirement already satisfied: posthog<3.0.0,>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (2.3.1)
Requirement already satisfied: PyYAML<7.0.0,>=5.4.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (6.0)
Requirement already satisfied: packaging<24.0.0,>=21.0.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (23.0)
Requirement already satisfied: pydantic<2.0.0,>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (1.10.5)
Requirement already satisfied: typing-extensions<5.0.0,>=4.4.0 in /opt/conda/lib/python3.9/site-packages (from datapane) (4.4.0)
Requirement already satisfied: entrypoints in /opt/conda/lib/python3.9/site-packages (from altair<5.0.0,>=4.0.0->datapane) (0.4)
Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from altair<5.0.0,>=4.0.0->datapane) (3.1.2)
Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from altair<5.0.0,>=4.0.0->datapane) (1.23.5)
Requirement already satisfied: six>=1.11.0 in /opt/conda/lib/python3.9/site-packages (from cheroot<10.0.0,>=9.0.0->datapane) (1.16.0)
Requirement already satisfied: more-itertools>=2.6 in /opt/conda/lib/python3.9/site-packages (from cheroot<10.0.0,>=9.0.0->datapane) (9.1.0)
Requirement already satisfied: jaraco.functools in /opt/conda/lib/python3.9/site-packages (from cheroot<10.0.0,>=9.0.0->datapane) (3.6.0)
Requirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.9/site-packages (from dulwich<0.22.0,>=0.20.0->datapane) (1.25.11)
Requirement already satisfied: orderedmultidict>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from furl<3.0.0,>=2.0.0->datapane) (1.0.1)
Requirement already satisfied: face==20.1.1 in /opt/conda/lib/python3.9/site-packages (from glom<24.0.0,>=22.1.0->datapane) (20.1.1)
Requirement already satisfied: attrs in /opt/conda/lib/python3.9/site-packages (from glom<24.0.0,>=22.1.0->datapane) (22.2.0)
Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from importlib_resources<6.0.0,>=3.0.0->datapane) (3.15.0)
Requirement already satisfied: ipykernel in /opt/conda/lib/python3.9/site-packages (from ipynbname<2022.0.0,>=2021.3.2->datapane) (6.21.2)
Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema<5.0.0,>=3.2.0->datapane) (0.19.3)
Requirement already satisfied: pygments>=2.4.1 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5.6.1->datapane) (2.14.0)
Requirement already satisfied: traitlets>=5.0 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5.6.1->datapane) (5.9.0)
Requirement already satisfied: markupsafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5.6.1->datapane) (2.1.2)
Requirement already satisfied: importlib-metadata>=3.6 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5.6.1->datapane) (6.0.0)
Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5.6.1->datapane) (1.5.0)
Requirement already satisfied: nbformat>=5.1 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5.6.1->datapane) (5.7.3)
Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5.6.1->datapane) (0.7.2)
Requirement already satisfied: bleach in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5.6.1->datapane) (6.0.0)
Requirement already satisfied: tinycss2 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5.6.1->datapane) (1.2.1)
Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5.6.1->datapane) (4.11.2)
Requirement already satisfied: mistune<3,>=2.0.3 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5.6.1->datapane) (2.0.5)
Requirement already satisfied: defusedxml in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5.6.1->datapane) (0.7.1)
Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5.6.1->datapane) (0.2.2)
Requirement already satisfied: jupyter-core>=4.7 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5.6.1->datapane) (5.2.0)
Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas<2.0.0,>=1.1.0->datapane) (2022.7.1)
Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas<2.0.0,>=1.1.0->datapane) (2.8.2)
Requirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.9/site-packages (from posthog<3.0.0,>=1.4.0->datapane) (2.2.1)
Requirement already satisfied: monotonic>=1.5 in /opt/conda/lib/python3.9/site-packages (from posthog<3.0.0,>=1.4.0->datapane) (1.6)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.19.0->datapane) (2.1.1)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.19.0->datapane) (2022.12.7)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.19.0->datapane) (2.8)
Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.9/site-packages (from jupyter-core>=4.7->nbconvert>=5.6.1->datapane) (3.0.0)
Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.9/site-packages (from nbclient>=0.5.0->nbconvert>=5.6.1->datapane) (8.0.3)
Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.9/site-packages (from nbformat>=5.1->nbconvert>=5.6.1->datapane) (2.16.3)
Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.9/site-packages (from beautifulsoup4->nbconvert>=5.6.1->datapane) (2.4)
Requirement already satisfied: webencodings in /opt/conda/lib/python3.9/site-packages (from bleach->nbconvert>=5.6.1->datapane) (0.5.1)
Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.9/site-packages (from ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (0.1.6)
Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (5.9.4)
Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.9/site-packages (from ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (1.6.6)
Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.9/site-packages (from ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (0.1.2)
Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.9/site-packages (from ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (6.2)
Requirement already satisfied: pyzmq>=20 in /opt/conda/lib/python3.9/site-packages (from ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (25.0.0)
Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.9/site-packages (from ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (1.5.6)
Requirement already satisfied: ipython>=7.23.1 in /opt/conda/lib/python3.9/site-packages (from ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (8.11.0)
Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (3.0.38)
Requirement already satisfied: pickleshare in /opt/conda/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (0.7.5)
Requirement already satisfied: stack-data in /opt/conda/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (0.6.2)
Requirement already satisfied: backcall in /opt/conda/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (0.2.0)
Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (4.8.0)
Requirement already satisfied: decorator in /opt/conda/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (5.1.1)
Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (0.18.2)
Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (0.8.3)
Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.9/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (0.7.0)
Requirement already satisfied: wcwidth in /opt/conda/lib/python3.9/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (0.2.6)
Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (2.2.1)
Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (1.2.0)
Requirement already satisfied: pure-eval in /opt/conda/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->ipynbname<2022.0.0,>=2021.3.2->datapane) (0.2.2)
Installing collected packages: datapane
Successfully installed datapane-0.16.1
```

</p>
</details>

"
datapane/datapane,"dp.cells_to_blocks() misses all figures: https://github.com/datapane/datapane/issues/346
Description: I'm super new to datapane and was trying to convert a notebook to HTML. I noticed that `dp.cells_to_blocks()` catches all text and dataframe cells but doesn't capture any outputs that include a matplotlib figure. Is this a bug?

"
datapane/datapane,"Notebook Parity Check: https://github.com/datapane/datapane/pull/337
Description: Thank you so much for contributing to Datapane, please help us by filing out this PR template and ensure you have read the [CONTRIBUTING.md](../CONTRIBUTING.md).

It may take a few days to review your PR and merge it - please bear with us!

Thanks!

## Prerequsites

- [x] Has been tested locally
- [ ] If a bugfix, have included a breaking test if possible

## Proposed Changes

We need to ensure parity between the state of the notebook and output cache. We can use this check to better guide the user towards successful conversion.

- First check makes sure both notebooks have the same cell execution count.
- Second check makes sure of those executed cells, are the inputs identical?
- Cells containing bangs and magics are marked and ignored in the parity check and cell conversion
   - Bangs and magics are transformed into Python code.
   - We will update this behaviour If we start using magics in the future.
- Exceptions are still thrown, but we are using the `_render_stacktrace` hook in IPython for better presentation.


Tested in Colab, VS Code + Jupyter Extension, Jupyter Lab, and Jupyter Notebook.


"
datapane/datapane,"Simplify Notebook to App API: https://github.com/datapane/datapane/pull/335
Description: Thank you so much for contributing to Datapane, please help us by filing out this PR template and ensure you have read the [CONTRIBUTING.md](../CONTRIBUTING.md).

It may take a few days to review your PR and merge it - please bear with us!

Thanks!

## Prerequsites

- [x] Has been tested locally
- [ ] If a bugfix, have included a breaking test if possible

## Proposed Changes

Simplifying the Notebook to App API by:

- Removing the need for users to provide the output cache (`Out`) variable.
- Adding a new function to our ipython_utils that transforms a notebook to an App directly.
- Adding `from_notebook()` to the `dp.App` class.
- Update Jupyter Integration page in docs to use the new API.
- Re-order Jupyter Integration page to demonstrate `.upload()` first.
- Added better handling for excluding the cell that invokes the conversion.

## Testing
﻿
Tested with:

- Jupyter Lab
- Jupyter Notebook
- Google Colab
- VS Code + Jupyter extension

Using `notebook_to_app_test.ipynb` (attached in Basecamp) with variation in the upload name, i.e.:

```python
dp.App.from_notebook().upload(""notebook to app from {IDE name}"", publicly_visible = True)
```

Note: to test in Colab you'll need to build, upload, and install the Datapane wheel.

Uploads can be found here:

- [Jupyter Lab](https://cloud.datapane.com/apps/OkpRzy3/notebook-to-app-from-jupyter-lab/)
- [Jupyter Notebook](https://cloud.datapane.com/apps/XknbMzk/notebook-to-app-from-jupyter-notebook/)
- [Google Colab](https://cloud.datapane.com/apps/qkWe2X3/notebook-to-app-from-google-colab/)
- [VS Code + Jupyter extension](https://cloud.datapane.com/apps/yklXLvk/notebook-to-app-from-vs-code/)

They appear to be correct and identical where they should be.

"
datapane/datapane,"App.save is saving singlehtml file in local but still trying to wrongly upload to events.datapane.com server : https://github.com/datapane/datapane/issues/334
Description: <!--
**NOTE** Please use this template to open issues, bugs, etc., only.
See our [GitHub Discussions Board](https://github.com/datapane/datapane/discussions) to discuss feature requests, general support, ideas, and to chat with the community.
-->

### System Information

<!-- Please fill this out to help us understand the bug/issue -->

- OS: Windows 11
- Python version: 3.10
- Python environment: pip 
- Using jupyter:  false, doing coding in Pycharm community IDE
- Datapane version: 0.15.4

### Bug / Issue

<!--
App.save is saving single html file in local but still trying to wrongly upload to events.datapane.com server.
I tried various parameter combination like cdn_base=None, cdn_base=localhost but same issue.
Even at last I tried to ommit the cdn_base parameter as well as I know there is a default.
Below is the relevant line of codes and I have attached screenshot as well.

=======================below is code ===============================
import altair as alt
import datapane as dp
from vega_datasets import data
source= data.cars()
plot1 = (
    alt.Chart(source)
    .mark_circle(size=60)
    .encode(
        x=""Horsepower"",
        y=""Miles_per_Gallon"",
        color=""Origin"",
        tooltip=[""Name"", ""Origin"", ""Horsepower"", ""Miles_per_Gallon""],
    )
    .interactive()
)
app = dp.App(""MyFirstapp"", dp.Plot(plot1,caption =""myplot""), 
             dp.DataTable(source,caption=""mydata""))
mypath=""D:/Python-Projects/datapanetest/mytest.html""
app.save(path=mypath,open=False,standalone=True,cdn_base=None)
=================================================================
![datapane calling upload during save](https://user-images.githubusercontent.com/3212064/201524124-f7824af2-a1ab-404e-9da1-5919b64f36c4.jpg)

-->


"
datapane/datapane,"Embedding report in medium not working: https://github.com/datapane/datapane/issues/332
Description: ### System Information

<!-- Please fill this out to help us understand the bug/issue -->

- OS: Windows
- Python version: 3.7.1
- Python environment: pip
- Using jupyter: true
- Datapane version: 0.15.4

### Bug / Issue

Hello, I'm trying to embed a visualization into a medium article. I've successfully created the report: https://cloud.datapane.com/apps/yklX4vk/my-plot/

When I click on share, I'm see this:

![image](https://user-images.githubusercontent.com/23065025/201077129-a20c830f-2433-4e16-a9c1-4de32f4c3dfd.png)

If I use that link in medium, I get this:

![image](https://user-images.githubusercontent.com/23065025/201077288-cadc7d32-4ed0-4fea-ae90-c33e29ddeb02.png)

I want to share it this way:

![image](https://user-images.githubusercontent.com/23065025/201077391-8e3e419b-4bce-41ce-a126-064085c4ab4d.png)

Do you know what I'm missing?

Thanks

"
voxel51/fiftyone,"[KAGGLE] image-matching-challenge-2023: https://github.com/voxel51/fiftyone/issues/2906
Description: ## Kaggle challenge URL

https://www.kaggle.com/competitions/image-matching-challenge-2023

## Setup

```
pip install fiftyone
```

## Load dataset

```py
import glob
import fiftyone as fo

# Download and unzip `image-matching-challenge-2023.zip` and put path here
dataset_dir = ""/path/to/image-matching-challenge-2023""

samples = []
for filepath in glob.glob(os.path.join(dataset_dir, ""**""), recursive=True):
    if filepath.endswith(("".jpg"", "".jpeg"", "".png"", "".JPG"")):
        folders = filepath[len(dataset_dir) + 1:].split(""/"")[:-2]
        sample = fo.Sample(
            filepath=filepath,
            tags=[folders[0]],
            type=folders[1],
            location=folders[2],
        )
        samples.append(sample)

dataset = fo.Dataset(""image-matching-challenge-2023"", persistent=True)
dataset.add_samples(samples)
dataset.compute_metadata()
```

## Optional: add embeddings

```py
import fiftyone.brain as fob

fob.compute_visualization(
    dataset,
    model=""clip-vit-base32-torch"",
    brain_key=""img_viz"",
)
```

## Visualize in the App

```py
session = fo.launch_app(dataset)
```

## Videos

https://user-images.githubusercontent.com/25985824/232956011-2de4f741-f063-450f-9d45-f278cf4efe8b.mp4

https://user-images.githubusercontent.com/25985824/232956003-d2f67a1a-a994-43ba-8c8a-f92e8da0a5d3.mp4

https://user-images.githubusercontent.com/25985824/232956007-65e8de97-c7d6-47ba-84bc-0ae89224209c.mp4



"
voxel51/fiftyone,"[BUG] Split vertically button is not displayed correctly in Jupyter notebook within VS Code: https://github.com/voxel51/fiftyone/issues/2893
Description: ### Instructions

Discussion in community slack channel: https://fiftyone-users.slack.com/archives/C018FNQRU8Z/p1681197462595279

### System information

-   **OS Platform and Distribution** (e.g., Linux Ubuntu 16.04): Mac OS Ventura 13.3.1
-   **Python version** (`python --version`): Python 3.10.6
-   **FiftyOne version** (`fiftyone --version`): FiftyOne v0.20.0, Voxel51, Inc.
-   **FiftyOne installed from** (pip or source): source

### Commands to reproduce

As thoroughly as possible, please provide the Python and/or shell commands used to encounter the issue. Application steps can be described in the next section.

```
1. Install Jupyter extension in VS Code
2. Create a new notebook file (fo.ipynb) to open with the extension
3. Add each command below as cell in notebook and execute
  - import fiftyone as fo  
  - import fiftyone.zoo as foz
  - q = foz.load_zoo_dataset(""quickstart"")
  - s = fo.launch_app(q) 
4. Add a second panel with the ""Add panel"" button to enable splitting of panels
5. The ""Split vertically"" button will be same as the ""Split horizontally"" button (see screenshot below)
```

### Describe the problem

Refer to **Commands to reproduce**

### Code to reproduce issue

Refer to **Commands to reproduce**

### Other info/logs

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Please do not use screenshots for sharing text. Code snippets should be used instead when providing tracebacks, logs, etc.

![image](https://user-images.githubusercontent.com/25350704/231488023-e2a893d8-96e1-4f3f-b8e4-94ad02c0d034.png)

### What areas of FiftyOne does this bug affect?

-   [x] `App`: FiftyOne application issue
-   [ ] `Core`: Core Python library issue
-   [ ] `Server`: FiftyOne server issue

### Willingness to contribute

The FiftyOne Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the FiftyOne codebase?

-   [ ] Yes. I can contribute a fix for this bug independently
-   [ ] Yes. I would be willing to contribute a fix for this bug with guidance from the FiftyOne community
-   [ ] No. I cannot contribute a bug fix at this time

"
interpretml/interpret-community,"fix nightly pytorch error on macos by installing numpy from conda: https://github.com/interpretml/interpret-community/pull/535
Description: rely on installing numpy from conda - otherwise we see segfaults on macos when installing pytorch and lightgbm together:

```
/Users/runner/work/_temp/166c9a15-3b69-4e3f-a25e-1b51a6c0154c.sh: line 2: 13956 Segmentation fault: 11  python -m pytest tests/ -m ""not notebooks"" --junitxml=./TEST--TEST.xml -o junit_suite_name=""MacOS Unit 3.7-Unit""
```

Update - see passing nightly build here:
https://dev.azure.com/responsibleai/interpret-community/_build/results?buildId=14736&view=results

"
cytoscape/py2cytoscape,"py2cytoscape connection from JupyterHub: https://github.com/cytoscape/py2cytoscape/issues/106
Description: Hi all, 

Thanks for this amazing extension. I am wondering if you run py2cytoscape connection from JupyterHub?

Best, HM 
"
nl4dv/nl4dv,"{""aggregate"": None/null} key in the output vega-lite spec cause Validation error when using the altair vega-lite renderer in Colab: https://github.com/nl4dv/nl4dv/issues/6
Description: **More info**: The above issue does not seem to occur in the online Vega-Lite Editor or with the ipyvega extension in a classic Jupyter Notebook.
"
Quantomatic/pyzx,"Diagram editor doesn't seem to work on modern browsers: https://github.com/Quantomatic/pyzx/issues/100
Description: `zx.editor.edit()` fails to run, even in legacy/Jupyter Notebook mode. Checking my browser console, the issue appears to be that for security reasons, browsers have started enforcing MIME type checking, and for whatever reason Jupyter is claiming that `d3.min.js` is a `text/html` file (and not a js script).

"
Quantomatic/pyzx,"Additional convenience features for the d3 drawing of diagrams: https://github.com/Quantomatic/pyzx/issues/88
Description: I suggest two new features for the d3 drawing of diagrams (using zx.draw()):

* When two spiders significantly overlap, there should be some kind of visual indication that allows you to easily see this. For instance, a number in a bright color indicating the number of spiders overlapping, and/or some kind of warning symbol.
* The ability to zoom in and out on a diagram, either by ctrl-scrolling, or by having +/- buttons displayed on the d3 window, whichever turns out to work best.
* If this is possible in Jupyter, the ability to make the display window smaller or bigger in order to show a larger or smaller part of the diagram.

"
Quantomatic/pyzx,"getting started example in demo notebook vs doc webpage: https://github.com/Quantomatic/pyzx/issues/86
Description: Hello, I am a newcomer to ZX calculus and PyZX so please pardon my ignorance.

I am running the demo notebook gettingstarted.ipynb using the PyZX 0.7 release. The demo starts by generating a random Clifford circuit but with the seed set to 1337. I notice that this is the same circuit as the web page https://pyzx.readthedocs.io/en/latest/gettingstarted.html.

However when I run the notebook, the simplified ZX diagram after clifford_simp() and normalize() is slightly different (both diagrams have 8 Z-spiders, but the phases and edges don't quite match) and after extract_circuit() the circuits are vastly different. The result on the web page is much simpler, it has just 9 Z spiders, 9 X spiders, and 6 Hadamard edges, whereas the result in my notebook is a much longer circuit, 39 Z spiders, 6 X spiders, and 18 Hadamard edges.

Am I doing something wrong, or was the demo web page perhaps created using an earlier version of PyZX? (e.g. I notice there is a call to Circuit.to_graph(), which doesn't appear to be in the current API) It would certainly be nice to get that simpler result, if that is also correct ...

"
QCDIS/NaaVRE,"Build and start the NaaVRE on the dev env: https://github.com/QCDIS/NaaVRE/issues/584
Description: Executed commands:
```
conda create -n jupyterlab  python=3.9 
conda activate jupyterlab
conda env update -f environment.yml
conda install jupyterlab nodejs yarn
conda install -c conda-forge typescript 
npm install lerna --force
npm install --force
make install-backend && make build-frontend && make install-ui && make link-ui
```

Succesful so far. Running `jupyter lab build && jupyter lab --debug --watch` gives:
```
[LabBuildApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `LabBuildApp`.
[LabBuildApp] JupyterLab 3.6.1
[LabBuildApp] Building in /usr/local/anaconda3/envs/jupyterlab/share/jupyter/lab
[LabBuildApp] Building jupyterlab assets (development)
Build failed.
Troubleshooting: If the build failed due to an out-of-memory error, you
may be able to fix it by disabling the `dev_build` and/or `minimize` options.

If you are building via the `jupyter lab build` command, you can disable
these options like so:

jupyter lab build --dev-build=False --minimize=False

You can also disable these options for all JupyterLab builds by adding these
lines to a Jupyter config file named `jupyter_config.py`:

c.LabBuildApp.minimize = False
c.LabBuildApp.dev_build = False

If you don't already have a `jupyter_config.py` file, you can create one by
adding a blank file of that name to any of the Jupyter config directories.
The config directories can be listed by running:

jupyter --paths

Explanation:

- `dev-build`: This option controls whether a `dev` or a more streamlined
`production` build is used. This option will default to `False` (i.e., the
`production` build) for most users. However, if you have any labextensions
installed from local files, this option will instead default to `True`.
Explicitly setting `dev-build` to `False` will ensure that the `production`
build is used in all circumstances.

- `minimize`: This option controls whether your JS bundle is minified
during the Webpack build, which helps to improve JupyterLab's overall
performance. However, the minifier plugin used by Webpack is very memory
intensive, so turning it off may help the build finish successfully in
low-memory environments.

An error occurred.
RuntimeError: JupyterLab failed to build
See the log file for details:  /var/folders/7n/q7mx8hx94bqdmbd91vplkyx40000gn/T/jupyterlab-debug-guejpcor.log

```




"
patrickfuller/imolecule,"'pybel' has no attribute 'ob': https://github.com/patrickfuller/imolecule/issues/33
Description: While attempting to draw() a smile, I get this error message:

`AttributeError                            Traceback (most recent call last)
<ipython-input-7-64ed846587c6> in <module>()
----> 1 import imolecule as i
      2 i.draw(""CC1(C(N2C(S1)C(C2=O)NC(=O)CC3=CC=CC=C3)C(=O)O)C"")

~/anaconda2/envs/python35/lib/python3.5/site-packages/imolecule/__init__.py in <module>()
----> 1 from imolecule.notebook import draw, generate, to_json  # noqa
      2 
      3 __title__ = 'imolecule'
      4 __version__ = '0.1.13'
      5 __author__ = 'Patrick Fuller'

~/anaconda2/envs/python35/lib/python3.5/site-packages/imolecule/notebook.py in <module>()
      6 
      7 from . import json_formatter as json
----> 8 from . import format_converter
      9 
     10 file_path = os.path.normpath(os.path.dirname(__file__))

~/anaconda2/envs/python35/lib/python3.5/site-packages/imolecule/format_converter.py in <module>()
     10 try:
     11     import pybel
---> 12     ob = pybel.ob
     13     table = ob.OBElementTable()
     14     has_ob = True

AttributeError: module 'pybel' has no attribute 'ob'`

I can't seem to find a reference to pybel.ob... Is this a mistake?

"
patrickfuller/imolecule,"Not working on jupyter lab: https://github.com/patrickfuller/imolecule/issues/32
Description: Recently I switched from jupyter notebook to jupyter lab and I found `imolecule` not working. 
In a jupyter lab notebook, `imolecule.draw(""CC1(C(N2C(S1)C(C2=O)NC(=O)CC3=CC=CC=C3)C(=O)O)C"")` yields no error nor output. However, in the bowser (chrome) inspection console, the browser reports that `JupyterLab does not execute inline JavaScript in HTML output`.
 If I use jupyter notebook, everything is fine.

"
patrickfuller/imolecule,"ttributeError: 'module' object has no attribute 'json_formatter': https://github.com/patrickfuller/imolecule/issues/31
Description: Hi, i'm trying to use see how imolecules works in jupyter but i keep getting the following error when trying to follow you example:

```
import imolecule
imolecule.draw(""CC1(C(N2C(S1)C(C2=O)NC(=O)CC3=CC=CC=C3)C(=O)O)C"")
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-2-c19f0d58009f> in <module>()
----> 1 import imolecule
      2 imolecule.draw(""CC1(C(N2C(S1)C(C2=O)NC(=O)CC3=CC=CC=C3)C(=O)O)C"")

/usr/local/lib/python2.7/site-packages/imolecule/__init__.py in <module>()
----> 1 from imolecule.notebook import draw, generate, to_json  # noqa
      2 
      3 __title__ = 'imolecule'
      4 __version__ = '0.1.13'
      5 __author__ = 'Patrick Fuller'

/usr/local/lib/python2.7/site-packages/imolecule/notebook.py in <module>()
      5 from IPython.display import HTML, display
      6 
----> 7 import imolecule.json_formatter as json
      8 from imolecule import format_converter
      9 

AttributeError: 'module' object has no attribute 'json_formatter'
```

Python2.7 has been installed using home brew along with Swig and jupyter. Python dependences; ipython, pybel and openbabel have all been installed using pip2. 

I have uninstalled and re-installed but that doesn't appear to work - any ideas?

Cheers, 
Ollie

"
patrickfuller/imolecule,"Changing camera view: https://github.com/patrickfuller/imolecule/issues/27
Description: In my attempt to replicate the `examples/mof.html`, my MOF is rotated at a very awkward angle when I first open it. Is it easy to change the default view, through an up vector and view vector? First, for the example in `examples/mof.html`. Second, in the IPython Notebook, this would be nice too. Such as:

```
imolecule.draw(""IRMOF-1.cif"", show_save=True, up_vector=[1, 0, 0], view_vector=[0, 1, 0])
```


"
patrickfuller/imolecule,"Displaying many structures in one notebook: https://github.com/patrickfuller/imolecule/issues/26
Description: It seems that only ten visualizations will display at once. e.g. when I display 15 structures, the screen is blank for the first 5. How do I overcome this limitation?


"
patrickfuller/imolecule,"Failed to initialize WebGL context: https://github.com/patrickfuller/imolecule/issues/23
Description: I've noticed that in a notebook with many imolecule output cells, both Safari and Chrome fail to render all the cells, some of them throw the following error:

![screen shot 2015-09-28 at 16 03 04](https://cloud.githubusercontent.com/assets/940353/10137617/b4c8b0c6-65fa-11e5-826c-89b231bed91a.png)

I've also noticed that when I develop a notebook and create a lot of imolecule output cells, the browser eventually starts throwing

> WARNING: Too many active WebGL contexts. Oldest context will be lost.

I've [found](http://stackoverflow.com/questions/21548247/clean-up-threejs-webgl-contexts) that this may be solved by keeping only a single renderer for different scene, but I don't know whether it would be possible to keep a single shared rendered for all imolecule outputs.


"
patrickfuller/imolecule,"Jupyter notebook compatibility warning: https://github.com/patrickfuller/imolecule/issues/22
Description: Importing imolecule in IPython 4 (Jupyter) generates following warning:

> ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.
>   ""`IPython.html.widgets` has moved to `ipywidgets`."", ShimWarning)

P.S. Great project, thanks!


"
patrickfuller/imolecule,"Fixed rawgit URL of imolecule.min.js: https://github.com/patrickfuller/imolecule/pull/20
Description: I have fixed the rawgit URL in notebook.py, and removed the duplicate file_path variable.


"
patrickfuller/imolecule,"Why is the package stored in `python` instead of `imolecule`?: https://github.com/patrickfuller/imolecule/issues/13
Description: Hey Pat, I'm trying to use imolecule in another project and would need to make some small tweaks. I'll file a PR for them later but was curious about the choice to name the folder containing the package `python` instead of the more standard `imolecule`?

This prevents me from running `python setup.py develop` which is how I normally install packages I want to fiddle with. How do you do this locally? 

For the record, just changing the name of the folder and adjusting the entry in `setup.py` fixes this issue for me but I may just not be using it the same way that you are.

This is the relatively obvious output in case you're curious:

``` python
$ python setup.py develop
Traceback (most recent call last):
  File ""setup.py"", line 2, in <module>
    from python import __version__
  File ""/Users/ctk3b/imolecule/python/__init__.py"", line 1, in <module>
    from imolecule.notebook import draw, generate, to_json
ImportError: No module named 'imolecule'
```


"
patrickfuller/imolecule,"404 on local nbextensions: https://github.com/patrickfuller/imolecule/issues/12
Description: I'm getting a `404` when running the `imolecule` nbextension:

``` bash
[W 10:47:15.960 NotebookApp] 404 GET /nbextensionsimolecule.min.js?v=20150729104512 (127.0.0.1) 
0.00ms referer=http://localhost:8888/notebooks/Untitled.ipynb?kernel_name=python2
```

I believe this has to do with the following line in the [notebook.py](https://github.com/patrickfuller/imolecule/blob/master/python/notebook.py#L12) file:

``` python
local_path = os.path.join(""nbextensions"", filename)
```

While running it on windows, the `local_path` is joined using `\\` instead of `/` and that might be causing the message and actually never loading the local `imolecule.min.js` library.


"
patrickfuller/imolecule,"Feature/animation loop optimization: https://github.com/patrickfuller/imolecule/pull/8
Description: Allows me to get from 30fps to 50fps. And significantly reduce lag and
CPU usaqe on notebooks containing >10 molecules.

You can give it a try to see if it also improves for you.

"
damoncrockett/ivpy,"Running on Google Colab?: https://github.com/damoncrockett/ivpy/issues/88
Description: Hi @damoncrockett thanks for this amazing package! I'm wondering if there's currently a way of running ivpy on a Google Colab notebook. I realize there's no `setup.py` yet, so `!pip install https://github.com/damoncrockett/ivpy.git` will fail. I am trying something along the lines of 
```
import sys
sys.path.append(""/content/ivpy/src/ivpy"") 
from ivpy import data
```
but it still can't find the modules:
```
ImportError: cannot import name 'data' from 'ivpy' (unknown location)
```
"
apache/beam,"Update jupyter-client requirement from !=6.1.13,<8.1.1,>=6.1.11 to >=6.1.11,!=6.1.13,<8.2.1 in /sdks/python: https://github.com/apache/beam/pull/26340
Description: Updates the requirements on [jupyter-client](https://github.com/jupyter/jupyter_client) to permit the latest version.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/jupyter/jupyter_client/releases"">jupyter-client's releases</a>.</em></p>
<blockquote>
<h2>v8.2.0</h2>
<h2>8.2.0</h2>
<p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v8.1.0...dbf6b81fa5ab606eaedc5e8d0843debce18e8746"">Full Changelog</a>)</p>
<h3>Enhancements made</h3>
<ul>
<li>use c.f.Future to wait across threads <a href=""https://redirect.github.com/jupyter/jupyter_client/pull/940"">#940</a> (<a href=""https://github.com/minrk""><code>@​minrk</code></a>)</li>
</ul>
<h3>Maintenance and upkeep improvements</h3>
<ul>
<li>Use local coverage <a href=""https://redirect.github.com/jupyter/jupyter_client/pull/945"">#945</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>
<li>Add more project URLs <a href=""https://redirect.github.com/jupyter/jupyter_client/pull/944"">#944</a> (<a href=""https://github.com/fcollonval""><code>@​fcollonval</code></a>)</li>
</ul>
<h3>Contributors to this release</h3>
<p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2023-03-20&amp;to=2023-04-13&amp;type=c"">GitHub contributors page for this release</a>)</p>
<p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ablink1073+updated%3A2023-03-20..2023-04-13&amp;type=Issues""><code>@​blink1073</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Afcollonval+updated%3A2023-03-20..2023-04-13&amp;type=Issues""><code>@​fcollonval</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Aminrk+updated%3A2023-03-20..2023-04-13&amp;type=Issues""><code>@​minrk</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Apre-commit-ci+updated%3A2023-03-20..2023-04-13&amp;type=Issues""><code>@​pre-commit-ci</code></a></p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/jupyter/jupyter_client/blob/main/CHANGELOG.md"">jupyter-client's changelog</a>.</em></p>
<blockquote>
<h2>8.2.0</h2>
<p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v8.1.0...dbf6b81fa5ab606eaedc5e8d0843debce18e8746"">Full Changelog</a>)</p>
<h3>Enhancements made</h3>
<ul>
<li>use c.f.Future to wait across threads <a href=""https://redirect.github.com/jupyter/jupyter_client/pull/940"">#940</a> (<a href=""https://github.com/minrk""><code>@​minrk</code></a>)</li>
</ul>
<h3>Maintenance and upkeep improvements</h3>
<ul>
<li>Use local coverage <a href=""https://redirect.github.com/jupyter/jupyter_client/pull/945"">#945</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>
<li>Add more project URLs <a href=""https://redirect.github.com/jupyter/jupyter_client/pull/944"">#944</a> (<a href=""https://github.com/fcollonval""><code>@​fcollonval</code></a>)</li>
</ul>
<h3>Contributors to this release</h3>
<p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2023-03-20&amp;to=2023-04-13&amp;type=c"">GitHub contributors page for this release</a>)</p>
<p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ablink1073+updated%3A2023-03-20..2023-04-13&amp;type=Issues""><code>@​blink1073</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Afcollonval+updated%3A2023-03-20..2023-04-13&amp;type=Issues""><code>@​fcollonval</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Aminrk+updated%3A2023-03-20..2023-04-13&amp;type=Issues""><code>@​minrk</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Apre-commit-ci+updated%3A2023-03-20..2023-04-13&amp;type=Issues""><code>@​pre-commit-ci</code></a></p>
<!-- raw HTML omitted -->
<h2>8.1.0</h2>
<p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v8.0.3...e3ac7a69355dd1af66038eda767e51e92ef034fb"">Full Changelog</a>)</p>
<h3>Bugs fixed</h3>
<ul>
<li>ThreadedZMQStream: close stream before socket <a href=""https://redirect.github.com/jupyter/jupyter_client/pull/936"">#936</a> (<a href=""https://github.com/minrk""><code>@​minrk</code></a>)</li>
</ul>
<h3>Maintenance and upkeep improvements</h3>
<h3>Documentation improvements</h3>
<ul>
<li>Adds spec for the copyToGlobals request <a href=""https://redirect.github.com/jupyter/jupyter_client/pull/932"">#932</a> (<a href=""https://github.com/brichet""><code>@​brichet</code></a>)</li>
</ul>
<h3>Contributors to this release</h3>
<p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2023-02-16&amp;to=2023-03-20&amp;type=c"">GitHub contributors page for this release</a>)</p>
<p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ablink1073+updated%3A2023-02-16..2023-03-20&amp;type=Issues""><code>@​blink1073</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Abrichet+updated%3A2023-02-16..2023-03-20&amp;type=Issues""><code>@​brichet</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Aminrk+updated%3A2023-02-16..2023-03-20&amp;type=Issues""><code>@​minrk</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Apre-commit-ci+updated%3A2023-02-16..2023-03-20&amp;type=Issues""><code>@​pre-commit-ci</code></a></p>
<h2>8.0.3</h2>
<p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v8.0.2...dc0eaba1f609079672ec739fcd977dc44431da92"">Full Changelog</a>)</p>
<h3>Bugs fixed</h3>
<ul>
<li>Fix kernelspec print output <a href=""https://redirect.github.com/jupyter/jupyter_client/pull/933"">#933</a> (<a href=""https://github.com/minrk""><code>@​minrk</code></a>)</li>
<li>Don't emit a trailng newline in base64-encoded data like 'image/png' <a href=""https://redirect.github.com/jupyter/jupyter_client/pull/931"">#931</a> (<a href=""https://github.com/xl0""><code>@​xl0</code></a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/c4bb03cf9c028c9b9374e3506e5a4908c35e1d44""><code>c4bb03c</code></a> Publish 8.2.0</li>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/dbf6b81fa5ab606eaedc5e8d0843debce18e8746""><code>dbf6b81</code></a> Use local coverage (<a href=""https://redirect.github.com/jupyter/jupyter_client/issues/945"">#945</a>)</li>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/58017fc04199ab012ad2b6f5a01b8d3e11698e7c""><code>58017fc</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://redirect.github.com/jupyter/jupyter_client/issues/943"">#943</a>)</li>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/c5d9e1a8504278cfbfdd8a5778c8d83f8f9620ac""><code>c5d9e1a</code></a> Add more project URLs (<a href=""https://redirect.github.com/jupyter/jupyter_client/issues/944"">#944</a>)</li>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/98c87fece3e6d28cb2b977eb9632584cdad4273f""><code>98c87fe</code></a> use c.f.Future to wait across threads (<a href=""https://redirect.github.com/jupyter/jupyter_client/issues/940"">#940</a>)</li>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/2f6357fd8069329211d00124434fd17930a2eb12""><code>2f6357f</code></a> Publish 8.1.0</li>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/e3ac7a69355dd1af66038eda767e51e92ef034fb""><code>e3ac7a6</code></a> ThreadedZMQStream: close stream before socket (<a href=""https://redirect.github.com/jupyter/jupyter_client/issues/936"">#936</a>)</li>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/f6a03ac83ed7bc286d8dd88ec72289f6e5de5dae""><code>f6a03ac</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://redirect.github.com/jupyter/jupyter_client/issues/935"">#935</a>)</li>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/df84040b05d4eb3a9a9530de8076b0f7c09f9c10""><code>df84040</code></a> Adds spec for the copyToGlobals request (<a href=""https://redirect.github.com/jupyter/jupyter_client/issues/932"">#932</a>)</li>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/adff6b1d4389c885ee7ff4764fc5ffad6fcbe53f""><code>adff6b1</code></a> Publish 8.0.3</li>
<li>Additional commits viewable in <a href=""https://github.com/jupyter/jupyter_client/compare/6.1.11...v8.2.0"">compare view</a></li>
</ul>
</details>
<br />


Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>

"
apache/beam,"Tensorflow Hub Notebook minor conciseness improvements: https://github.com/apache/beam/pull/26336
Description: We don't need the init and can simplify the logic a bit here

------------------------

Thank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:

 - [ ] Mention the appropriate issue in your description (for example: `addresses #123`), if applicable. This will automatically add a link to the pull request in the issue. If you would like the issue to automatically close on merging the pull request, comment `fixes #<ISSUE NUMBER>` instead.
 - [ ] Update `CHANGES.md` with noteworthy changes.
 - [ ] If this contribution is large, please file an Apache [Individual Contributor License Agreement](https://www.apache.org/licenses/icla.pdf).

See the [Contributor Guide](https://beam.apache.org/contribute) for more tips on [how to make review process smoother](https://beam.apache.org/contribute/get-started-contributing/#make-the-reviewers-job-easier).

To check the build health, please visit [https://github.com/apache/beam/blob/master/.test-infra/BUILD_STATUS.md](https://github.com/apache/beam/blob/master/.test-infra/BUILD_STATUS.md)

GitHub Actions Tests Status (on master branch)
------------------------------------------------------------------------------------------------
[![Build python source distribution and wheels](https://github.com/apache/beam/workflows/Build%20python%20source%20distribution%20and%20wheels/badge.svg?branch=master&event=schedule)](https://github.com/apache/beam/actions?query=workflow%3A%22Build+python+source+distribution+and+wheels%22+branch%3Amaster+event%3Aschedule)
[![Python tests](https://github.com/apache/beam/workflows/Python%20tests/badge.svg?branch=master&event=schedule)](https://github.com/apache/beam/actions?query=workflow%3A%22Python+Tests%22+branch%3Amaster+event%3Aschedule)
[![Java tests](https://github.com/apache/beam/workflows/Java%20Tests/badge.svg?branch=master&event=schedule)](https://github.com/apache/beam/actions?query=workflow%3A%22Java+Tests%22+branch%3Amaster+event%3Aschedule)
[![Go tests](https://github.com/apache/beam/workflows/Go%20tests/badge.svg?branch=master&event=schedule)](https://github.com/apache/beam/actions?query=workflow%3A%22Go+tests%22+branch%3Amaster+event%3Aschedule)

See [CI.md](https://github.com/apache/beam/blob/master/CI.md) for more information about GitHub Actions CI.

"
ipyannotate/ipyannotate,"Can't make it work with JupyterLab 2.x: https://github.com/ipyannotate/ipyannotate/issues/12
Description: Dear all,

really neat idea! I am looking forward to experimenting it with JupyterLab and Voila.
Unfortunately I can't make it work with JupyterLab 2.1.5 - 2.2.4, while it works with the traditional notebook server.
I followed the instructions as reported, which are pretty standard.

My configuration shows
```bash
$ jupyter labextension list
JupyterLab v2.1.5
Known labextensions:
   app dir: .../share/jupyter/lab
        @jupyter-widgets/jupyterlab-manager v2.0.0  enabled  OK
        ipyannotate v0.1.0  enabled  OK
        ipycanvas v0.5.0  enabled  OK
        jupyter-leaflet v0.13.2  enabled  OK

$ conda list ipyannotate
# packages in environment at ...:
#
# Name                    Version                   Build  Channel
ipyannotate               0.1.0b0                  pypi_0    pypi
```

And JupyterLab complains with
```javascript
Error: Module ipyannotate, semver range ^0.1.0 is not registered as a widget module
```

Any idea about what I am doing wrong?
Thanks!

"
ipyannotate/ipyannotate,"Default way to save and load tasks: https://github.com/ipyannotate/ipyannotate/issues/6
Description: Hi! Is it possible to save a widget's state to a notebook?

> A Jupyter widget could not be displayed because the widget state could not be found. This could happen if the kernel storing the widget is no longer available, or if the widget state was not saved in the notebook. You may be able to create the widget by running the appropriate cells.

Recently I annotated about 2000 samples, results were't saved :disappointed: 
"
bmabey/pyLDAvis,"'TfidfVectorizer' object has no attribute 'get_feature_names': https://github.com/bmabey/pyLDAvis/issues/244
Description: I installed pyldavis in colab but when i run this code
"" pyLDAvis.sklearn.prepare(nmf_tfidf, tfidf, vectorizer)""
this error is show:""'TfidfVectorizer' object has no attribute 'get_feature_names'""

where is my code trouble and how to solve it 


"
bmabey/pyLDAvis,"`tsne` won't work with `sklearn.prepare`: https://github.com/bmabey/pyLDAvis/issues/233
Description: When running the same steps with the same data on colab I get pretty good results with `tsne`, but locally (probably because of the Python version) I'm not able to run `pyLDAvis.sklearn.prepare` as I get `ValueError: perplexity must be less than n_samples`.

I know that colab is running `3.7` and locally I got `3.10` . I also know that both use pyLDAvis version `3.3.1` so its probably broken because of a Scikit update.

I was able to get it to work by manually setting a `perplexity` value in the `TSNE` object initialization under [pyLDAvis/_prepare.py](https://github.com/bmabey/pyLDAvis/blob/master/pyLDAvis/_prepare.py) but it sure isn´t optimal.

<details>
<summary>Error log</summary>

```raw
Traceback (most recent call last):
  File ""/home/isinyaaa/projects/foss-gpgpu-stack/analyze.py"", line 418, in <module>
    main(args)
  File ""/home/isinyaaa/projects/foss-gpgpu-stack/analyze.py"", line 347, in main
    args.workers).run(vectorizer, processed_data)
  File ""/home/isinyaaa/projects/foss-gpgpu-stack/analyze.py"", line 128, in run
    self.save_result_as_html(model, data, vectorizer)
  File ""/home/isinyaaa/projects/foss-gpgpu-stack/analyze.py"", line 148, in save_result_as_html
    super().save_result_as_html(prepare, model, data, vectorizer, mds='tsne')
  File ""/home/isinyaaa/projects/foss-gpgpu-stack/analyze.py"", line 111, in save_result_as_html
    LDAvis_prepared = prepare(*args, **kwargs)
  File ""/home/isinyaaa/.local/lib/python3.10/site-packages/pyLDAvis/sklearn.py"", line 95, in prepare
    return pyLDAvis.prepare(**opts)
  File ""/home/isinyaaa/.local/lib/python3.10/site-packages/pyLDAvis/_prepare.py"", line 443, in prepare
    topic_coordinates = _topic_coordinates(mds, topic_term_dists, topic_proportion, start_index)
  File ""/home/isinyaaa/.local/lib/python3.10/site-packages/pyLDAvis/_prepare.py"", line 192, in _topic_coordinates
    mds_res = mds(topic_term_dists)
  File ""/home/isinyaaa/.local/lib/python3.10/site-packages/pyLDAvis/_prepare.py"", line 167, in js_TSNE
    return model.fit_transform(dist_matrix)
  File ""/home/isinyaaa/.local/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py"", line 1122, in fit_transform
    self._check_params_vs_input(X)
  File ""/home/isinyaaa/.local/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py"", line 793, in _check_params_vs_input
    raise ValueError(""perplexity must be less than n_samples"")
ValueError: perplexity must be less than n_samples
```

</details>

"
bmabey/pyLDAvis,"Video presentation not available anymore: https://github.com/bmabey/pyLDAvis/issues/230
Description: In [pyLDAvis_overview.ipynb](https://github.com/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb), when it says ""from the original R project and this presentation ([slides](https://speakerdeck.com/bmabey/visualizing-topic-models), [video](https://www.youtube.com/watch?v=tGxW2BzC_DU))"", the link to the Youtube video doesn't work anymore :(

Any backup?

"
bmabey/pyLDAvis,"Fix background color in Notebooks with dark themes: https://github.com/bmabey/pyLDAvis/pull/222
Description: This PR fixes #167 
"
ywx649999311/vizic,"Ipyw6: https://github.com/ywx649999311/vizic/pull/14
Description: fixed ipywidgets 6.0.0 and notebook > 4.3.2 compatibility issue

"
ywx649999311/vizic,"Installation on Ubuntu 16.04 64 bit: https://github.com/ywx649999311/vizic/issues/8
Description: Hello, I am afraid something went wrong with my Installation of vizic on Ubuntu 16.04
64 bit.
I am not familiar with python that much, can you please figure uot, what exactly went wrong?
The output from terminal:
charlyms@ubuntu:~/Downloads/vizic/examples$ jupyter notebook --NotebookApp.server_extensions=""['vizic.mongo_ext.extension']""
[W 07:15:47.440 NotebookApp] server_extensions is deprecated, use nbserver_extensions
[I 07:15:47.609 NotebookApp] Writing notebook server cookie secret to /run/user/1000/jupyter/notebook_cookie_secret

WARNING: version mismatch between CFITSIO header (v3.39) and linked library (v3.37).


WARNING: version mismatch between CFITSIO header (v3.39) and linked library (v3.37).


WARNING: version mismatch between CFITSIO header (v3.39) and linked library (v3.37).

/usr/local/lib/python3.5/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.
  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')
/usr/local/lib/python3.5/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.
  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')
/usr/local/lib/python3.5/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.
  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')
/usr/local/lib/python3.5/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.
  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')
[W 07:17:11.041 NotebookApp] Error loading server extension vizic.mongo_ext.extension
    Traceback (most recent call last):
      File ""/usr/local/lib/python3.5/dist-packages/matplotlib/font_manager.py"", line 1416, in <module>
        _rebuild()
      File ""/usr/local/lib/python3.5/dist-packages/matplotlib/font_manager.py"", line 1408, in _rebuild
        pickle_dump(fontManager, _fmcache)
      File ""/usr/local/lib/python3.5/dist-packages/matplotlib/font_manager.py"", line 956, in pickle_dump
        with open(filename, 'wb') as fh:
    PermissionError: [Errno 13] Permission denied: '/home/charlyms/.cache/matplotlib/fontList.py3k.cache'
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""/usr/local/lib/python3.5/dist-packages/notebook/notebookapp.py"", line 1046, in init_server_extensions
        mod = importlib.import_module(modulename)
      File ""/usr/lib/python3.5/importlib/__init__.py"", line 126, in import_module
        return _bootstrap._gcd_import(name[level:], package, level)
      File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
      File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
      File ""<frozen importlib._bootstrap>"", line 944, in _find_and_load_unlocked
      File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
      File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
      File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
      File ""<frozen importlib._bootstrap>"", line 944, in _find_and_load_unlocked
      File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
      File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
      File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
      File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
      File ""<frozen importlib._bootstrap>"", line 673, in _load_unlocked
      File ""<frozen importlib._bootstrap_external>"", line 665, in exec_module
      File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
      File ""/usr/local/lib/python3.5/dist-packages/vizic/__init__.py"", line 3, in <module>
        from .astroleaflet import AstroMap, GridLayer
      File ""/usr/local/lib/python3.5/dist-packages/vizic/astroleaflet.py"", line 12, in <module>
        from .healpix import get_vert_bbox
      File ""/usr/local/lib/python3.5/dist-packages/vizic/healpix.py"", line 1, in <module>
        import healpy as hp
      File ""/usr/local/lib/python3.5/dist-packages/healpy/__init__.py"", line 55, in <module>
        from .visufunc import (mollview,graticule,delgraticules,gnomview,
      File ""/usr/local/lib/python3.5/dist-packages/healpy/visufunc.py"", line 55, in <module>
        from . import projaxes as PA
      File ""/usr/local/lib/python3.5/dist-packages/healpy/projaxes.py"", line 24, in <module>
        import matplotlib.axes
      File ""/usr/local/lib/python3.5/dist-packages/matplotlib/axes/__init__.py"", line 4, in <module>
        from ._subplots import *
      File ""/usr/local/lib/python3.5/dist-packages/matplotlib/axes/_subplots.py"", line 10, in <module>
        from matplotlib.axes._axes import Axes
      File ""/usr/local/lib/python3.5/dist-packages/matplotlib/axes/_axes.py"", line 20, in <module>
        import matplotlib.collections as mcoll
      File ""/usr/local/lib/python3.5/dist-packages/matplotlib/collections.py"", line 27, in <module>
        import matplotlib.backend_bases as backend_bases
      File ""/usr/local/lib/python3.5/dist-packages/matplotlib/backend_bases.py"", line 62, in <module>
        import matplotlib.textpath as textpath
      File ""/usr/local/lib/python3.5/dist-packages/matplotlib/textpath.py"", line 15, in <module>
        import matplotlib.font_manager as font_manager
      File ""/usr/local/lib/python3.5/dist-packages/matplotlib/font_manager.py"", line 1421, in <module>
        _rebuild()
      File ""/usr/local/lib/python3.5/dist-packages/matplotlib/font_manager.py"", line 1408, in _rebuild
        pickle_dump(fontManager, _fmcache)
      File ""/usr/local/lib/python3.5/dist-packages/matplotlib/font_manager.py"", line 956, in pickle_dump
        with open(filename, 'wb') as fh:
    PermissionError: [Errno 13] Permission denied: '/home/charlyms/.cache/matplotlib/fontList.py3k.cache'
[I 07:17:11.856 NotebookApp] Serving notebooks from local directory: /home/charlyms/Downloads/vizic/examples
[I 07:17:11.858 NotebookApp] 0 active kernels 
[I 07:17:11.859 NotebookApp] The Jupyter Notebook is running at: http://localhost:8888/
[I 07:17:11.859 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 07:17:45.787 NotebookApp] Kernel started: 9c7d9bf7-8259-4f63-b00a-b10d91353082
[W 07:17:55.939 NotebookApp] Timeout waiting for kernel_info reply from 9c7d9bf7-8259-4f63-b00a-b10d91353082

Jupyter output:

```python
from vizic import *
import pandas as pd
import numpy as np
from ipywidgets import *
```


```python

```


```python
# If Jupyter sever is not running on the local host or having a port different than
# 8888, NotebookUrl widget is needed to obtain server address
url = NotebookUrl()
url
```


```python
# Initiate connection to mongodb database, the defaults are:
# host = 'localhost'; port=27017; db='vis'; 
# url='http://localhost:8888/' (this is for jupyter server)
c = Connection(url = url.nb_url)
```


    ---------------------------------------------------------------------------

    MissingSchema                             Traceback (most recent call last)

    <ipython-input-3-d885b0444e7b> in <module>()
          2 # host = 'localhost'; port=27017; db='vis';
          3 # url='http://localhost:8888/' (this is for jupyter server)
    ----> 4 c = Connection(url = url.nb_url)
    

    /usr/local/lib/python3.5/dist-packages/vizic/connection.py in __init__(self, host, port, db, url)
         15             self.client = pg.MongoClient(host, port)
         16             self.db = self.client[db]
    ---> 17             self.change_db(db)
         18         except ConnectionFailure as err:
         19             print('Error: Connection to MongoDB instance is refused!')


    /usr/local/lib/python3.5/dist-packages/vizic/connection.py in change_db(self, db)
         29         }
         30         path = url_path_join(self._url, '/connection/')
    ---> 31         req = requests.post(path, data=body)
         32         if req.status_code != 200:
         33             raise Exception('Change database failed!')


    /usr/lib/python3/dist-packages/requests/api.py in post(url, data, json, **kwargs)
        105     """"""
        106 
    --> 107     return request('post', url, data=data, json=json, **kwargs)
        108 
        109 


    /usr/lib/python3/dist-packages/requests/api.py in request(method, url, **kwargs)
         51     # cases, and look like a memory leak in others.
         52     with sessions.Session() as session:
    ---> 53         return session.request(method=method, url=url, **kwargs)
         54 
         55 


    /usr/lib/python3/dist-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
        452             hooks = hooks,
        453         )
    --> 454         prep = self.prepare_request(req)
        455 
        456         proxies = proxies or {}


    /usr/lib/python3/dist-packages/requests/sessions.py in prepare_request(self, request)
        386             auth=merge_setting(auth, self.auth),
        387             cookies=merged_cookies,
    --> 388             hooks=merge_hooks(request.hooks, self.hooks),
        389         )
        390         return p


    /usr/lib/python3/dist-packages/requests/models.py in prepare(self, method, url, headers, files, data, params, auth, cookies, hooks, json)
        291 
        292         self.prepare_method(method)
    --> 293         self.prepare_url(url, params)
        294         self.prepare_headers(headers)
        295         self.prepare_cookies(cookies)


    /usr/lib/python3/dist-packages/requests/models.py in prepare_url(self, url, params)
        351             error = error.format(to_native_string(url, 'utf8'))
        352 
    --> 353             raise MissingSchema(error)
        354 
        355         if not host:


    MissingSchema: Invalid URL 'connection/': No schema supplied. Perhaps you meant http://connection/?



```python
# display catalog for objects clicked
PopupDis(layer=g)
```


```python
df_vizic = pd.read_csv('demo.csv')
df_vizic['radius'] = df_vizic.petroR90_r # assign radius column
```


```python
g = GridLayer(c, coll_name=""vizic"", df=df_vizic, scale_r=2)
```


    ---------------------------------------------------------------------------

    NameError                                 Traceback (most recent call last)

    <ipython-input-5-7da0a1898616> in <module>()
    ----> 1 g = GridLayer(c, coll_name=""vizic"", df=df_vizic, scale_r=2)
    

    NameError: name 'c' is not defined



```python
# create map widget
m = AstroMap(default_tiles=g, zoom=1)
m
```


```python
# color picker for layers
LayerColorPicker(layer=g)
```


```python
# color by property
g.custom_c = True
cdrop = CFDropdown(g)
cdrop.layout.width= '50%'
cdrop
```


```python
# filter object through slider bar
f = FilterWidget(g)
g.filter_obj = True
f.link()
f
```


```python
jupyter nbextension enable --py --sys-prefix widgetsnbextension

```


```python

```

Kind regards from Germany
Charlyms

"
ywx649999311/vizic,"compatibility with jupyter notebook >= 4.3.0: https://github.com/ywx649999311/vizic/issues/6
Description: newest version added the _xrsf cookie argument and tokens to access the notebook, need to address 
"
holoviz/geoviews,"Param 2: allow `Dataset` to have any number of `kdims`: https://github.com/holoviz/geoviews/pull/626
Description: Fixes https://github.com/holoviz/geoviews/issues/624, in preparation of Param 2.

`kdims.bounds` was inherited from `_Element` with a value of `(2, 2)`, which appeared not to be valid for `Dataset`. Running the notebooks pointed out in the issue, I saw that `Dataset` could have more `kdims` than 2. I was also slightly surprised to see that it could have less than 2, I got an error where `kdims` was just `[Dimension('time')]`. I decided to let `kdims` accept any number of dimensions, i.e. `bounds=(0, None)`, as this anyway the default value in Param 1.

"
holoviz/geoviews,"Gridded dataset notebook raise ValueError: https://github.com/holoviz/geoviews/issues/624
Description: Running the `examples/user_guide/Gridded_Datasets_I.ipynb` and `examples/user_guide/Gridded_Datasets_II.ipynb` with the latest param and Holoviews, we get the following error: `ValueError: kdims: list length must be between 2 and 2 (inclusive)`.

Related PRs: https://github.com/holoviz/holoviews/pull/5667 and https://github.com/holoviz/param/pull/605


![image](https://user-images.githubusercontent.com/19758978/232283130-43214c27-7b6f-47f1-b1d3-7c0f91ff3bd1.png)


``` python-traceback
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[4], line 4
      1 kdims = ['time', 'longitude', 'latitude']
      2 vdims = ['surface_temperature']
----> 4 xr_dataset = gv.Dataset(xr_ensemble, kdims=kdims, vdims=vdims)
      5 iris_dataset = gv.Dataset(iris_ensemble, kdims=kdims, vdims=vdims)

File ~/Repos/holoviz/geoviews/geoviews/element/geo.py:110, in _Element.__init__(self, data, kdims, vdims, **kwargs)
    108 elif isinstance(data, _Element):
    109     kwargs['crs'] = data.crs
--> 110 super(_Element, self).__init__(data, kdims=kdims, vdims=vdims, **kwargs)

File ~/Repos/holoviz/holoviews/holoviews/core/data/__init__.py:334, in Dataset.__init__(self, data, kdims, vdims, **kwargs)
    331 initialized = Interface.initialize(type(self), data, kdims, vdims,
    332                                    datatype=kwargs.get('datatype'))
    333 (data, self.interface, dims, extra_kws) = initialized
--> 334 super().__init__(data, **dict(kwargs, **dict(dims, **extra_kws)))
    335 self.interface.validate(self, validate_vdims)
    337 # Handle _pipeline property

File ~/Repos/holoviz/holoviews/holoviews/core/dimension.py:844, in Dimensioned.__init__(self, data, kdims, vdims, **params)
    841 if 'cdims' in params:
    842     params['cdims'] = {d if isinstance(d, Dimension) else Dimension(d): val
    843                        for d, val in params['cdims'].items()}
--> 844 super().__init__(data, **params)
    845 self.ndims = len(self.kdims)
    846 cdims = [(d.name, val) for d, val in self.cdims.items()]

File ~/Repos/holoviz/holoviews/holoviews/core/dimension.py:503, in LabelledData.__init__(self, data, id, plot_id, **params)
    500     util.group_sanitizer.add_aliases(**{alias:long_name})
    501     params['group'] = long_name
--> 503 super().__init__(**params)
    504 if not util.group_sanitizer.allowable(self.group):
    505     raise ValueError(""Supplied group %r contains invalid characters."" %
    506                      self.group)

File ~/Repos/holoviz/param/param/parameterized.py:3231, in Parameterized.__init__(self, **params)
   3228 self._dynamic_watchers = defaultdict(list)
   3230 self.param._generate_name()
-> 3231 self.param._setup_params(**params)
   3232 object_count += 1
   3234 self.param._update_deps(init=True)

File ~/Repos/holoviz/param/param/parameterized.py:1453, in as_uninitialized.<locals>.override_initialization(self_, *args, **kw)
   1451 original_initialized = parameterized_instance.initialized
   1452 parameterized_instance.initialized = False
-> 1453 fn(parameterized_instance, *args, **kw)
   1454 parameterized_instance.initialized = original_initialized

File ~/Repos/holoviz/param/param/parameterized.py:1704, in Parameters._setup_params(self_, **params)
   1702     self.param.warning(""Setting non-parameter attribute %s=%s using a mechanism intended only for parameters"", name, val)
   1703 # i.e. if not desc it's setting an attribute in __dict__, not a Parameter
-> 1704 setattr(self, name, val)

File ~/Repos/holoviz/param/param/parameterized.py:382, in instance_descriptor.<locals>._f(self, obj, val)
    380     instance_param.__set__(obj, val)
    381     return
--> 382 return f(self, obj, val)

File ~/Repos/holoviz/param/param/parameterized.py:1266, in Parameter.__set__(self, obj, val)
   1263 if hasattr(self, 'set_hook'):
   1264     val = self.set_hook(obj,val)
-> 1266 self._validate(val)
   1268 _old = NotImplemented
   1269 # obj can be None if __set__ is called for a Parameterized class

File ~/Repos/holoviz/param/param/__init__.py:1671, in List._validate(self, val)
   1666 """"""
   1667 Checks that the value is numeric and that it is within the hard
   1668 bounds; if not, an exception is raised.
   1669 """"""
   1670 self._validate_value(val, self.allow_None)
-> 1671 self._validate_bounds(val, self.bounds)
   1672 self._validate_item_type(val, self.item_type)

File ~/Repos/holoviz/param/param/__init__.py:1682, in List._validate_bounds(self, val, bounds)
   1680 if min_length is not None and max_length is not None:
   1681     if not (min_length <= l <= max_length):
-> 1682         raise ValueError(""%s: list length must be between %s and %s (inclusive)""%(self.name,min_length,max_length))
   1683 elif min_length is not None:
   1684     if not min_length <= l:

ValueError: kdims: list length must be between 2 and 2 (inclusive)
```



"
holoviz/geoviews,"Geoviews does not support NaT with time zone aware datetimes: https://github.com/holoviz/geoviews/issues/617
Description: I have a dataset that includes a datetime variable that is time aware, and has NaT values. I can create a gv.Dataset and also a gv.Points geometry with it, but it fails on display -> which makes me think it can't actually manage it.

After looking at it updside down once and again, I've realized that this is because my dataset has time zone aware datetimes. With the example I give below, it is clear for me that a dataset with NaT works fine when datetimes are time zone naive, but not if they are time zone aware. 

I am running Geoviews version 1.9.6

(Unfortunately I cannot get GitHub uploading the image when the time zone naive gv.Points feature is displayed, but it does display correctly.)

```python
import datetime
import numpy as np
import pandas as pd
import geoviews as gv
gv.extension('bokeh')
import geoviews.tile_sources as gvts
from bokeh.models.tools import HoverTool

print('Geoviews version', gv.__version__)

lats = [50,50.2,51]
lons = [0.1,2,0.6]
dates = [datetime.datetime(2023,1,1,1),np.nan,datetime.datetime(2023,1,2,1)]
values = [2,3,-1]
source = gvts.CartoEco

data_df = pd.DataFrame({'lat':lats,'lon':lons,'naive_date':dates,'value':values})
data_df['aware_date'] = data_df['naive_date'].dt.tz_localize(tz='UTC')

cdims =  ['lon', 'lat']
vdims_naive = ['naive_date','value'] 
vdims_aware = ['aware_date','value'] 
dataset_naive = gv.Dataset(data_df,cdims,vdims_naive)
dataset_aware = gv.Dataset(data_df,cdims,vdims_aware)
naive = source*dataset_naive.to(gv.Points,cdims,vdims_naive)
aware = source*dataset_aware.to(gv.Points,cdims,vdims_aware)
```
Geoviews version 1.9.6

```python
naive
```

(map with 3 points displays here...)

```python
aware
```

```python
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/.conda/envs/notebooks/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj, include, exclude)
    968 
    969             if method is not None:
--> 970                 return method(include=include, exclude=exclude)
    971             return None
    972         else:

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/core/dimension.py in _repr_mimebundle_(self, include, exclude)
   1288         combined and returned.
   1289         """"""
-> 1290         return Store.render(self)
   1291 
   1292 

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/core/options.py in render(cls, obj)
   1423         data, metadata = {}, {}
   1424         for hook in hooks:
-> 1425             ret = hook(obj)
   1426             if ret is None:
   1427                 continue

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/ipython/display_hooks.py in pprint_display(obj)
    277     if not ip.display_formatter.formatters['text/plain'].pprint:
    278         return None
--> 279     return display(obj, raw_output=True)
    280 
    281 

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/ipython/display_hooks.py in display(obj, raw_output, **kwargs)
    245     elif isinstance(obj, (CompositeOverlay, ViewableElement)):
    246         with option_state(obj):
--> 247             output = element_display(obj)
    248     elif isinstance(obj, (Layout, NdLayout, AdjointLayout)):
    249         with option_state(obj):

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/ipython/display_hooks.py in wrapped(element)
    139         try:
    140             max_frames = OutputSettings.options['max_frames']
--> 141             mimebundle = fn(element, max_frames=max_frames)
    142             if mimebundle is None:
    143                 return {}, {}

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/ipython/display_hooks.py in element_display(element, max_frames)
    185         return None
    186 
--> 187     return render(element)
    188 
    189 

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/ipython/display_hooks.py in render(obj, **kwargs)
     66         renderer = renderer.instance(fig='png')
     67 
---> 68     return renderer.components(obj, **kwargs)
     69 
     70 

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/plotting/renderer.py in components(self, obj, fmt, comm, **kwargs)
    392 
    393         if embed or config.comms == 'default':
--> 394             return self._render_panel(plot, embed, comm)
    395         return self._render_ipywidget(plot)
    396 

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/plotting/renderer.py in _render_panel(self, plot, embed, comm)
    399         doc = Document()
    400         with config.set(embed=embed):
--> 401             model = plot.layout._render_model(doc, comm)
    402         if embed:
    403             return render_model(model, comm)

~/.conda/envs/notebooks/lib/python3.7/site-packages/panel/viewable.py in _render_model(self, doc, comm)
    506         if comm is None:
    507             comm = state._comm_manager.get_server_comm()
--> 508         model = self.get_root(doc, comm)
    509 
    510         if config.embed:

~/.conda/envs/notebooks/lib/python3.7/site-packages/panel/viewable.py in get_root(self, doc, comm, preprocess)
    557         """"""
    558         doc = init_doc(doc)
--> 559         root = self._get_model(doc, comm=comm)
    560         if preprocess:
    561             self._preprocess(root)

~/.conda/envs/notebooks/lib/python3.7/site-packages/panel/layout/base.py in _get_model(self, doc, root, parent, comm)
    144         if root is None:
    145             root = model
--> 146         objects = self._get_objects(model, [], doc, root, comm)
    147         props = dict(self._init_params(), objects=objects)
    148         model.update(**self._process_param_change(props))

~/.conda/envs/notebooks/lib/python3.7/site-packages/panel/layout/base.py in _get_objects(self, model, old_objects, doc, root, comm)
    129             else:
    130                 try:
--> 131                     child = pane._get_model(doc, root, model, comm)
    132                 except RerenderError:
    133                     return self._get_objects(model, current_objects[:i], doc, root, comm)

~/.conda/envs/notebooks/lib/python3.7/site-packages/panel/pane/holoviews.py in _get_model(self, doc, root, parent, comm)
    263             plot = self.object
    264         else:
--> 265             plot = self._render(doc, comm, root)
    266 
    267         plot.pane = self

~/.conda/envs/notebooks/lib/python3.7/site-packages/panel/pane/holoviews.py in _render(self, doc, comm, root)
    340                 kwargs['comm'] = comm
    341 
--> 342         return renderer.get_plot(self.object, **kwargs)
    343 
    344     def _cleanup(self, root: Model | None = None) -> None:

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/plotting/bokeh/renderer.py in get_plot(self_or_cls, obj, doc, renderer, **kwargs)
     68         combining the bokeh model with another plot.
     69         """"""
---> 70         plot = super().get_plot(obj, doc, renderer, **kwargs)
     71         if plot.document is None:
     72             plot.document = Document() if self_or_cls.notebook_context else curdoc()

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/plotting/renderer.py in get_plot(self_or_cls, obj, doc, renderer, comm, **kwargs)
    234             init_key = tuple(v if d is None else d for v, d in
    235                              zip(plot.keys[0], defaults))
--> 236             plot.update(init_key)
    237         else:
    238             plot = obj

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/plotting/plot.py in update(self, key)
    934     def update(self, key):
    935         if len(self) == 1 and ((key == 0) or (key == self.keys[0])) and not self.drawn:
--> 936             return self.initialize_plot()
    937         item = self.__getitem__(key)
    938         self.traverse(lambda x: setattr(x, '_updated', True))

~/.conda/envs/notebooks/lib/python3.7/site-packages/geoviews/plotting/bokeh/plot.py in initialize_plot(self, ranges, plot, plots, source)
    105     def initialize_plot(self, ranges=None, plot=None, plots=None, source=None):
    106         opts = {} if isinstance(self, HvOverlayPlot) else {'source': source}
--> 107         fig = super(GeoPlot, self).initialize_plot(ranges, plot, plots, **opts)
    108         if self.geographic and self.show_bounds and not self.overlaid:
    109             from . import GeoShapePlot

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/plotting/bokeh/element.py in initialize_plot(self, ranges, plot, plots)
   2332             if self.tabs:
   2333                 subplot.overlaid = False
-> 2334             child = subplot.initialize_plot(ranges, plot, plots)
   2335             if isinstance(element, CompositeOverlay):
   2336                 # Ensure that all subplots are in the same state

~/.conda/envs/notebooks/lib/python3.7/site-packages/geoviews/plotting/bokeh/plot.py in initialize_plot(self, ranges, plot, plots, source)
    105     def initialize_plot(self, ranges=None, plot=None, plots=None, source=None):
    106         opts = {} if isinstance(self, HvOverlayPlot) else {'source': source}
--> 107         fig = super(GeoPlot, self).initialize_plot(ranges, plot, plots, **opts)
    108         if self.geographic and self.show_bounds and not self.overlaid:
    109             from . import GeoShapePlot

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/plotting/bokeh/element.py in initialize_plot(self, ranges, plot, plots, source)
   1380         self.handles['plot'] = plot
   1381 
-> 1382         self._init_glyphs(plot, element, ranges, source)
   1383         if not self.overlaid:
   1384             self._update_plot(key, plot, style_element)

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/plotting/bokeh/element.py in _init_glyphs(self, plot, element, ranges, source)
   1324         else:
   1325             style = self.style[self.cyclic_index]
-> 1326             data, mapping, style = self.get_data(element, ranges, style)
   1327             current_id = element._plot_id
   1328 

~/.conda/envs/notebooks/lib/python3.7/site-packages/geoviews/plotting/bokeh/plot.py in get_data(self, element, ranges, style)
    164     def get_data(self, element, ranges, style):
    165         if self._project_operation and self.geographic:
--> 166             element = self._project_operation(element, projection=self.projection)
    167         return super(GeoPlot, self).get_data(element, ranges, style)
    168 

~/.conda/envs/notebooks/lib/python3.7/site-packages/param/parameterized.py in __new__(class_, *args, **params)
   3656         inst = class_.instance()
   3657         inst.param._set_name(class_.__name__)
-> 3658         return inst.__call__(*args,**params)
   3659 
   3660     def __call__(self,*args,**kw):

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/core/operation.py in __call__(self, element, **kwargs)
    218         kwargs['link_dataset'] = self._propagate_dataset
    219         kwargs['link_inputs'] = self.p.link_inputs
--> 220         return element.apply(self, **kwargs)
    221 
    222 

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/core/accessors.py in pipelined_call(*args, **kwargs)
     41 
     42             try:
---> 43                 result = __call__(*args, **kwargs)
     44 
     45                 if not in_method:

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/core/accessors.py in __call__(self, apply_function, streams, link_inputs, link_dataset, dynamic, per_element, **kwargs)
    197             if hasattr(apply_function, 'dynamic'):
    198                 inner_kwargs['dynamic'] = False
--> 199             new_obj = apply_function(self._obj, **inner_kwargs)
    200             if (link_dataset and isinstance(self._obj, Dataset) and
    201                 isinstance(new_obj, Dataset) and new_obj._dataset is None):

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/core/operation.py in __call__(self, element, **kwargs)
    212             elif ((self._per_element and isinstance(element, Element)) or
    213                   (not self._per_element and isinstance(element, ViewableElement))):
--> 214                 return self._apply(element)
    215         elif 'streams' not in kwargs:
    216             kwargs['streams'] = self.p.streams

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/core/operation.py in _apply(self, element, key)
    139             if not in_method:
    140                 element._in_method = True
--> 141         ret = self._process(element, key)
    142         if hasattr(element, '_in_method') and not in_method:
    143             element._in_method = in_method

~/.conda/envs/notebooks/lib/python3.7/site-packages/geoviews/operation/projection.py in _process(self, element, key)
     38 
     39     def _process(self, element, key=None):
---> 40         return element.map(self._process_element, self.supported_types)
     41 
     42 

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/core/data/__init__.py in pipelined_fn(*args, **kwargs)
    197 
    198             try:
--> 199                 result = method_fn(*args, **kwargs)
    200                 if PipelineMeta.disable:
    201                     return result

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/core/data/__init__.py in map(self, *args, **kwargs)
   1205     @wraps(LabelledData.map)
   1206     def map(self, *args, **kwargs):
-> 1207         return super().map(*args, **kwargs)
   1208 
   1209     @wraps(LabelledData.relabel)

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/core/dimension.py in map(self, map_fn, specs, clone)
    698             return deep_mapped
    699         else:
--> 700             return map_fn(self) if applies else self
    701 
    702 

~/.conda/envs/notebooks/lib/python3.7/site-packages/geoviews/operation/projection.py in _process_element(self, element)
    167         mask = np.isfinite(coordinates[:, 0])
    168         dims = [d for d in element.dimensions() if d not in (xdim, ydim)]
--> 169         new_data = {k: v[mask] for k, v in element.columns(dims).items()}
    170         new_data[xdim.name] = coordinates[mask, 0]
    171         new_data[ydim.name] = coordinates[mask, 1]

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/core/data/__init__.py in pipelined_fn(*args, **kwargs)
    197 
    198             try:
--> 199                 result = method_fn(*args, **kwargs)
    200                 if PipelineMeta.disable:
    201                     return result

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/core/data/__init__.py in columns(self, dimensions)
   1154         else:
   1155             dimensions = [self.get_dimension(d, strict=True) for d in dimensions]
-> 1156         return OrderedDict([(d.name, self.dimension_values(d)) for d in dimensions])
   1157 
   1158 

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/core/data/__init__.py in <listcomp>(.0)
   1154         else:
   1155             dimensions = [self.get_dimension(d, strict=True) for d in dimensions]
-> 1156         return OrderedDict([(d.name, self.dimension_values(d)) for d in dimensions])
   1157 
   1158 

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/core/data/__init__.py in pipelined_fn(*args, **kwargs)
    197 
    198             try:
--> 199                 result = method_fn(*args, **kwargs)
    200                 if PipelineMeta.disable:
    201                     return result

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/core/data/__init__.py in dimension_values(self, dimension, expanded, flat)
   1091         """"""
   1092         dim = self.get_dimension(dimension, strict=True)
-> 1093         values = self.interface.values(self, dim, expanded, flat)
   1094         if dim.nodata is not None:
   1095             # Ensure nodata applies to boolean data in py2

~/.conda/envs/notebooks/lib/python3.7/site-packages/holoviews/core/data/pandas.py in values(cls, dataset, dim, expanded, flat, compute, keep_index)
    353         if data.dtype.kind == 'M' and getattr(data.dtype, 'tz', None):
    354             dts = [dt.replace(tzinfo=None) for dt in data.dt.to_pydatetime()]
--> 355             data = np.array(dts, dtype=data.dtype.base)
    356         if not expanded:
    357             return pd.unique(data)

ValueError: cannot convert float NaN to integer

:Overlay
   .WMTS.I   :WMTS   [Longitude,Latitude]
   .Points.I :Points   [lon,lat]   (aware_date,value)
```



"
holoviz/geoviews,"gv.feature.states - TypeError: 'MultiPolygon' object is not iterable: https://github.com/holoviz/geoviews/issues/605
Description: Software version info:
```
geoviews=1.9.5
holoviews=1.15.3
bokeh=2.4.3
python=3.8.15
jupyterlab=3.5.2
shapely=2.0.0
```

I can plot every other feature geometry just fine in my notebook but just `gf.states` as a singular command in its own cell throws an error from trying to iterate through a MultiPolygon:

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/IPython/core/formatters.py:972, in MimeBundleFormatter.__call__(self, obj, include, exclude)
    969     method = get_real_method(obj, self.print_method)
    971     if method is not None:
--> 972         return method(include=include, exclude=exclude)
    973     return None
    974 else:

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/core/dimension.py:1293, in Dimensioned._repr_mimebundle_(self, include, exclude)
   1286 def _repr_mimebundle_(self, include=None, exclude=None):
   1287     """"""
   1288     Resolves the class hierarchy for the class rendering the
   1289     object using any display hooks registered on Store.display
   1290     hooks.  The output of all registered display_hooks is then
   1291     combined and returned.
   1292     """"""
-> 1293     return Store.render(self)

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/core/options.py:1426, in Store.render(cls, obj)
   1424 data, metadata = {}, {}
   1425 for hook in hooks:
-> 1426     ret = hook(obj)
   1427     if ret is None:
   1428         continue

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/ipython/display_hooks.py:277, in pprint_display(obj)
    275 if not ip.display_formatter.formatters['text/plain'].pprint:
    276     return None
--> 277 return display(obj, raw_output=True)

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/ipython/display_hooks.py:247, in display(obj, raw_output, **kwargs)
    245 elif isinstance(obj, (CompositeOverlay, ViewableElement)):
    246     with option_state(obj):
--> 247         output = element_display(obj)
    248 elif isinstance(obj, (Layout, NdLayout, AdjointLayout)):
    249     with option_state(obj):

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/ipython/display_hooks.py:141, in display_hook.<locals>.wrapped(element)
    139 try:
    140     max_frames = OutputSettings.options['max_frames']
--> 141     mimebundle = fn(element, max_frames=max_frames)
    142     if mimebundle is None:
    143         return {}, {}

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/ipython/display_hooks.py:187, in element_display(element, max_frames)
    184 if type(element) not in Store.registry[backend]:
    185     return None
--> 187 return render(element)

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/ipython/display_hooks.py:68, in render(obj, **kwargs)
     65 if renderer.fig == 'pdf':
     66     renderer = renderer.instance(fig='png')
---> 68 return renderer.components(obj, **kwargs)

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/plotting/renderer.py:398, in Renderer.components(self, obj, fmt, comm, **kwargs)
    395 embed = (not (dynamic or streams or self.widget_mode == 'live') or config.embed)
    397 if embed or config.comms == 'default':
--> 398     return self._render_panel(plot, embed, comm)
    399 return self._render_ipywidget(plot)

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/plotting/renderer.py:405, in Renderer._render_panel(self, plot, embed, comm)
    403 doc = Document()
    404 with config.set(embed=embed):
--> 405     model = plot.layout._render_model(doc, comm)
    406 if embed:
    407     return render_model(model, comm)

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/panel/viewable.py:508, in Renderable._render_model(self, doc, comm)
    506 if comm is None:
    507     comm = state._comm_manager.get_server_comm()
--> 508 model = self.get_root(doc, comm)
    510 if config.embed:
    511     embed_state(self, model, doc,
    512                 json=config.embed_json,
    513                 json_prefix=config.embed_json_prefix,
    514                 save_path=config.embed_save_path,
    515                 load_path=config.embed_load_path,
    516                 progress=False)

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/panel/viewable.py:559, in Renderable.get_root(self, doc, comm, preprocess)
    542 """"""
    543 Returns the root model and applies pre-processing hooks
    544 
   (...)
    556 Returns the bokeh model corresponding to this panel object
    557 """"""
    558 doc = init_doc(doc)
--> 559 root = self._get_model(doc, comm=comm)
    560 if preprocess:
    561     self._preprocess(root)

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/panel/layout/base.py:146, in Panel._get_model(self, doc, root, parent, comm)
    144 if root is None:
    145     root = model
--> 146 objects = self._get_objects(model, [], doc, root, comm)
    147 props = dict(self._init_params(), objects=objects)
    148 model.update(**self._process_param_change(props))

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/panel/layout/base.py:131, in Panel._get_objects(self, model, old_objects, doc, root, comm)
    129 else:
    130     try:
--> 131         child = pane._get_model(doc, root, model, comm)
    132     except RerenderError:
    133         return self._get_objects(model, current_objects[:i], doc, root, comm)

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/panel/pane/holoviews.py:265, in HoloViews._get_model(self, doc, root, parent, comm)
    263     plot = self.object
    264 else:
--> 265     plot = self._render(doc, comm, root)
    267 plot.pane = self
    268 backend = plot.renderer.backend

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/panel/pane/holoviews.py:342, in HoloViews._render(self, doc, comm, root)
    339     if comm:
    340         kwargs['comm'] = comm
--> 342 return renderer.get_plot(self.object, **kwargs)

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/plotting/bokeh/renderer.py:70, in BokehRenderer.get_plot(self_or_cls, obj, doc, renderer, **kwargs)
     63 @bothmethod
     64 def get_plot(self_or_cls, obj, doc=None, renderer=None, **kwargs):
     65     """"""
     66     Given a HoloViews Viewable return a corresponding plot instance.
     67     Allows supplying a document attach the plot to, useful when
     68     combining the bokeh model with another plot.
     69     """"""
---> 70     plot = super().get_plot(obj, doc, renderer, **kwargs)
     71     if plot.document is None:
     72         plot.document = Document() if self_or_cls.notebook_context else curdoc()

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/plotting/renderer.py:240, in Renderer.get_plot(self_or_cls, obj, doc, renderer, comm, **kwargs)
    237     defaults = [kd.default for kd in plot.dimensions]
    238     init_key = tuple(v if d is None else d for v, d in
    239                      zip(plot.keys[0], defaults))
--> 240     plot.update(init_key)
    241 else:
    242     plot = obj

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/plotting/plot.py:948, in DimensionedPlot.update(self, key)
    946 def update(self, key):
    947     if len(self) == 1 and ((key == 0) or (key == self.keys[0])) and not self.drawn:
--> 948         return self.initialize_plot()
    949     item = self.__getitem__(key)
    950     self.traverse(lambda x: setattr(x, '_updated', True))

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/geoviews/plotting/bokeh/plot.py:111, in GeoPlot.initialize_plot(self, ranges, plot, plots, source)
    109 def initialize_plot(self, ranges=None, plot=None, plots=None, source=None):
    110     opts = {} if isinstance(self, HvOverlayPlot) else {'source': source}
--> 111     fig = super(GeoPlot, self).initialize_plot(ranges, plot, plots, **opts)
    112     if self.geographic and self.show_bounds and not self.overlaid:
    113         from . import GeoShapePlot

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/plotting/bokeh/element.py:1397, in ElementPlot.initialize_plot(self, ranges, plot, plots, source)
   1394     self.handles['y_range'] = plot.y_range
   1395 self.handles['plot'] = plot
-> 1397 self._init_glyphs(plot, element, ranges, source)
   1398 if not self.overlaid:
   1399     self._update_plot(key, plot, style_element)

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/plotting/bokeh/element.py:1341, in ElementPlot._init_glyphs(self, plot, element, ranges, source)
   1339 else:
   1340     style = self.style[self.cyclic_index]
-> 1341     data, mapping, style = self.get_data(element, ranges, style)
   1342     current_id = element._plot_id
   1344 with abbreviated_exception():

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/geoviews/plotting/bokeh/__init__.py:237, in FeaturePlot.get_data(self, element, ranges, style)
    235     el_type = Polygons
    236 polys = el_type(geoms, crs=element.crs, **util.get_param_values(element))
--> 237 return super(FeaturePlot, self).get_data(polys, ranges, style)

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/geoviews/plotting/bokeh/plot.py:170, in GeoPlot.get_data(self, element, ranges, style)
    168 def get_data(self, element, ranges, style):
    169     if self._project_operation and self.geographic:
--> 170         element = self._project_operation(element, projection=self.projection)
    171     return super(GeoPlot, self).get_data(element, ranges, style)

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/param/parameterized.py:3658, in ParameterizedFunction.__new__(class_, *args, **params)
   3656 inst = class_.instance()
   3657 inst.param._set_name(class_.__name__)
-> 3658 return inst.__call__(*args,**params)

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/core/operation.py:220, in Operation.__call__(self, element, **kwargs)
    218 kwargs['link_dataset'] = self._propagate_dataset
    219 kwargs['link_inputs'] = self.p.link_inputs
--> 220 return element.apply(self, **kwargs)

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/core/accessors.py:45, in AccessorPipelineMeta.pipelined.<locals>.pipelined_call(*args, **kwargs)
     42     inst._obj._in_method = True
     44 try:
---> 45     result = __call__(*args, **kwargs)
     47     if not in_method:
     48         init_op = factory.instance(
     49             output_type=type(inst),
     50             kwargs={'mode': getattr(inst, 'mode', None)},
     51         )

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/core/accessors.py:202, in Apply.__call__(self, apply_function, streams, link_inputs, link_dataset, dynamic, per_element, **kwargs)
    200 if hasattr(apply_function, 'dynamic'):
    201     inner_kwargs['dynamic'] = False
--> 202 new_obj = apply_function(self._obj, **inner_kwargs)
    203 if (link_dataset and isinstance(self._obj, Dataset) and
    204     isinstance(new_obj, Dataset) and new_obj._dataset is None):
    205     new_obj._dataset = self._obj.dataset

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/core/operation.py:214, in Operation.__call__(self, element, **kwargs)
    210         return element.clone([(k, self._apply(el, key=k))
    211                               for k, el in element.items()])
    212     elif ((self._per_element and isinstance(element, Element)) or
    213           (not self._per_element and isinstance(element, ViewableElement))):
--> 214         return self._apply(element)
    215 elif 'streams' not in kwargs:
    216     kwargs['streams'] = self.p.streams

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/core/operation.py:141, in Operation._apply(self, element, key)
    139     if not in_method:
    140         element._in_method = True
--> 141 ret = self._process(element, key)
    142 if hasattr(element, '_in_method') and not in_method:
    143     element._in_method = in_method

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/geoviews/operation/projection.py:40, in _project_operation._process(self, element, key)
     39 def _process(self, element, key=None):
---> 40     return element.map(self._process_element, self.supported_types)

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/core/data/__init__.py:204, in PipelineMeta.pipelined.<locals>.pipelined_fn(*args, **kwargs)
    201     inst._in_method = True
    203 try:
--> 204     result = method_fn(*args, **kwargs)
    205     if PipelineMeta.disable:
    206         return result

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/core/data/__init__.py:1216, in Dataset.map(self, *args, **kwargs)
   1214 @wraps(LabelledData.map)
   1215 def map(self, *args, **kwargs):
-> 1216     return super(Dataset, self).map(*args, **kwargs)

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/holoviews/core/dimension.py:700, in LabelledData.map(self, map_fn, specs, clone)
    698     return deep_mapped
    699 else:
--> 700     return map_fn(self) if applies else self

File /storage/work/c/cnd5285/mambaforge/envs/uviz/lib/python3.8/site-packages/geoviews/operation/projection.py:79, in project_path._process_element(self, element)
     77     continue
     78 elif isinstance(geom, MultiPolygon):
---> 79     polys = [g for g in geom if g.area > 1e-15]
     80     if not polys:
     81         continue

TypeError: 'MultiPolygon' object is not iterable
```

"
agermanidis/pigeon,"""No module named ""pigeon"" Module error after successfully installing in Mac M1 environment: https://github.com/agermanidis/pigeon/issues/14
Description: Is there an known interoperability issue with MacM1, Jupyter notebooks and Pigeon?  I have successfully installed it as you can see below but I get a module import error on my Jupyter notebook. 

```

Package            Version
------------------ ----------
appnope            0.1.3
asttokens          2.2.0
backcall           0.2.0
blis               0.7.9
catalogue          2.0.8
certifi            2022.9.24
charset-normalizer 2.1.1
click              8.1.3
confection         0.0.3
cymem              2.0.7
debugpy            1.6.4
decorator          5.1.1
en-core-web-sm     3.4.1
en-core-web-trf    3.4.1
entrypoints        0.4
executing          1.2.0
filelock           3.8.0
huggingface-hub    0.11.0
idna               3.4
ipykernel          6.17.1
ipython            8.7.0
ipywidgets         8.0.2
jedi               0.18.2
Jinja2             3.1.2
jupyter_client     7.4.7
jupyter_core       5.1.0
jupyterlab-widgets 3.0.3
langcodes          3.3.0
MarkupSafe         2.1.1
matplotlib-inline  0.1.6
murmurhash         1.0.9
nest-asyncio       1.5.6
numpy              1.23.4
packaging          21.3
pandas             1.5.1
parso              0.8.3
pathy              0.10.0
pexpect            4.8.0
pickleshare        0.7.5
pigeon-jupyter     0.1.0
pip                22.3.1
platformdirs       2.5.4
preshed            3.0.8
prompt-toolkit     3.0.33
psutil             5.9.4
ptyprocess         0.7.0
pure-eval          0.2.2
pydantic           1.10.2
Pygments           2.13.0
pyparsing          3.0.9
python-dateutil    2.8.2
pytz               2022.6
PyYAML             6.0
pyzmq              24.0.1
regex              2022.10.31
requests           2.28.1
semantic-version   2.10.0
setuptools         65.6.3
setuptools-rust    1.5.2
six                1.16.0
smart-open         5.2.1
spacy              3.4.3
spacy-alignments   0.8.6
spacy-legacy       3.0.10
spacy-loggers      1.0.3
spacy-transformers 1.1.8
srsly              2.4.5
stack-data         0.6.2
thinc              8.1.5
tokenizers         0.12.1
torch              1.13.0
tornado            6.2
tqdm               4.64.1
traitlets          5.6.0
transformers       4.21.3
typer              0.7.0
typing_extensions  4.4.0
urllib3            1.26.13
wasabi             0.10.1
wcwidth            0.2.5
wheel              0.37.1
widgetsnbextension 4.0.3
```


"
agermanidis/pigeon,"won't work on Jupyter lab: https://github.com/agermanidis/pigeon/issues/5
Description: I seem to be having some problems to run it. It won't render the buttons on the options. This might be related to me using Jupyter lab instead of regular notebooks.

I'm running:

```
Jupiter lab 4.4.0
Python 3.7.3
```

<img width=""948"" alt=""Screen Shot 2019-06-06 at 11 47 37"" src=""https://user-images.githubusercontent.com/8628802/59003456-fd4db400-8850-11e9-9fe6-ec39a25e362e.png"">
"
